[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Modelling in Drug Development",
    "section": "",
    "text": "Preface\nThis website contains materials that were developed to accompany a series of workshops held in the Analytics department at Novartis over 2022-2024, illustrating the utility of the R package brms for solving drug- development problems. In order to highlight the versatility of that package, we have developed a collection of case studies covering a diverse set of clinical questions that were addressed via Bayesian modelling with the brms package.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#navigating-the-site",
    "href": "index.html#navigating-the-site",
    "title": "Applied Modelling in Drug Development",
    "section": "Navigating the site",
    "text": "Navigating the site\nThe material is organized as follows:\n\n1  Introduction introduces brms and the objectives of these vignettes\n2  Basic workflow highlights the basic workflow (analysis steps and syntax) for carrying out an analysis with brms\nThe next series of sections contain the case studies. You can find a listing with descriptions on this page.\nSections 16  Exposing stan code and 17  Parallel computation cover more technical topics, such as exposing and injecting custom stan code to brms models, and efficient parallel computation to support sampling in brms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Applied Modelling in Drug Development",
    "section": "Updates",
    "text": "Updates\nThis web-site is intended as a live document with updates as appropiate. Key changes to the web-site are tracked here:\n\n29th April 2024: Third edition course from Paul Bürkner with the new case studies on probability of success, meta-analytic priors with covariates & time-to-event modelling in Oncology phase I dose-escalation. Pre-read version.\n19th April 2024: First public version at opensource.nibr.com/bamdd\n5th January 2024: Release web-site in more modern quarto book based format\n26th April 2023: Second edition course from Paul Bürkner with the new case studies on MMRM, time-to-event data, surrogate endpoint meta-regression and network meta-analysis\n27th April 2022: First release with course from Paul Bürkner",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license-information",
    "href": "index.html#license-information",
    "title": "Applied Modelling in Drug Development",
    "section": "License information",
    "text": "License information\n\nThe material on this website is provided under a CC BY 4.0 license\nThe source code is provided under a GPL-2 license",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "src/01a_introduction.html",
    "href": "src/01a_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Public documentation\nThere are many resources available online for learning about brms. We list a few:\nAn excellent place to ask questions on brms in the public is the Stan discourse forum.\nMore useful ressources from the Stan community:",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/01a_introduction.html#public-documentation",
    "href": "src/01a_introduction.html#public-documentation",
    "title": "1  Introduction",
    "section": "",
    "text": "The homepage for the package, https://paul-buerkner.github.io/brms/\nBürkner, P.-C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\nPaul-Christian Bürkner, Advanced Bayesian Multilevel Modeling with the R Package. The R Journal (2018) 10:1, pages 395-411. https://journal.r-project.org/archive/2018/RJ-2018-017/index.html\n\n\n\n\nFAQ on cross-validation\nStan runtime warnings and convergence problems explained",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html",
    "href": "src/01b_basic_workflow.html",
    "title": "2  Basic workflow",
    "section": "",
    "text": "2.1 Simple example\nHere we will use a simplified version of the case study presented in Chapter 4. Specifically, we will run a random-effects meta-analysis for a binary responder variable which has been measured in multiple trials:\n# load historical data on responses by arm from the RBesT package:\narm_data &lt;- RBesT::AS\nknitr::kable(arm_data)\n\n\n\n\nstudy\nn\nr\n\n\n\n\nStudy 1\n107\n23\n\n\nStudy 2\n44\n12\n\n\nStudy 3\n51\n19\n\n\nStudy 4\n39\n9\n\n\nStudy 5\n139\n39\n\n\nStudy 6\n20\n6\n\n\nStudy 7\n78\n9\n\n\nStudy 8\n35\n10\nWhile in the case study an informative prior for a future study will be derived, we here restrict the analysis to a random-effects meta-analysis with the goal to infer the mean response rate of the control arm as measured in the 8 reported studies.\nThe statistical model we wish to fit to this data is a random-effects varying intercept model\n\\[ y_i|\\theta_i,n_i \\sim \\mbox{Binomial}(\\theta_i,n_i) \\] \\[ \\mbox{logit}{(\\theta_i)}|\\beta,\\eta_i = \\beta + \\eta_i \\] \\[ \\eta_i|\\tau \\sim \\mbox{Normal}(0, \\tau^2).\\]\nThus, each study \\(i\\) will recieve it’s study-specific response rate \\(\\theta_i\\) as given by the sum of the study-specific random effect \\(\\eta_i\\) and the overall typical responder rate \\(\\beta\\), which are inferred on the logit scale. We choose the priors in a conservative manner aligned with the case study\n\\[ \\beta \\sim \\mbox{Normal}(0, 2^2)\\] \\[ \\tau \\sim \\mbox{Normal}^+(0, 1).\\]",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#r-session-setup",
    "href": "src/01b_basic_workflow.html#r-session-setup",
    "title": "2  Basic workflow",
    "section": "2.2 R session setup",
    "text": "2.2 R session setup\nIdeally your R session will have access to sufficient computational resources, e.g. at least 4 CPU cores and 8000 MB of RAM. The 4 cores are needed to run multiple chains in parallel and the RAM is used during compilation of Stan models.\nWhen working with brms in an applied modeling setting, one often wishes to fit various related models and compare their outputs. With this in mind a recommended preamble for an analysis R script may start with:\n\nlibrary(brms)\n# tools to process model outputs\nlibrary(posterior)\n# useful plotting facilities\nlibrary(bayesplot)\n# further customization of plots\nlibrary(ggplot2)\n# common data processing utilities\nlibrary(dplyr)\nlibrary(tidyr)\n# other utilities\nlibrary(knitr) # used for the \"kable\" command\nlibrary(here)  # useful to specify path's relative to project\n# ...\n\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\n\n# set the R random seed of the session for reproducibility\nset.seed(5886935)\n\nAs brms models are translated to Stan model files, which must be compiled as a C++ program before the model is run, it is useful to cache this compilation step such that repeated model evaluation avoids the compilation (speeding up model reruns with different data or model reruns after restarting R). Thus, the brms.backend option cmdstanr is configured as a default. Doing so allows to cache the binary Stan executables in the directory configured with the option cmdstanr_write_stan_file_dir.\nIn case the model fitted is taking a long time for one chain (more than a minute at least), then it is advisable to turn on parallelization of the multiple chains by default by adding to the preamble:\n\n# use 4 cores for parallel sampling by default - only recommended if\n# model takes &gt; 1 minute to run for each chain otherwise the\n# parallelization introduces substantial overhead\noptions(mc.cores = 4)\n\nIt is discouraged to run chains always in parallel, since the parallelization itself consumes some ressources and is only beneficial if the model runtime is at least at the order of minutes. In case the model runtime becomes excessivley long, then each chain can itself be run with multiple cores as discussed in the section Parallel computation.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-formula",
    "href": "src/01b_basic_workflow.html#model-formula",
    "title": "2  Basic workflow",
    "section": "2.3 Model formula",
    "text": "2.3 Model formula\nWith brms statistical models are specified via a model formula and specification of a response family. The family encodes the likelihood and link functions used for the distribution parameters. A distribution parameter is, for example, the response rate of a binomial outcome, the counting rate for a negative binomial endpoint or the overdispersion of a negative binomial.\nIn general formulaes can be recognized by a ~ sign. Anything on the left-hand side of the ~ describes the response (outcome) while terms on the right-hand side setup the design matrix, random effects or even a non-linear model formula. The left-hand side and the right-hand side can contain special terms which encode additional information and the chosen example makes use of this feature. It’s good practice to store the brms model formula in a separate R variable like:\n\nmodel_meta_random &lt;- bf(r | trials(n) ~ 1 + (1 | study), family=binomial)\n\nThe left-hand side of the formula r | trials(n) encodes the main outcome response variable r (number of responders) and it passes along additional information needed for the binomial response here, the number of respondents using trials(n). Using the same notation, the case-study “Meta-analysis to estimate treatment effects” shows how the exposure time can be varyied for count outcomes. The right-hand side of the formula specifies a linear predictor 1. This defines the fixed effect design matrix, which is the intercept only term in this example, but covariates could be included here. In addition, the term (1 | study) defines a random effect with a by study varying intercept. Finally, the family argument specifies the statistical family being used. The link function is by default a logit link for the binomial family. Note that brms supports a large set of families, please refer to\n\n?brmsfamily\n\nfor an overview on the available families in brms. In case your specific family neededed is not available, users even have the option to define their own family. This is a somewhat advanced use of brms, but as brms is designed to accomodate many different families this is in fact not too difficult and a full vignette is available online.\nAn alternative model in this context could be a fixed effect only model:\n\nmodel_meta_fixed &lt;- bf(r | trials(n) ~ 1, family=binomial)\n\nNote that for some models one may even need to specify multiple model formulas. This can be the case whenever multiple distribution parameters must be specified (a negative binomial needs a mean rate model and a model for the dispersion parameter) or whenever a non-linear model is used, see the case study on dose finding in section @ref(dose-finding).",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#prior-specification",
    "href": "src/01b_basic_workflow.html#prior-specification",
    "title": "2  Basic workflow",
    "section": "2.4 Prior specification",
    "text": "2.4 Prior specification\nThe specification of priors is not strictly required for generalized linear models in brms. In this case very wide defaults priors are setup for the user. However, these only work well whenever a lot of data is available and it is furthermore a much better practice to be specific about the priors being used in a given analysis.\nIn order to support the user in specifying priors, the brms package provides the get_prior function. We start with the simple fixed effect model:\n\nkable(get_prior(model_meta_fixed, arm_data))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\n\n\n\nNote that brms requires the model formula and the data to which the model is being fitted to. This is due to fact that only model and data together define all parameters fully. For example, the model formula could contain a categorical variable and then only knowing all categories as defined by the data allows to define the full model with all parameters. For the fixed effect model only a single parameter, the intercept only term is defined. To now define a prior for the intercept, we may use the prior command as:\n\nprior_meta_fixed &lt;- prior(normal(0,2), class=\"Intercept\")\n\nThe prior for the random effects model is slightly more complicated as it involves a heterogeniety parameter \\(\\tau\\) (standard deviation of the study random effect):\n\nkable(get_prior(model_meta_random, arm_data))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\nstudy\n\n\n\n\n\ndefault\n\n\n\nsd\nIntercept\nstudy\n\n\n\n\n\ndefault\n\n\n\n\n\nGiven that the random effects model is a generalization of the fixed effect model, it is natural to write the prior in a way which expands the fixed effect case as:\n\nprior_meta_random &lt;- prior_meta_fixed +\n    prior(normal(0,1), class=\"sd\", coef=\"Intercept\", group=\"study\")\n\nThe logic to defined priors on specifc parameters is to use the different parameter identifiers as provided by the get_prior command. In this context class, coef and group is used for parameter class, parameter coefficient and grouping, respectivley. The identifiers resp, dlpar and nlpar are used in the context of multiple responses, multiple distribution parameters oe non-linear parameters, respectivley. It is preferable to be specific as above in the definition of priors while it is admissable to be less specific like:\n\nprior_meta_random &lt;- prior_meta_fixed +\n    prior(normal(0,1), class=\"sd\")\n\nThis statement assign to all random effect standard deviations of the model the same prior, which can be convenient in some situations.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#fitting-the-model",
    "href": "src/01b_basic_workflow.html#fitting-the-model",
    "title": "2  Basic workflow",
    "section": "2.5 Fitting the model",
    "text": "2.5 Fitting the model\nHaving defined the model formula, the family and the priors we can now fit the models with the brm command.\n\nfit_meta_fixed  &lt;- brm(model_meta_fixed, data=arm_data, prior=prior_meta_fixed,\n                       ## setup Stan sampler to be more conservative, but more robust\n                       control=list(adapt_delta=0.95),\n                       ## these options silence Stan\n                       refresh=0, silent=TRUE,\n                       seed=4658758)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nfit_meta_random  &lt;- brm(model_meta_random, data=arm_data, prior=prior_meta_random,\n                        control=list(adapt_delta=0.95),\n                        refresh=0, silent=TRUE,\n                        seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\n\nWhen calling brm a Stan model file will be created, compiled and run with the data provided. The arguments used are:\n\nformula is the first argument here. It specifies the model. Since we have in this case provided family as part of the formula, we do not need to specify it as argument to brm, which is also possible to do.\ndata the data used to fit.\nprior definition of the priors for all model parameters\ncontrol defines additional control arguments to the Stan sampler. A higher than standard adapt_delta (defaults to 0.8) causes the sampler to run less aggressively which makes the sampler more robust, but somewhat slower.\nrefresh & silent are used here to suppress Stan sampling output.\nseed sets the seed use for the fit.\n\nIn the course of an exploratory analysis one often wishes to modify a given model slightly to study, for example, the sensitivity to different assumptions. For this purpose the update mechanism is a convenient way to simply change certain arguments specified previously. To change the prior on the study level random effect parameter one may use:\n\nprior_meta_random_alt &lt;- prior_meta_fixed +\n    prior(normal(0,0.5), class=\"sd\", coef=\"Intercept\", group=\"study\")\n    \nfit_meta_random_alt  &lt;- update(fit_meta_random, prior=prior_meta_random_alt,\n                               control=list(adapt_delta=0.95),\n                               refresh=0, silent=TRUE,\n                               seed=6845736)\n\nThe desired updates require recompiling the model\n\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\n\nAs changing the prior results in this case in a need to recompile the model, the benefits of using update are not large in this case. However, whenever a model recompilation is not needed, then using update significantly speeds up the workflow as the compilation step is avoided, e.g. when looking at data subsets:\n\nprior_meta_random_alt &lt;- prior_meta_fixed +\n    prior(normal(0,0.5), class=\"sd\", coef=\"Intercept\", group=\"study\")\n    \nfit_meta_random_alt2  &lt;- update(fit_meta_random, newdata=slice_head(arm_data, n=4),\n                                control=list(adapt_delta=0.95),\n                                refresh=0, silent=TRUE,\n                                seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\n\nNow the fit starts instantly leading to a faster and more convenient modeling workflow.\nWhen working with brms you will encounter warnings now and then which originate from Stan itself. While brms attempts to add explanations to these warnings as to what they mean and how to avoid them, the Stan community has provided an online help-page on possible warnings occuring during a Stan fit.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-summary",
    "href": "src/01b_basic_workflow.html#model-summary",
    "title": "2  Basic workflow",
    "section": "2.6 Model summary",
    "text": "2.6 Model summary\nOnce models are fit, we obtain - by default - a posterior sample of the model. Given that priors are used in such an analysis it can be convenient to first consider the priors which were used to create a given fitting object like:\n\nkable(prior_summary(fit_meta_random))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nnormal(0, 2)\nIntercept\n\n\n\n\n\n\n\nuser\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\nstudy\n\n\n\n\n\ndefault\n\n\nnormal(0, 1)\nsd\nIntercept\nstudy\n\n\n\n\n\nuser\n\n\n\n\n\nAn even more explicit way to analyze the prior of a model is to sample it directly:\n\nprior_meta_random  &lt;- update(fit_meta_random, sample_prior=\"only\",\n                             control=list(adapt_delta=0.95),\n                             refresh=0, silent=TRUE,\n                             seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\n\nThis will sample the model in the usual way, but simply leave out the terms implied by the data likelihood. The resulting model sample can be analyzed in exactly the same way as the model posterior itself. This technique is called prior (predictive) checks.\nReturning to the model posterior, the obvious first thing to do is to simply print the results:\n\nprint(fit_meta_random)\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 1 + (1 | study) \n   Data: arm_data (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.39      0.22     0.04     0.90 1.00     1052     1256\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.11      0.20    -1.50    -0.70 1.00     1127     1227\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe see that we fitted the default 4 chains and ran each chain for 1000 warmup iterations and 2000 total iterations such that we are left with 4k iterations overall. brms reports by default as estimate the mean of the posterior with it’s standard error and the 95% credible interval. The Rhat column is a convergence diagnostic which should be close to 1.0. Values above 1.1 indicate non-convergence of the Markov Chain for the particular parameter. The additional two columns on bulk and tail ESS indicate statistical information on central moments and tail properties of the target quantitiy. The ESS is the effective sample size of the MC estimator, which assumes that the estimation error of the mean scales with \\(1/\\sqrt{ESS}\\). An ESS of ~200 is usually sufficient for a precise estimate of the mean. It is important to note that the Stan HMC sampler often requires fewer iterations (for which more time is spent) as compared to BUGS or JAGS in order to reach a comparable ESS. For more details on the ESS, please refer to the Stan reference manual on ESS.\nbrms supports the common R functions to extract model results:\n\nfitted to get the posterior estimates of expected values of for each data row (no sampling uncertainty)\npredict to get the posterior predictive estimates of the response for each data row (includes sampling uncertainty)\ncoef/ranef to get the random effect estimates including/excluding the linear predictor part\nmany more, please refer to the reference manual of brms\n\nOf note is the argument summary, which is used in a number of these functions. The default is set to TRUE such that the output are summaries of the posterior sample in the form of means, credible intervals, etc. When setting the argument summary=FALSE then the posterior sample is returned in the format of a matrix. Each row of the matrix is one iteration while the columns of the matrix run with the rows of the input data-set.\nFor example, we can compare the estimated mean response fo the two different models as:\n\nfitted(fit_meta_random)\n\n      Estimate Est.Error      Q2.5     Q97.5\n[1,] 24.422596  3.688009 17.310181 31.705575\n[2,] 11.498271  2.131884  7.613735 16.218670\n[3,] 16.123568  3.074037 11.162169 22.887761\n[4,]  9.410968  1.905810  5.770311 13.470430\n[5,] 37.780937  4.620416 29.236132 47.479274\n[6,]  5.382889  1.218695  3.209769  8.154431\n[7,] 13.513251  3.589218  7.002771 20.590007\n[8,]  9.320552  1.880953  6.039857 13.473071\n\nfitted(fit_meta_fixed)\n\n      Estimate Est.Error      Q2.5     Q97.5\n[1,] 26.533687 2.0380505 22.574910 30.640824\n[2,] 10.911049 0.8380768  9.283141 12.599965\n[3,] 12.646898 0.9714072 10.760004 14.604505\n[4,]  9.671157 0.7428408  8.228238 11.168151\n[5,] 34.468995 2.6475609 29.326285 39.804435\n[6,]  4.959568 0.3809440  4.219609  5.727257\n[7,] 19.342314 1.4856817 16.456477 22.336302\n[8,]  8.679243 0.6666520  7.384316 10.022699\n\n\nAs expected, the standard errors for the fixed effect analysis are considerably smaller.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-analysis",
    "href": "src/01b_basic_workflow.html#model-analysis",
    "title": "2  Basic workflow",
    "section": "2.7 Model analysis",
    "text": "2.7 Model analysis\nHow to analyze a given model obviously depends very much on details of the problem at hand. Often we wish to evaluate how well the model describes the problem data used to fit the model. Thus, the ability of the model to describe certain summaries of the data set accuratley is one way to critize the model. Such an approach requires to compare predictions of the data by the model \\(y_{rep}\\) to the actual data \\(y\\). This procedure is referred to as posterior predictive checks. A key feautre of the approach is to account for sampling uncertainty implied by the endpoint and sample size.\nFor a meta-analysis we may wish to check how well the summary statistics of the historical data is predicted by the model. The brms package integrates with the bayesplot package for the purpose of posterior predictive checks, which could look in this case like:\n\n# create intervals plot and label plot accordingly\npp_check(fit_meta_random, type=\"intervals\", ndraws=NULL) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"Random effects model\")\n\n\n\n\n\n\n\npp_check(fit_meta_fixed, type=\"intervals\", ndraws=NULL) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"Fixed effects model\")\n\n\n\n\n\n\n\n\nShown is a central 50% credible interval with a thick line, a thinner line shows the 90% credible interval, the open dot marks the median prediction and the filled dark dot marks the observed value of the data. We see is that in particular study 7 is predicted unsatisfactory by the fixed effect model. An additional interesting predictice check in this case is in view of a possibile use as historical control information part of a new study. While the above predictive check of the random effects was conditional on the fitted data, we can instead use the random effects model and predict each study as if it were a new study:\n\n# create intervals plot and label plot accordingly\npp_arm_data_new &lt;- posterior_predict(fit_meta_random,\n                                     newdata=mutate(arm_data, study=paste0(\"new_\", study)),\n                                     allow_new_levels=TRUE,\n                                     sample_new_levels=\"gaussian\")\n\n# use bayesplot::ppc_intervals directly here\nppc_intervals(y=arm_data$r, yrep=pp_arm_data_new) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check - new studies\", \"Random effects model\")\n\n\n\n\n\n\n\n\nNow the credible intervals are wider as these contain additional variability. As for study 7 the random effect is not anymore conditioned on the data of the study, we see again that the model struggles to account for the study nicely. However, with the additional heterogeniety the result of study 7 is at least plausible given that it is contained in the 90% credible interval.\nIn the above discussion we have ensured the comparability of the plots via matching the axis limits. A more straightforward is a side by side comparison of the models, which can be achieved like:\n\n# use bayesplot::ppc_intervals_data directly here\nmodel_cmp &lt;- bind_rows(random_new=ppc_intervals_data(y=arm_data$r, yrep=pp_arm_data_new),\n                       random=ppc_intervals_data(y=arm_data$r, yrep=posterior_predict(fit_meta_random)),\n                       fixed=ppc_intervals_data(y=arm_data$r, yrep=posterior_predict(fit_meta_fixed)),\n                       .id=\"Model\") %&gt;%\n    # add in labels into the data-set\n    left_join(select(mutate(arm_data, x=1:8), x, study), by=\"x\")\n\nggplot(model_cmp, aes(study, m, colour=Model)) +\n    geom_pointrange(aes(ymin=ll, ymax=hh), position=position_dodge(width=0.4)) +\n    geom_point(aes(y=y_obs), colour=\"black\") +\n    ylab(\"Number of Responders\") +\n    xlab(\"Study\") +\n    coord_flip() +\n    theme(legend.position=\"right\",\n          # suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"All models\")\n\n\n\n\n\n\n\n\nA more formal way to compare these models is to consider their ability to predict new data. In absence of new data we may instead turn to scores which evaluate the ability of the model to predict data which has been left out when fitting the model. The leave-one-out scores can be calculated using fast approximations (see loo command) whenever the data-set is large enough. We refer the reader to the case study on Dose finding where these model comparisons are discussed in greater detail.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01c_priors.html",
    "href": "src/01c_priors.html",
    "title": "3  Model setup & priors",
    "section": "",
    "text": "Within the Bayesian regression modeling in Stan framework brms priors are required to perform inference. This introductory material is intended to provide some pragmatic considerations on how to setup priors and point readers to material to follow-up on. The case studies themselves provide details on setting up priors for the respective problem. By default these priors have been chosen to have minimal impact on the posterior whenever appropriate and also ensure stable numerical inference. As a consequence, the results from the case studies will resemble respective Frequentist model results in most cases. Nonetheless, priors are defined explicitly for all models, since these are an integral part of any Bayesian analysis. This can be easily seen by considering Bayes rule to obtain the posterior:\n\\[ p(\\theta | y) = \\frac{p(y|\\theta) \\, p(\\theta)}{p(y)} \\propto p(y|\\theta) \\, p(\\theta)\\]\nThe marginal likelihood term \\(p(y)\\) can be dropped, since this is merely a normalization constant given that we condition the inference on the observed data \\(y\\). What is left is hence the product of the likelihood \\(p(y|\\theta)\\) multiplied by the prior \\(p(\\theta)\\). The posterior information on \\(\\theta\\) is hence provided equally by likelihood and prior.\nFrom a practical perspective one may ask under which circumstances it actually matters which prior we set. In many cases the posterior is dominated by the data, which means that the likelihood term \\(p(y|\\theta)\\) is much larger than the prior term \\(p(\\theta)\\). This is the case for most case studies. It is tempting to drop the prior in these cases and not worry about it. However, this is not recommended as it is in many instances of interest to study a model with small sample sizes eventually (by applying the model to subsets, increasing model complexity, etc.). Another practical aspect is the numerical stability and quality of the Markov Chain Monte Carlo (MCMC) sample we obtain from Stan. Without a prior the inference problem becomes much harder to solve. That is, the Markov Chain Monte Carlo (MCMC) sampler Stan has to consider the full sampling space of the prior. Dropping the prior entirely implies an improper prior on the sampling space which becomes un-countably infinitely large. Rather than dropping the prior entirely we strongly recommend to consider so-called weakly informative priors. While no formal definition is given in the literature, these priors aim to identify the scale of parameters. This requires a basic understanding of the parameters for which priors are being defined. Thus, a prior can only be understood in the overall context of the likelihood, parametrization and problem at hand (see also Gelman, Simpson, and Betancourt 2017). For most statistical analyses this means to consider the endpoint and the applied transformations.\n\nTo just get started with brms one may choose to not specify priors when calling brm. Doing so will let brms provide in most cases reasonable default priors. These default priors are intended to avoid any influence on the calculated posterior. Hence, the results are fully data driven and will be very close to the respective Frequentist maximum likelihood inference result. However, the default prior is not guaranteed to stay stable between releases and can thus change whenever the brms version changes.\nGiven that any Bayesian analysis requires a prior, we recommend to always explicitly define these - even if these just repeat the default prior from brms, which one can easily obtain. Here we use as example binomially distributed data (responder in a control arm) and we use the simplest possible model, which will pool the information across the different studies (in practice one would allow for between-trial heterogeneity):\n\nmodel &lt;- bf(r | trials(n) ~ 1, family=binomial)\n\n## with brms &gt;= 2.21.0 we can  directly write... \n## default_prior(model, data=RBesT::AS)\n\n## a workaround is to create an empty brms model and obtain the\n## defined prior like\nprior_summary(brm(model, data=RBesT::AS, empty=TRUE)) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\n\n\n\nNote that we have defined first the model via the bf call. This defines the linear predictor and the likelihood using the family argument of bf. We recommend to always explicitly do this step first as it defines the likelihood and the model parametrization in one step. Choosing these is a critical first step in defining the model and, most importantly, the chosen priors can only be understood in the context of the model (likelihood and parametrization) (see Gelman, Simpson, and Betancourt 2017).\nRather than running the model with no explicit prior definition we encourage users to explicitly define the priors used for the analysis - even if these are simply the default priors:\n\nmodel_def &lt;- bf(r | trials(n) ~ 1, family=binomial)\nmodel_prior &lt;- prior(student_t(3, 0, 2.5), class=Intercept)\n\nfit &lt;- brm(model_def, data=RBesT::AS, prior=model_prior, seed=467657, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nfixef(fit)\n\n           Estimate Est.Error      Q2.5     Q97.5\nIntercept -1.106901 0.1062962 -1.314909 -0.907616\n\n\nThe provided default prior is indeed not informative, as can be seen by comparing the posterior estimate to the respective Frequentist fit from glm:\n\nfit_freq &lt;- glm(cbind(r, n-r) ~ 1, data=RBesT::AS, family=binomial)\n## intercept estimate\ncoef(fit_freq)\n\n(Intercept) \n   -1.11165 \n\n## standard error estimate\nsqrt(vcov(fit_freq)[1,1])\n\n[1] 0.1022971\n\n\nIn this case the data with a total of 513 subjects dominates the posterior. Nonetheless, it is illustrative to consider in more detail the prior here as an example. Importantly, the intercept of the model is defined on the logit scale of the response probability as it is common for a standard logistic regression. The transformation changes the scale from the interval 0 to 1 to a new scale which covers the entire space of reals. While formally very large or very small logits are admissible, the range from 5% to 95% response rate corresponds to -3 to 3 on the logit space approximately. Thus, logit values smaller or greater than these values would be considered extreme effects.\nWhen defining a prior for a model we would typically want to compare different choices of the prior to one another. As example prior for the intercept only model we may compare here three different priors: (i) a very wide prior \\(\\mbox{Normal}(0,10^2)\\), (2) the default brms prior and (3) the density \\(\\mbox{Normal}(0,2^2)\\) as used as default prior in RBesT for this problem. In a first step we sample these priors and assess them graphically. We could use brms to sample these priors directly with the argument sample_prior=\"only\", but we instead use basic R functions for simplicity:\n\n\nShow the code\nnum_draws &lt;- 1E4\npriors_cmp &lt;- tibble(case=c(\"wide\", \"brms\", \"RBesT\"),\n                     density=c(\"N(0,10^2)\", \"S(3, 0, 2.5^2)\", \"N(0, 2^2)\"),\n                     prior=rvar(cbind(rnorm(num_draws, 0, 10),\n                                      rstudent_t(num_draws, 3, 0, 2.5),\n                                      rnorm(num_draws, 0, 2))))\n\nkable(priors_cmp)\n\n\n\n\n\ncase\ndensity\nprior\n\n\n\n\nwide\nN(0,10^2)\n0.105 ± 10.1\n\n\nbrms\nS(3, 0, 2.5^2)\n0.022 ± 4.5\n\n\nRBesT\nN(0, 2^2)\n0.017 ± 2.0\n\n\n\n\n\nShow the code\npriors_logit &lt;- priors_cmp |&gt;\n    ggplot(aes(y=case, xdist=prior)) +\n    stat_slab() +\n    xlab(\"Intercept\\nlinear predictor\") +\n    ylab(NULL) +\n    labs(title=\"Prior logit scale\") +\n    coord_cartesian(xlim=c(-30, 30))\n\npriors_response &lt;- priors_cmp |&gt;\n    ggplot(aes(y=case, xdist=rdo(inv_logit(prior)))) +\n    stat_slab() +\n    xlab(\"Intercept\\nresponse scale\") +\n    ylab(NULL) +\n    labs(title=\"Prior response scale\") +\n    coord_cartesian(xlim=c(0, 1))\n\nbayesplot_grid(priors_logit, priors_response, grid_args=list(nrow=1))\n\n\n\n\n\n\n\n\n\nAs one can easily see, the wide prior has density at very extreme logit values of -20 and 20. In comparison, the brms and the RBesT default prior place most probability mass in the vicinity of zero. While the wide prior may suggest to be very non-informative it’s meaning becomes clearer when back transforming to the original response scale from 0 to 1. It is clear that the wide prior implies that we expect extreme response rates of either 0 or 1. In contrast, the RBesT default prior has an almost uniform distribution on the 0 to 1 range while the default brms prior has some “U” shape. To now further discriminate these priors a helpful concept is to consider interval probabilities for pre-defined categories. The definition of cut-points of the categories is problem specific, but can be defined with usual common sense for most endpoints. The critical step here is in defining these categories and documenting these along with the analysis. Here we categorize the response as extreme 0-1% & 99-100%, very small 1-5% & 95-99%, small 5-10% & 90-95% and moderate for the remainder. Comparing then these priors gives:\n\n\nShow the code\nresponse_category &lt;- function(r) {\n    fct_rev(cut(pmin(r, 1-r), c(0, 0.01, 0.05, 0.1, 0.5), labels=c(\"extreme\", \"very_small\", \"small\", \"moderate\")))\n}\n\npriors_cmp |&gt; unnest_rvars()|&gt; \n    ggplot(aes(x=case, fill=response_category(inv_logit(prior)))) +\n    geom_bar(position=\"fill\") +\n    xlab(\"Prior\") +\n    ylab(\"Probability\") +\n    labs(title=\"Prior interval probabilities\", fill=\"Category\") +\n    scale_y_continuous(breaks=seq(0,1,by=0.2)) +\n    theme(legend.position=\"right\")\n\n\n\n\n\n\n\n\n\nShow the code\nmutate(priors_cmp,\n       prior_response=rfun(inv_logit)(prior),\n       summarise_draws(prior_response,\n                       extreme=~100*E(.x &lt; 0.01 | .x &gt; 0.99),\n                       very_small=~100*E( (0.01 &lt;= .x & .x &lt; 0.05) | (0.95 &lt; .x & .x &lt;= 0.99) ),\n                       small=~100*E( (0.05 &lt;= .x & .x &lt; 0.1) | (0.9 &lt; .x & .x &lt;= 0.95) ),\n                       moderate=~100*E( (0.1 &lt;= .x & .x &lt; 0.9) )\n                       )) |&gt;\n    select(case, density, extreme, very_small, small, moderate) |&gt;\n    kable(digits=1, caption=\"Interval probabilities per prior given in percent\")\n\n\n\nInterval probabilities per prior given in percent\n\n\ncase\ndensity\nextreme\nvery_small\nsmall\nmoderate\n\n\n\n\nwide\nN(0,10^2)\n64.9\n12.0\n5.6\n17.5\n\n\nbrms\nS(3, 0, 2.5^2)\n16.4\n16.2\n12.7\n54.7\n\n\nRBesT\nN(0, 2^2)\n2.2\n11.8\n12.8\n73.3\n\n\n\n\n\nIt is apparent that the seemingly non-informative prior “wide” is in fact an informative prior implying that the expected response rates are extreme. This is the consequence of transforming to the logit scale.\nIn practice one would most certainly not fit the intercept only model, which pools the information. The RBesT::AS data set is in fact the standard example for the use of historical control data.\n\nkable(RBesT::AS)\n\n\n\n\nstudy\nn\nr\n\n\n\n\nStudy 1\n107\n23\n\n\nStudy 2\n44\n12\n\n\nStudy 3\n51\n19\n\n\nStudy 4\n39\n9\n\n\nStudy 5\n139\n39\n\n\nStudy 6\n20\n6\n\n\nStudy 7\n78\n9\n\n\nStudy 8\n35\n10\n\n\n\n\n\nTo use this data as historical control data one uses instead of the intercept only model a random intercept model, which casts this into a meta-analtyic model:\n\nmodel_meta_def &lt;- bf(r | trials(n) ~ 1 + (1 | study), family=binomial)\nmodel_meta_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 0.5), class=sd, coef=Intercept, group=study)\n\nfit_meta &lt;- brm(model_meta_def, data=RBesT::AS, prior=model_meta_prior,\n                seed=982345, refresh=0, control=list(adapt_delta=0.95))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\nsummary(fit_meta)\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 1 + (1 | study) \n   Data: RBesT::AS (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.34      0.17     0.04     0.73 1.01     1186     1066\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.11      0.17    -1.47    -0.75 1.00     1622     1731\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe meta-analytic model is generative in the sense of allowing to predict the mean response rate for future studies by way of the hierarchical model structure. Now the importance of the prior on the overall intercept becomes more relevance as the information from each study is discounted through the random effects model leading to less data to estimate the parameter in comparison to the full pooling model. However, the key parameter in this model is the between-trial heterogeneity parameter. In the example a half-normal prior with scale 1 is used (brms knows that the parameter must be positive and hence truncates the prior at zero automatically). This distribution of a half-normal density has been studied extensively in the literature and found to be a robust choice in a wide range of problems. For this reason, it is a very rational choice to use this prior in the current (and future) analyses. The choice of the scale of 0.5 is often referred to as a conservative choice in this problem while 1 is a very conservative choice and 0.25 a less conservative choice. A complication with the hierarchical model is that \\(\\tau\\) itself implies a distribution. To simplify this, we first consider the case of a known between-trial heterogeneity parameter and study what this implies. For a known \\(\\tau\\) the range of implied log odds can be characterized by the respective 95% probability mass range given by \\(2\n\\cdot 1.96 \\, \\tau\\), which is interpret-able as the largest log odds ratio. Recalling that the between-study heterogeneity parameter controls the (random) differences in the logits of response rates between studies, it is helpful to consider the distribution of implied log odds ratios between random pairs of two studies. These differences (log odds ratios) have a distribution of \\(\\mbox{Normal}(0, (\\sqrt{2} \\,\n\\tau)^2)\\). Considering the absolute value of these differences, the distributions becomes a half-normal with scale \\(\\sqrt{2} \\, \\tau\\), which has it’s median value at \\(1.09 \\, \\tau\\). This results for a range of values of \\(\\tau\\) on the respective odds scale to:\n\n\n\n\n\ntau\nlargest_odds_ratio\nmedian_odds_ratio\n\n\n\n\n0.000\n1.000\n1.000\n\n\n0.125\n1.632\n1.146\n\n\n0.250\n2.664\n1.313\n\n\n0.500\n7.099\n1.725\n\n\n1.000\n50.400\n2.974\n\n\n1.500\n357.809\n5.129\n\n\n2.000\n2540.205\n8.846\n\n\n\n\n\nIn (Neuenschwander and Schmidli 2020) suggest the categorization of the between-study heterogeneity \\(\\tau\\) parameter the values of \\(1\\) for being large, \\(0.5\\) substantial, \\(0.25\\) moderate, and \\(0.125\\) small. Considering these with the table above qualifies these categories as plausible. An alternative approach to the categorization is to consider what is an extreme value for \\(\\tau\\) (like unity) and then choose the prior on \\(\\tau\\) such that a large quantile (like the 95% quantile) corresponds to this extreme value.\nWith the categorization of specific values for \\(\\tau\\) we can now proceed and consider the interval probabilities for these categories for the different choices of \\(\\tau \\sim \\mbox{Normal}^+(s^2)\\) for \\(s=1\\) and \\(s=1/2\\).\n\n\nShow the code\npriors_cmp_tau &lt;- tibble(case=c(\"less_conservative\", \"conservative\", \"very_conservative\"),\n                         density=c(\"HN(0, (1/4)^2)\", \"HN(0, (1/2)^2)\", \"HN(0, 1^2)\"),\n                         prior=rvar(cbind(abs(rnorm(num_draws, 0, 0.25)),\n                                          abs(rnorm(num_draws, 0, 0.5)),\n                                          abs(rnorm(num_draws, 0, 1))))\n                         )\n\nkable(priors_cmp_tau)\n\n\n\n\n\ncase\ndensity\nprior\n\n\n\n\nless_conservative\nHN(0, (1/4)^2)\n0.20 ± 0.15\n\n\nconservative\nHN(0, (1/2)^2)\n0.40 ± 0.30\n\n\nvery_conservative\nHN(0, 1^2)\n0.81 ± 0.61\n\n\n\n\n\nShow the code\npriors_hetero &lt;- priors_cmp_tau |&gt;\n    ggplot(aes(y=case, xdist=prior)) +\n    stat_slab() +\n    xlab(\"Between-study heterogeneity\") +\n    ylab(NULL) +\n    scale_x_continuous(breaks=0:3) +\n    labs(title=\"Between-study\\nheterogeneity\") +\n    coord_cartesian(xlim=c(0, 3))\n\npriors_lor &lt;- priors_cmp_tau |&gt;\n    mutate(lor1=rvar_rng(rnorm, n=n(), mean=0, prior),\n           lor2=rvar_rng(rnorm, n=n(), mean=0, prior)) |&gt;\n    ggplot(aes(y=case, xdist=lor1-lor2)) +\n    stat_slab() +\n    xlab(\"Log-odds ratio\") +\n    ylab(NULL) +\n    scale_x_continuous(breaks=-5:5) +\n    labs(title=\"Log-odds ratio\\nof random pair\") +\n    coord_cartesian(xlim=c(-3, 3))\n\nbayesplot_grid(priors_hetero, priors_lor, grid_args=list(nrow=1))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntau_category &lt;- function(tau) {\n    fct_rev(cut(tau, c(0, 0.125, 0.25, 0.5, 1, Inf), labels=c(\"small\", \"moderate\", \"substantial\", \"large\", \"very_large\")))\n}\n\npriors_cmp_tau |&gt; unnest_rvars()|&gt; \n    ggplot(aes(x=case, fill=tau_category(prior))) +\n    geom_bar(position=\"fill\") +\n    xlab(\"Prior\") +\n    ylab(\"Probability\") +\n    labs(title=\"Prior interval probabilities\", fill=\"Category\") +\n    ##scale_y_continuous(breaks=seq(0,1,by=0.2)) +\n    theme(legend.position=\"right\")\n\n\n\n\n\n\n\n\n\nShow the code\nmutate(priors_cmp_tau,\n       summarise_draws(prior,\n                       small=~100*E(.x &lt; 0.125),\n                       moderate=~100*E( 0.125 &lt;= .x & .x &lt; 0.25 ),\n                       substantial=~100*E( 0.25 &lt;= .x & .x &lt; 0.5 ),\n                       large=~100*E( 0.5 &lt;= .x & .x &lt; 1 ),\n                       very_large=~100*E( .x &gt;= 1 )\n                       )) |&gt;\n    select(case, density, small, moderate, substantial, large, very_large) |&gt;\n    kable(digits=1, caption=\"Interval probabilities per prior given in percent\")\n\n\n\nInterval probabilities per prior given in percent\n\n\n\n\n\n\n\n\n\n\n\ncase\ndensity\nsmall\nmoderate\nsubstantial\nlarge\nvery_large\n\n\n\n\nless_conservative\nHN(0, (1/4)^2)\n37.9\n30.2\n27.4\n4.5\n0.0\n\n\nconservative\nHN(0, (1/2)^2)\n20.2\n18.4\n28.9\n27.9\n4.6\n\n\nvery_conservative\nHN(0, 1^2)\n9.4\n9.7\n18.3\n30.4\n32.1\n\n\n\n\n\nWe can see that the “conservative” choice has an about 5% tail probability exceeding the value of large. This implies that some degree of homogeneity between the studies is given. This is usually the case whenever meta-analyses are conducted, since the inclusion & exclusion criteria for studies are aligned to a certain degree in order to ensure that a similar patient population is included in the analysis. In contrast, the “very conservative” choice admits with 30% probability mass the possibility of values in the domain “very large” for which practically no borrowing of historical information occurs. The “less conservative” choice on the hand has 5% tail probability above substantial. This way a large heterogeneity is considered to be unlikely as it can be the case for twin Phase III studies, for example.\nAdditional literature for consideration:\n\nEmpirical priors study for HTA treatment effect evaluation by the German IQWIG (Lilienthal et al. 2023)\nEmpirical priors for meta-analyses organized in disease specific manner (Turner et al. 2015)\nEndpoint specific considerations for between-trial heterogeneity parameter priors in random effect meta-analyses (Röver et al. 2021)\nComprehensive introductory book to applied Bayesian data analysis with detailed discussion on many examples (Gelman et al. 2014)\nLive wiki document maintained by Stan user community (heavily influenced by Andrew Gelman & Aki Vehtari) (Stan 2024)\nPrior strategy based on nested modeling considerations (penalization of more complex models), (Simpson et al. 2014)\nGlobal model shrinkage regularized horseshoe prior (Piironen and Vehtari 2017) or R2D2 prior (overall \\(R^2\\)) (Zhang et al. 2022)\n\n\n\n\n\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. Bayesian Data Analysis. Chapman Texts in Statistical Science Series. Third edit. https://doi.org/10.1007/s13398-014-0173-7.2.\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The Prior Can Often Only Be Understood in the Context of the Likelihood.” Entropy 19 (October). https://doi.org/10.3390/e19100555.\n\n\nLilienthal, Jona, Sibylle Sturtz, Christoph Schürmann, Matthias Maiworm, Christian Röver, Tim Friede, and Ralf Bender. 2023. “Bayesian Random‐effects Meta‐analysis with Empirical Heterogeneity Priors for Application in Health Technology Assessment with Very Few Studies.” Research Synthesis Methods, December. https://doi.org/10.1002/jrsm.1685.\n\n\nNeuenschwander, Beat, and Heinz Schmidli. 2020. “Use of Historical Data.” https://CRAN.R-project.org/package=RBesT.\n\n\nPiironen, Juho, and Aki Vehtari. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” Electronic Journal of Statistics 11 (January): 5018–51. https://doi.org/10.1214/17-EJS1337SI.\n\n\nRöver, Christian, Ralf Bender, Sofia Dias, Christopher H. Schmid, Heinz Schmidli, Sibylle Sturtz, Sebastian Weber, and Tim Friede. 2021. “On Weakly Informative Prior Distributions for the Heterogeneity Parameter in Bayesian Random-Effects Meta-Analysis.” Research Synthesis Methods 12 (January): 448–74. https://doi.org/10.1002/jrsm.1475.\n\n\nSimpson, Daniel P., Håvard Rue, Thiago G. Martins, Andrea Riebler, and Sigrunn H. Sørbye. 2014. “Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.” Statistical Science 32 (February): 1–28. https://doi.org/10.1214/16-STS576.\n\n\nStan. 2024. “Prior Choice Recommendations.” 2024. https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations.\n\n\nTurner, Rebecca M, Dan Jackson, Yinghui Wei, Simon G Thompson, and Julian P T Higgins. 2015. “Predictive Distributions for Between-Study Heterogeneity and Simple Methods for Their Application in Bayesian Meta-Analysis.” Statistics in Medicine 34: 984–98. https://doi.org/10.1002/sim.6381.\n\n\nZhang, Yan Dora, Brian P. Naughton, Howard D. Bondell, and Brian J. Reich. 2022. “Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior.” Journal of the American Statistical Association 117: 862–74. https://doi.org/10.1080/01621459.2020.1825449.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model setup & priors</span>"
    ]
  },
  {
    "objectID": "src/02_case_studies.html",
    "href": "src/02_case_studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Problem\nTechnique\n\n\n\n\n4  Use of historical control data\nnested random effects\n\n\n5  Meta-analysis to estimate treatment effects\naggregate data modeling & varying exposure times of count data\n\n\n6  Use of historical control data with stratification\nmeta-analysis with covariates & use of mixture priors\n\n\n7  Probability of success from a single arm trial\nprior elicitation and use of RBesT mixtures as priors\n\n\n8  Dose finding\nnon-linear models\n\n\n9  Oncology dose escalation\nconstrained parameters\n\n\n10  Time-to-event modelling in Oncology dose escalation\npiece-wise constant survival model with Poisson regression & non-linear link function\n\n\n11  Multiple imputation\nmulti-variate outcome modeling\n\n\n12  Longitudinal data\nlongitudinal modeling with different covariance structures (MMRM)\n\n\n13  Bayesian Mixed effects Model for Repeated Measures\nunstructured MMRM for a continuous endpoint\n\n\n14  Time-to-event data\nparametric time-to-event modeling with customized parametrization by using user-defined contrasts\n\n\n15  Network meta-analysis\narm based network meta-analysis",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html",
    "href": "src/02a_meta_analysis.html",
    "title": "4  Use of historical control data",
    "section": "",
    "text": "4.1 Background\nGiven the relevance of the use of historical control data problem for drug development, a full R package RBesT (R Bayesian evidence synthesis tools) is available on CRAN. Here we will re-implement the example of the vignette of RBesT for the binary case and will illustrate how brms can be used in a more complex setting as a case study. In particular, we are going to assume as a complication that the historical trial data has been collected in specific regions of the world and how this can be used to borrow strength between regions. As a simplifying assumption it is assumed that trials are nested within regions thereby implying that trials are conducting exclusively in specific regions.\nFor details on the RBesT R package, please refer to",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#background",
    "href": "src/02a_meta_analysis.html#background",
    "title": "4  Use of historical control data",
    "section": "",
    "text": "Weber et al. (2021) doi:10.18637/jss.v100.i19 for details on applying the RBesT package, and\nNeuenschwander et al. (2010) doi:10.1177/1740774509356002 and\nSchmidli et al. (2014) doi:10.1111/biom.12242 for details on the MAP methodology.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#data",
    "href": "src/02a_meta_analysis.html#data",
    "title": "4  Use of historical control data",
    "section": "4.2 Data",
    "text": "4.2 Data\nA Phase II study is planned to evaluate the efficacy of a test treatment in a randomized comparison with placebo in the disease ankylosing spondilityis. At the design stage of the trial control group data were available from a total of eight historical studies.\nThis data-set is part of the RBesT package as the AS data-set and here we add as additional column a randomly assigned region variable:\n\nlibrary(RBesT)\nAS_region &lt;- bind_cols(AS, region=sample(c(\"asia\", \"europe\", \"north_america\"), 8, TRUE))\nkable(AS_region)\n\n\n\n\nstudy\nn\nr\nregion\n\n\n\n\nStudy 1\n107\n23\nasia\n\n\nStudy 2\n44\n12\nnorth_america\n\n\nStudy 3\n51\n19\nasia\n\n\nStudy 4\n39\n9\nnorth_america\n\n\nStudy 5\n139\n39\neurope\n\n\nStudy 6\n20\n6\neurope\n\n\nStudy 7\n78\n9\neurope\n\n\nStudy 8\n35\n10\neurope\n\n\n\n\n\nThe total number of 513 patients in the 8 trials is quite substantial.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#model-description",
    "href": "src/02a_meta_analysis.html#model-description",
    "title": "4  Use of historical control data",
    "section": "4.3 Model description",
    "text": "4.3 Model description\nThe RBesT package implements the MAP approach following a standard generalized linear modeling framework for a random-effects meta-analysis:\n\n\\(Y\\) is the (control) group summary data for \\(H\\) historical trials\n\\(Y_{h}|\\theta_{h} \\sim f(\\theta_{h})\\)\n\\(g(\\theta_{h}) = \\beta + \\eta_h\\)\n\\(\\eta_h|\\tau \\sim \\mbox{Normal}(0, \\tau^2)\\)\n\\(f\\) likelihood: Binomial, Normal (known \\(\\sigma\\)) or Poisson\n\\(g\\) link function for each likelihood \\(f\\): \\(\\mbox{logit}\\), identity or \\(\\log\\)\n\\(\\beta\\) population mean with prior \\(\\mbox{Normal}(m_{\\beta}, s_{\\beta}^2)\\)\n\\(\\tau\\) between-trial heterogeneity with prior \\(P_\\tau\\)\n\nThe priors used for this data-set will be:\n\n\\(\\beta \\sim \\mbox{Normal}(0, 2^2)\\)\n\\(\\tau \\sim \\mbox{Normal}^+(0, 1)\\)\n\nWe will first run the analysis with the RBesT command gMAP. As a next step we will convert the analysis to use brms for the inference. Finally, we will add an additional random effect for the region \\(j\\) and treat the random effect for the studies to be nested within the region. As the more general model requires two levels of random effects, it is outside the possible models of RBesT. Such a more general region specific model can be useful in various situations whenever we wish to borrow strength across regions. Denoting with \\(j\\) specific regions, the more general model is then:\n\n\\(Y_{h,j}|\\theta_{h,j} \\sim f(\\theta_{h,j})\\)\n\\(g(\\theta_{h,j}) = \\beta + \\eta_h + \\nu_j\\)\n\\(\\eta_h|\\tau \\sim \\mbox{Normal}(0, \\tau^2)\\)\n\\(\\nu_j|\\omega \\sim \\mbox{Normal}(0, \\omega^2)\\)\n\nIn our case study we make a simplifying assumption that any trial \\(h\\) is run entirely within a given region \\(j\\). Therefore we have a nested structure (trials within regions) such that no correlation is modeled between region and trial. This would be different if some trials were run across different regions and trial results would be reported by region.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#implementation",
    "href": "src/02a_meta_analysis.html#implementation",
    "title": "4  Use of historical control data",
    "section": "4.4 Implementation",
    "text": "4.4 Implementation\nWith the gMAP command in RBesT we can obtain MCMC samples from posterior for the first model as follows:\n\nset.seed(34767)\nmap_mc_rbest &lt;- gMAP(cbind(r, n-r) ~ 1 | study,\n                     family=binomial,\n                     data=AS_region,\n                     tau.dist=\"HalfNormal\", tau.prior=1,\n                     beta.prior=cbind(0,2))\n\nmap_mc_rbest\n\nGeneralized Meta Analytic Predictive Prior Analysis\n\nCall:  gMAP(formula = cbind(r, n - r) ~ 1 | study, family = binomial, \n    data = AS_region, tau.dist = \"HalfNormal\", tau.prior = 1, \n    beta.prior = cbind(0, 2))\n\nExchangeability tau strata: 1 \nPrediction tau stratum    : 1 \nMaximal Rhat              : 1 \n\nBetween-trial heterogeneity of tau prediction stratum\n  mean     sd   2.5%    50%  97.5% \n0.3730 0.2040 0.0441 0.3490 0.8450 \n\nMAP Prior MCMC sample\n  mean     sd   2.5%    50%  97.5% \n0.2560 0.0863 0.1090 0.2470 0.4710 \n\n\nUsing brms we now specify the MAP model step by step. Binomial data is specified slightly different in brms. We first define the model:\n\nmodel &lt;- bf(r | trials(n) ~ 1 + (1 | study), family=binomial)\n\nThe left hand side of the formula, r | trials(n) ~ ..., denotes with r the data being modeled - the number of responders - and adds with a bar | additional information on the response, which are the number of overall trials, needed to interpret the binomial likelihood.\nWith the model (and data) being defined, we are left to specify the model priors. With the help of the call\n\nget_prior(model, AS_region)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd           study                  0   \n student_t(3, 0, 2.5)        sd Intercept study                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n\n\nwe can ask brms as to what model parameters it has detected for which priors should be specified. In this example, we need to define the population mean intercept (\\(\\beta\\)) and the between-study heterogeneity parameter (\\(\\tau\\)):\n\nmodel_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=sd, coef=Intercept, group=study)\n\nNow we are ready to run the model in brms (we are setting refresh=1000 to suppress most progress output):\n\nmap_mc_brms  &lt;- brm(model, AS_region, prior=model_prior,\n                    seed=4767, refresh=1000)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\n\nThe model is compiled and then run. Occasionally one observes a warning on divergent transitions after warmup reported like:\n\n## Warning: There were 1 divergent transitions after warmup.\n\nThis is caused in this case by the choice of very conservative priors, which lead to a difficult to sample posterior. As a quick fix we may reduce the aggressiveness of the sampler and increase the sampler parameter on the target acceptance probability adapt_delta from it’s default value \\(0.8\\) to a value closer to the maximum possible value of \\(1.0\\). For most analyses with weak priors using a value of \\(0.95\\) can be used as a starting value. This is at the cost of some sampling speed as the sampler will take smaller steps, but the choice of a higher than default acceptance probability results in more robust inference and avoids in many instances the warning about divergences. For a more comprehensive overview on possible warnings, their meanings and how to address these, please refer to the online help of the Stan project on possible Stan stampler warnings and messages.\nIn order to also avoid having to compile the Stan code for the model once more, we use the update functionality of brms:\n\nmap_mc_brms_2 &lt;- update(map_mc_brms, control=list(adapt_delta=0.95),\n                        # the two options below only silence Stan sampling output\n                        refresh=0, silent=0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\nmap_mc_brms_2\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 1 + (1 | study) \n   Data: AS_region (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.38      0.22     0.04     0.89 1.00      978     1021\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.10      0.19    -1.48    -0.70 1.01     1059     1173\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can see that the estimate of the between-study heterogeneity \\(\\tau\\) is very similar between RBesT and brms. However, the MAP prior is not apparent from the output of brms directly (as it’s not designed with this specific application in mind).\nTo obtain the MAP prior from brms, we have to predict the response rate of a new study. To do so, a new data set with the same columns as the modeling data sets needs to be created.\n\nAS_region_new &lt;- data.frame(study=\"new_study_asia\", r=0, n=6, region=\"asia\")\npost_map_mc_brms &lt;- posterior_linpred(map_mc_brms_2,\n                                      newdata=AS_region_new,\n                                      # apply inverse link function\n                                      transform=TRUE,\n                                      # allows new studies\n                                      allow_new_levels = TRUE,\n                                      # and samples these according to the model\n                                      sample_new_levels = \"gaussian\"\n                                      )\n# Let's have a look at what we got:\nstr(post_map_mc_brms)\n\n num [1:4000, 1] 0.258 0.221 0.218 0.236 0.278 ...\n\n\nModel outputs are returned in the standard format of a matrix which contains the model simulations. While the rows label the draws, the columns go along with the rows of the input data set. As in this case we have as input data set a 1-row data frame AS_region_new corresponding to predictions for a (single) new study, the output is a 1 column matrix with 4000 rows, since 4000 draws in total were obtained from the sampler run with 4 chains and 1000 draws per chain from the sampling phase.\nNote the following important arguments used to obtain the posterior:\n\ntransform=TRUE applies automatically the inverse link function such that we get response rates rather than logit values.\nallow_new_levels=TRUE is needed to instruct brms that new levels of the fitted random effects are admissible in the data. In this case we sample a new study random effect level.\nsample_new_levels=\"gaussian\" ensures that the new random effect is sampled according to normal distributions as specified with the model. The default option \"uncertainty\" samples for each draw from the fitted random effect levels one realization, which is essentially bootstrapping random effects on the level of posterior draws. The option \"old_levels\" samples a random effect level and substitutes all draws for the new level corresponding to bootstrapping the existing levels. While this avoids normality assumptions, it can only work well in situations with many levels of the random effect. The option \"gaussian\" is for most models the preferred choice and for more details, please refer to the brms help page on prepare_predictions.\n\nA convenient way to get a summary of the samples is to use the summarize_draws function from the posterior package (used as a helper package in brms already):\n\nsummarize_draws(post_map_mc_brms)\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ...1     0.259  0.249 0.0864 0.0663 0.135 0.415  1.00    2552.    2242.\n\n\nThese estimates are now very similar to the results reported from RBesT reported above (up to sampling error).\nExpanding the model to include region would only be possible in RBesT via the use of an additional fixed effect. However, this would essentially refit the model for each region separately and hence limit the amount of information we can borrow among regions. With brms it is straightforward to specify the nested random effects structure described in the Model Details Section. Following the same steps as before, setting up the brms model may look like:\n\nregion_model &lt;- bf(r | trials(n) ~ 1 + (1 | region/study), family=binomial)\nget_prior(region_model, AS_region)\n\n                prior     class      coef        group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                             \n student_t(3, 0, 2.5)        sd                                         0   \n student_t(3, 0, 2.5)        sd                 region                  0   \n student_t(3, 0, 2.5)        sd Intercept       region                  0   \n student_t(3, 0, 2.5)        sd           region:study                  0   \n student_t(3, 0, 2.5)        sd Intercept region:study                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\nregion_model_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 0.5), class=sd, coef=Intercept, group=region) +\n    prior(normal(0, 0.25), class=sd, coef=Intercept, group=region:study)\nregion_map_mc_brms  &lt;- brm(region_model, AS_region, prior=region_model_prior, seed=4767,\n                           control=list(adapt_delta=0.99),\n                           refresh=0, silent=0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\npost_region_map_mc_brms &lt;- posterior_linpred(region_map_mc_brms,\n                                             newdata=AS_region_new,\n                                             transform=TRUE,\n                                             allow_new_levels = TRUE,\n                                             sample_new_levels = \"gaussian\"\n                                             )\n# Let's have a look at what we got:\nsummarize_draws(post_region_map_mc_brms)\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ...1     0.263  0.256 0.0676 0.0552 0.164 0.380  1.00    3243.    3960.\n\n\nThe key difference to the previous model is the nested random effect specification term (1 | region/study) of the model formula. This syntax denotes a random intercept term for region and study in a way which assumes a nested data structure in that a given study is only run in a single region.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#results",
    "href": "src/02a_meta_analysis.html#results",
    "title": "4  Use of historical control data",
    "section": "4.5 Results",
    "text": "4.5 Results\nOnce the MAP prior is obtained in MCMC form a model check of is recommended. In RBesT a forest plot augmented with model shrinkage estimates is suggested for this purpose:\n\nplot(map_mc_rbest)$forest_model\n\n\n\n\n\n\n\n\nThe dashed lines show the 95% confidence intervals of each study estimate on it’s own while the solid line shows the respective shrinkage estimate of the MAP model. This plot is useful to assess the plausibility of the results and may unveil possible issues with the model specification. In brms model diagnostic functions are directly available and essentially expose the functionality found in the bayesplot R package. A suitable bayesplot plot in this situation could be an intervals plot as:\n\npp_check(map_mc_brms_2, type=\"intervals\") +\n    scale_x_continuous(\"Study\", breaks=1:nrow(AS_region), labels=AS_region$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip() +\n    theme(legend.position=\"right\",\n          # suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank())\n\nUsing all posterior draws for ppc type 'intervals' by default.\n\n\n\n\n\n\n\n\n\nThe call of the pp_check function is forwarded to the respective ppc_* functions for posterior predictive checks from bayesplot (depending on the type argument). The plots are designed to compare the posterior predictive distribution to the observed data rather than comparing mean estimates to one another. Thus, the outcome of each trial in the original data set is sampled according to the fitted model and the resulting predictive distribution of the outcome (number of responders) is compared to the observed outcome. The intervals predictive probability check summarises the predictive distributions using a light color for an outer credible interval range and a darker line for an inner credible interval. The outer defaults to a 90% credible interval (prob_outer argument) while the inner uses a 50% credible interval (prob argument). The light dot in the middle is the median of the predictive distribution and the dark dot is the outcome \\(y\\). As we can observe, the outcomes \\(y\\) of the trials all are contained within outer credible intervals of the predictive distributions for the simulated replicate data \\(y_{rep}\\). However, one may critizise that also the 50% credible intervals contain all but two trials (study 3, study 7). Hence, the calibration of the model with the data is possibly not ideal given that every other trial outcome should be outside (or inside) of the 50% predictive interval. Comparing with a binomial distribution one can find that such an outcome can occur in 14% of the cases and does not represent an extreme finding such that we can conclude that the model is consistent with the data.\nOnce the model has been checked for plausibility, we can proceed and derive the main target of the MAP analysis, which is the MAP prior in parametric form. RBesT provides a fitting procedure, based in the EM algorithm, for approximating the MCMC output of the MAP prior in parametric form using mixture distributions. In the case of a binomial response Beta mixtures are being estimated:\n\nmap_rbest &lt;- automixfit(map_mc_rbest)\n\nAnd a comparison of the fitted density vs the histogram of the MCMC sample is available as:\n\nplot(map_rbest)$mix\n\n\n\n\n\n\n\n\nThe automixfit function above recognizes that the map_mc_rbest object is a gMAP analysis object and automatically calls the correct Beta EM mixture algorithm for proportions. When working with brms we also do obtain the MAP prior in MCMC form on the response scale, but we need to provide automixfit additional information on the provided MCMC sample like this:\n\nmap_brms &lt;- automixfit(post_map_mc_brms[,1], type=\"beta\")\n\nAt this stage we can work with map_brms_2 just like we would when using RBesT directly such that the graphical diagnostic of the fit still works:\n\nplot(map_brms)$mix\n\n\n\n\n\n\n\n\nComparing the results of using either packages shows that the two resulting MAP prior distributions are representing the same evidence (up to MCMC sampling error):\n\nkable(rbind(rbest=summary(map_rbest),\n            brms=summary(map_brms)),\n      digits=3)\n\n\n\n\n\nmean\nsd\n2.5%\n50.0%\n97.5%\n\n\n\n\nrbest\n0.256\n0.086\n0.105\n0.247\n0.472\n\n\nbrms\n0.259\n0.087\n0.111\n0.248\n0.479\n\n\n\n\n\nFor the region specific model, two different types of priors can be derived. One may wish to obtain a MAP prior for one of the considered regions or for a new region:\n\n# predict a new study for all fitted region and other (=a new region)\nAS_region_all &lt;- data.frame(region=c(\"asia\", \"europe\", \"north_america\", \"other\")) %&gt;%\n    mutate(study=paste(\"new_study\", region, sep=\"_\"), r=0, n=6)\n\npost_region_all_map_mc_brms &lt;- posterior_linpred(region_map_mc_brms,\n                                                 newdata=AS_region_all,\n                                                 transform=TRUE,\n                                                 allow_new_levels = TRUE,\n                                                 sample_new_levels = \"gaussian\"\n                                                 )\n\n\n# name columns according to their region...\ncolnames(post_region_all_map_mc_brms) &lt;- AS_region_all$region\n\n#...to obtain nice labels in a visualization with bayesplot\nbayesplot::mcmc_intervals(post_region_all_map_mc_brms)\n\n\n\n\n\n\n\n# obtain parametric mixture for each region, always using \n# 3 mixture components (often sufficient) to speed up inference\nmap_region &lt;- list()\nfor(r in AS_region_all$region) {\n    map_region[[r]] &lt;- mixfit(post_region_all_map_mc_brms[,r], type=\"beta\", Nc=3, constrain_gt1=TRUE)\n}\n\nThese MAP priors summaries are:\n\nkable(bind_rows(lapply(map_region, summary), .id=\"MAP\"), digits=3)\n\n\n\n\nMAP\nmean\nsd\n2.5%\n50.0%\n97.5%\n\n\n\n\nasia\n0.265\n0.068\n0.147\n0.258\n0.431\n\n\neurope\n0.248\n0.061\n0.136\n0.244\n0.391\n\n\nnorth_america\n0.255\n0.069\n0.140\n0.249\n0.427\n\n\nother\n0.263\n0.091\n0.115\n0.253\n0.502\n\n\n\n\n\nThe summaries show that we have higher precision for regions with more trials and the least precision for the MAP prior for a new (\"other\") region, for which there were no trials. An alternative way to quantify the informativeness of the MAP prior is the effective sample size as provided by RBesT:\n\nsapply(map_region, ess)\n\n         asia        europe north_america         other \n     52.57311      56.77707      49.12890      32.85157 \n\n\nAt this point the tools from RBesT can be used to assess further properties of trial designs which use these MAP priors. Please refer to the getting started vignette of RBesT.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#conclusion",
    "href": "src/02a_meta_analysis.html#conclusion",
    "title": "4  Use of historical control data",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nThe random-effects meta-analysis model implemented in RBesT has been re-implemented with brms. In a second step the meta-analysis has been extended to account for trial regions. This enables stronger borrowing within regions and hence a more informative MAP prior as can be seen by the effective sample size measure. Moreover, the case study also demonstrates how posterior samples produced with brms can be used as an input to RBesT such that both tools can be used in combination.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#exercises",
    "href": "src/02a_meta_analysis.html#exercises",
    "title": "4  Use of historical control data",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\nCreate a posterior predictive check based on the predictive distribution for the response rate.\nSteps:\n\nUse posterior_predict to create samples from the predictive distribution of outcomes per trial.\nUse sweep(predictive, 2, AS_region$n, \"/\") to convert these samples from the predictive distribution of the outcome counts to samples from the predictive distribution for responder rates.\nUse ppc_intervals from bayesplot to create a plot showing your results.\n\nRedo the analysis with region, but treat region as a fixed effect. Evaluate the informativeness of the obtained MAP priors. The model formula for brms should look like region_model_fixed &lt;- bf(r | trials(n) ~ 1 + region + (1 | study),    family=binomial). Steps:\n\nConsider the prior for the region fixed effect first. The reference region is included in the intercept. The reference region is implicitly defined by the first level of the variable region when defined as factor.\n\nDefine asia to be the reference region in the example. Also include a level other in the set of levels.\nAssume that an odds-ratio of \\(2\\) between regions can be seen as very large such that a prior of \\(\\mbox{Normal}(0, (\\log(2)/1.96)^2)\\) for the region main effect is adequate.\n\nObtain the MAP prior for each region by using the AS_region_all data frame defined above and apply posterior_linpred as shown above.\nConvert the MCMC samples from the MAP prior distribution into mixture distributions with the same code as above.\nCalculate the ESS for each prior distribution with the ess function from RBesT.\n\nRun the analysis for the normal endpoint in the crohn data set of RBesT. Refer to the RBesT vignette for a normal endpoint on more details and context. Steps:\n\nUse as family=gaussian and use the se response modifier in place of trials to specify a known standard error.\nUse the same priors as proposed in the vignette.\nCompare the obtained MAP prior (in MCMC sample form) from RBesT and brms.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html",
    "href": "src/02ab_meta_analysis_trtdiff.html",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "",
    "text": "5.1 Background\nMeta-analyses have become a key tool of evidence based medicine (Higgins and Green, 2011, Section 1.2.2) for when we have multiple studies that address the same (or closely related) question(s). Each study may not be able to conclusively answer these questions on its own or the results may vary across the studies. In this scenario, a meta-analysis can quantitatively combine the results of these multiple studies to provide an overall result. Literally, a meta-analysis is a statistical analysis of the analysis results from individual studies (for the purpose of combining the results) (Glass, 1976).\nOne of most common types of meta-analysis in evidence based medicine is that of treatment effects estimated from randomized controlled trials. Meta-analyses of treatment effects from randomized controlled trials importantly respect within study randomization (“analyze as you randomize”). In contrast, simply pooling data from all trial arms ignoring from what study they come, is usually inappropriate and can lead to wildly inappropriate conclusions.\nIn this section we focus on meta-analyses for assessing treatment effects, but meta-analytic methods can be used for combining data across studies for many other purposes. For example, in the next section we willcover predicting control group outcomes for new trials from historical control groups of historical trials. Meta-analyses can be conducted using treatment differences, using outcomes from each trial arm or using individual patient data. We will illustrate all of these approaches.\nIn this section we focus on how to conduct meta-analysis in brms, but ignore a wide range of other important topics. These include the selection of trials for inclusion in meta-analyses, how to handle unavailabile study results, the potential for there to be studies that we are not aware of, and indirect treatment comparisons using network meta-analysis. These topics are covered in-depth in the Cochrane handbook for systematic reviews of interventions, as well as in the book by Borenstein et al. (2009).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#data",
    "href": "src/02ab_meta_analysis_trtdiff.html#data",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "5.2 Data",
    "text": "5.2 Data\nLet us look at a meta-analysis for the effect of sodium-glucose transport protein 2 (SGLT-2) inhibitors on major adverse cardiovascular events (MACE). MACE is a composite endpoint defined by the occurrence of either cardiovascular death, myocardial infarction or stroke. It is usually analyzed using time-to-first-event methods. We will use the data from all 7 SGLT-2 inhibitor trials in the meta-analysis of Barbarawi et al., for which MACE results have been published. This example lets us try out different approaches and input data formats.\nThe first way of inputting data that we will look at is using estimated treatment differences from each study and their standard erorrs. Here, the treatment effect estimate from each study is a hazard ratio from a Cox regression model. The figure below shows the estimate hazard ratios with their 95% confidence intervals from each of the 7 studies with a log-scaled x-axis. For hazard ratios (and similarly for odds and rate ratios) it is preferable to work on the log-scale, because using a normal approximation to the sampling distribution of the estimator works a lot better for smaller sample sizes. Thus, we work with log-hazard ratios and their standard errors.\n\nsglt2_hr = tibble(trial = c(\"DECLARE–TIMI 58\", \"EMPA-REG OUTCOME\",\n                            \"CANVAS\", \"CANVAS-R\", \"CREDENCE\", \n                            \"SCORED\", \"VERTIS CV\"),\n                  drug = c(\"dapagliflozin\", \"empagliflozin\",\n                           \"canagliflozin\", \"canagliflozin\", \"canagliflozin\",\n                           \"sotagliflozin\", \"ertugliflozin\"),\n                  studyno = c(5, 4, 1, 2, 3, 6, 7),\n                  hr = c(0.93, 0.86, 0.88, 0.82, 0.80, 0.77, 0.97),\n                  hr_lcl = c(0.84, 0.74, 0.75, 0.66, 0.67, 0.65, 0.85),\n                  hr_ucl = c(1.03, 0.99, 1.03, 1.01, 0.95, 0.91, 1.11)) %&gt;%\n  arrange(desc(studyno)) %&gt;%\n  mutate(studyno = factor(studyno, levels=studyno, labels=trial),\n         log_hr = log(hr),\n         log_hr_stderr = (log(hr_ucl)-log(hr_lcl))/2/qnorm(0.975))\n\nsglt2_hr %&gt;%\n  ggplot(aes(x=hr, y=studyno, xmin=hr_lcl, xmax=hr_ucl, \n             col=drug, label=drug)) +\n  geom_vline(xintercept=1, linewidth=0.25, linetype=2) +\n  geom_point(shape=19) +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10() +\n  geom_text_repel(data=.%&gt;% filter(studyno %in% c(\"CANVAS\", \"DECLARE–TIMI 58\",\n                                                  \"EMPA-REG OUTCOME\", \"SCORED\",\n                                                  \"VERTIS CV\"))) +\n  xlab(\"Hazard ratio for MACE (&lt;1 indicates\\nhazard rate reduction vs. placebo)\") +\n  ylab(\"Study\")\n\n\n\n\n\n\n\n\nThe second type of input to meta-analyses that we will use is outcomes for each treatment group. While meta-analyses more commonly use treatment differences as an input, we do not have to use that approach. Instead, we can use outcomes from each treatment group in each study as an input. Often, the two approaches are essentially equivalent, but especially when there is more than two treatment groups per study an approach using treatment differences does need modification in order to correctly capture the correlation of multiple treatment differences compared with the same control group. In contrast, an arm-based approach will automatically reflect this correlation.\nIn the SGLT-2 inhibitor case study, we will assume that event times approximately follow an exponential distribution, which allows us to conduct an arm-based meta-analysis with the number of cases per arm and the total follow-up time to firstevent or censoring per arm as inputs. These are the sufficient statistics for exponentially distributed event times with right-censoring.\n\nsglt2 = \"record, study, trialid, studyno, drugname, treatment, cases, patients, py_to_evt_or_cens, cases_per_py\n1,CANVAS,NCT01032629, 1,canagliflozin, 1, 425, 2888, 15799.25651, 0.0269\n2,CANVAS,NCT01032629, 1,canagliflozin, 0, 233, 1442, 7664.473684, 0.0304\n3,CANVAS-R,NCT01989754, 2,canagliflozin, 1, 160, 2907, 5904.059041, 0.0271\n4,CANVAS-R,NCT01989754, 2,canagliflozin, 0, 193, 2905, 5848.484848, 0.0330\n5,CREDENCE,NCT02065791, 3,canagliflozin, 1, 217, 2202, 5607.235142, 0.0387\n6,CREDENCE,NCT02065791, 3,canagliflozin, 0, 269, 2199, 5523.613963, 0.0487\n7,EMPA-REG OUTCOME,NCT01131676, 4,empagliflozin, 1, 490, 4687, 13102, 0.0374\n8,EMPA-REG OUTCOME,NCT01131676, 4,empagliflozin, 0, 282, 2333, 6424, 0.0439\n9,DECLARE–TIMI 58,NCT01730534, 5,dapagliflozin, 1, 756, 8582, 33451.32743, 0.0226\n10,DECLARE–TIMI 58,NCT01730534, 5,dapagliflozin, 0, 803, 8578, 33181.81818, 0.0242\n11,SCORED,NCT03315143, 6,sotagliflozin, 1, 343, 5292, 7145.833333, 0.048\n12,SCORED,NCT03315143, 6,sotagliflozin, 0, 442, 5292, 7015.873016, 0.063\n13,VERTIS CV,NCT01986881, 7,ertugliflozin, 1, 653, 5493, 16743.58974, 0.039\n14,VERTIS CV,NCT01986881, 7,ertugliflozin, 0, 327, 2745, 8175, 0.04\" %&gt;%\n  read.csv(text=.) %&gt;%\n  tibble() %&gt;% \n  mutate(treatment = factor(treatment, levels=0L:1L, \n                            labels=c(\"Placebo\", \"SGLT2 inhibitor\")))\n\nsglt2 %&gt;%\n  arrange(desc(studyno)) %&gt;%\n  mutate(studyno = factor(studyno, levels=studyno, labels=study)) %&gt;%\n  ggplot(aes(x=cases_per_py, y=studyno, col=treatment, label=treatment)) +\n  geom_vline(xintercept=0, linewidth=0.25, linetype=2) +\n  geom_point(shape=19, size=3, alpha=0.6) +\n  geom_text_repel(data=.%&gt;% filter(studyno==\"EMPA-REG OUTCOME\")) +\n  xlab(\"Exponential hazard rate for MACE\") +\n  ylab(\"Study\")\n\n\n\n\n\n\n\n\nLet us also briefly illustrate why naively pooling data instead of using meta-analytic approaches is inappropriate. Take e.g. the CANVAS and CANVAS-R studies, which both have hazard ratios &lt;1 favoring canagliflozin over placebo. A naive pooling of these two studies followed by calculating the proportion of patients with an event would seem to indicate that placebo performs better! This effect is called Simpson’s paradox.\n\nsglt2 %&gt;% \n  filter(str_detect(study, \"CANVAS\")) %&gt;% \n  rename(Treatment=treatment) %&gt;%\n  group_by(Treatment) %&gt;% \n  summarize(`Proportion of cases` = sum(cases)/sum(patients)) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nTreatment\nProportion of cases\n\n\n\n\nPlacebo\n0.098\n\n\nSGLT2 inhibitor\n0.101",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#model-description",
    "href": "src/02ab_meta_analysis_trtdiff.html#model-description",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "5.3 Model description",
    "text": "5.3 Model description\n\n5.3.1 Fixed- and random-effects meta-analysis\nTaking a Bayesian approach with at least weakly informative priors tends to be very useful to avoid various issues that arise with frequentist methods e.g. when there are very few studies, or no events in one or all arms of a study in a meta-analysis of event occurrence. Commonly, two types of meta-analysis model are distinguished: fixed- and random-effects meta-analysis models. Both can expressed as regression models. Fitting such regression models is straightforward in brms. While this is even arguably useful in simple situations due to the understanding and insights it provides, its true value lies in making it easier to deal with more complex situations. E.g. when we wish to adjust estimates or predictions for some study-level covariates (“meta-regression”), we simply add an additional term to our meta-analytic regression model.\nA “fixed-effects meta analysis” assumes that there is a single fixed parameter that describes the treatment effect in all trials. For example, the very simple inverse-variance approach uses estimates \\(\\hat{\\delta}_i\\) from each trial \\(i=1,\\ldots,I\\) and their standard errors \\(\\text{SE}(\\hat{\\delta}_i)\\) as its inputs, and results in an estimate that is the weighted mean of the estimates from each study with the weights of each study proportional to the inverse of the squared standard errors \\[\\hat{\\delta}_\\text{inv.-var.} = \\frac{1}{{\\sum_{j=1}^I \\frac{1}{\\text{SE}(\\hat{\\delta}_j)^{2}}}} \\times \\sum_{i=1}^I \\frac{\\hat{\\delta}_i}{\\text{SE}(\\hat{\\delta}_i)^2}.\\]\nExpressed as a regression model, the inverse-variance approach corresponds to a regression model for the observed treatment effect estimates that only has an intercept term, i.e.: \\[\\text{E} \\hat{\\delta}_i = \\delta\\] with \\(\\hat{\\delta}_i \\sim N(\\text{E} \\hat{\\delta}_i, \\text{SE}(\\hat{\\delta}_i))\\) (conditioning on the observed value of the standard error). In this model, inference is about \\(\\delta\\). In brms formula syntax, this model can be specified as log_hr | se(log_hr_stderr) ~ 1.\nIn contrast, a “random-effects meta analysis” assumes that there the parameter that describes the treatment effect varies across trials. It is commonly assumed that it varies according to a normal distribution with unknown mean and standard deviation. Such an approach will give greater weight to studies with larger standard errors than the inverse-variance approach does.\nOne common method for random-effects meta-analysis is the DerSimonian and Laird method (DerSimonian 1986). Expressed as a regression model, this method corresponds to a regression model for the observed treatment effect estimates that has an intercept term and a random study effect on the intercept, i.e.: \\[\\text{E} \\hat{\\delta}_i = \\mu + \\nu_i\\] with \\(\\nu_i \\sim N(0, \\tau)\\) for some \\(\\tau&gt;0\\) and \\(\\hat{\\delta}_i \\sim N(\\text{E} \\hat{\\delta}_i, \\text{SE}(\\hat{\\delta}_i))\\) for \\(i=1, \\ldots, I\\) (again, conditioning on the observed value of the standard error). In brmsformula syntax, this model can be expressed as log_hr | se(log_hr_stderr) ~ 1 + (1|study).\nIn this model, inference is usually made for \\(\\mu\\), although arguably the effect \\(\\delta^* \\sim N(\\mu, \\tau)\\) in a new population drawn from the distribution of study populations may also be of interest.\n\n\n5.3.2 With data from each arm as an input\nWhen we have data from each arm as an input, we can use a fixed-effects approach with a regression model for the observed outcomes in each arm that has fixed study and treatment effects: \\[\\text{E} \\hat{\\theta}_j = \\sum_{i=1}^I \\alpha_i \\times 1\\{\\text{study}_j=i\\} + \\delta \\times 1\\{\\text{treatment}_j = 1\\}\\] with \\(\\hat{\\theta}_i \\sim N(\\text{E} \\hat{\\theta}_i, \\text{SE}(\\hat{\\theta}_i))\\). The expected control group outcome \\(\\alpha_i\\) for each study is a nuissance parameter and we aim to make inference about \\(\\delta\\). In brms formula syntax this model can be expressed as theta | se(se_theta) ~ 0 + study + treatment.\nIn a frequentist setting, there is only value in using this input data format, if we have more than two treatments, because different comparisons versus the same control group from the same study are correlated and this approach can automatically reflects this. The generalization of the regression equation above is straightforward.\nWhen we have data from each arm as an input, we can implement a random-effects approach via a regression model for the observed outcomes in each arm that has fixed study effects and random treatment effects: \\[\\text{E} \\hat{\\theta}_j = \\sum_{i=1}^I \\alpha_i \\times 1\\{\\text{study}_j=i\\} + \\left( \\mu + \\sum_{i=1}^I \\nu_i \\times 1\\{\\text{study}_j=i\\} \\right) \\times 1\\{\\text{treatment}_j = 1\\}\\] with \\(\\hat{\\theta}_i \\sim N(\\text{E} \\hat{\\theta}_i, \\text{SE}(\\hat{\\theta}_i))\\). In theory, the brms formula syntax for such a model should be something like theta | se(se_theta) ~ 0 + study + (treatment|study), but this does not seem to do quite what we want, so we implement something else below.\n\n\n5.3.3 What to do when a normal approximation is not appropriate?\nIn the previous subsection, we assumed that it is a good approximation to assume that least-squares-means for each treatment group or treatment differences have a normal sampling distribution. That is often the case, especially after suitable transformations such as the log-transformation for odds ratios, rate ratios, hazard ratios, event rates, odds, and hazard rates. However, making this assumption is not always appropriate. Examples when different approaches are preferable include:\n\nsparse binomial, time-to-event or count outcomes (e.g. arms in some studies have a low number of events or no events - this is one of the most common scenarios),\nthe published information does not lend itself easily towards being represented as an estimate with a standard error (e.g. data consisting of Kaplan-Meier curves for each treatment arm),\ncorrelations in the data need to be reflected appropriately (e.g. meta-analysis of multivariate data such as data from multiple timepoints, or joint meta-analysis of diagnostic sensitivity and specificity), or\nmeta-analysis of individual-patient data accounting for covariates.\n\nIn these situations, there are a number of ideas we can conveniently use in a regression setting.\n\nUsing a likelihood corresponding to the individual patient data likelihood when sufficient statistics are available. Examples include:\n\nWhen number of patients with an event \\(Y_{ij}\\) and the total number of patients \\(N_{ij}\\) are available for each arm \\(j\\) of study \\(i\\), we could assume that \\(Y_{ij} \\sim \\text{Bin}(\\pi_{ij}, N_{ij})\\) and work with a regression equation for \\(\\text{logit}(\\pi_{ij})\\). For this, brms offers the formula = cases | trials(patients) = regression terms syntax in combination with specifying family = binomial(link=\"logit\").\nWhen the number of patients with at least an event \\(Y_{ij}\\) and the total time until first event or censoring \\(t_{ij}\\) are available for exponentially distributed time-to-event data, we can use the the log-likelihood is - up to a constant - the same as when we assume \\(Y_{ij} \\sim \\text{Poisson}(\\lambda_{ij} t_{ij})\\) and work with Poisson regression with a \\(\\log t_{ij}\\) offset. For this, brms offers the formula = cases | rate(followuptime) ~ regression terms syntax in combination with specifying family = poisson(link=\"log\").\nWhen the number of total events experienced by all patients \\(Y_{ij}\\) and the total time at risk of events for all patients \\(t_{ij}\\) are available for Poisson distributed count data, we can use \\(Y_{ij} \\sim \\text{Poisson}(\\lambda_{ij} t_{ij})\\) and work with Poisson regression with a \\(\\log t_{ij}\\) offset. For this, brms offers the formula = cases | rate(followuptime) ~ regression terms syntax in combination with specifying family = poisson(link=\"log\"). However, note that this is rarely appropriate, because in practice count data are nearly always over-dispersed compared to the Poisson distribution. In contrast, assuming an exponential distribution for the time-to-first-event seems to occasionally describe data from real trials reasonably well.\n\nFor individual patient data, we would use a regression model that we would use to analyze each trial, but let all model terms other than the treatment effect differ between trials. Most typically, we would simply assume that there are completely different parameters for each trial. For regression coefficients, this is equivalent to having an interaction between them and the trial effect. For distributional parameters such as the residual standard deviation, we can use the specific syntax offered by brms to specify that they should differ by trial, e.g. for the residual standard deviation in a linear regression we can write formula = bf(outcome ~ regression terms, sigma ~ 0 + trial), family = gaussian() to specify this. For more details see the brms vignette on distributional models.\nUsing custom likelihood functions can also be an option.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#implementation",
    "href": "src/02ab_meta_analysis_trtdiff.html#implementation",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "5.4 Implementation",
    "text": "5.4 Implementation\n\n5.4.1 Meta-analysis with treatment effects as inputs\nWe will first implement a Bayesian version of the “traditional” meta-analysis approach, where we use the estimates of a treatment effect and their standard errors as inputs. The brms code below performs a fixed-effects meta-analysis with log-hazard ratios and their standard errors as inputs.\n\nbrmfit1 = brm(data=sglt2_hr,\n              formula = log_hr | se(log_hr_stderr) ~ 1,\n              prior = prior(class=\"Intercept\", student_t(3, 0, 2.5)),\n              control=list(adapt_delta=0.99),\n              refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nsummary(brmfit1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_hr | se(log_hr_stderr) ~ 1 \n   Data: sglt2_hr (Number of observations: 7) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.13      0.03    -0.18    -0.07 1.00     1152     1068\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis brms code performs a random-effects meta-analysis with log-hazard ratios and their standard errors as inputs.\n\nbrmfit2 = brm(data=sglt2_hr,\n              formula = log_hr | se(log_hr_stderr) ~ 1 + (1|studyno),\n              prior = prior(class=\"Intercept\", student_t(3, 0, 2.5)) +\n                prior(class=\"sd\", normal(0,1)),\n              control=list(adapt_delta=0.99),\n              refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.9 seconds.\n\n\nAs we noted before, there’s at least two ways of performing posterior inference after fitting the model above: 1. inference about the mean effect of the SGLT-2 inhibitors across studies, or 2. inference about the predicted effect in a new study drawn from the same distribution of studies as the studies under analysis. This difference becomes very clear when working in a Bayesian regression setting using brms, but is somewhat obscured when we use standard meta-analysis software.\n\nsummary(brmfit2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_hr | se(log_hr_stderr) ~ 1 + (1 | studyno) \n   Data: sglt2_hr (Number of observations: 7) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~studyno (Number of levels: 7) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.07      0.06     0.00     0.20 1.00      932     1264\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.14      0.05    -0.23    -0.05 1.00     1598     1289\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\npredict(object=brmfit2, \n        newdata=tibble(studyno=\"New study\", log_hr_stderr=10e-19), \n        allow_new_levels=TRUE, sample_new_levels = \"gaussian\")\n\n       Estimate Est.Error       Q2.5      Q97.5\n[1,] -0.1376752 0.1027312 -0.3394645 0.05522499\n\n\n\n\n5.4.2 Meta-analysis with data from each arm as inputs\nThis brms code performs a fixed-effects meta-analysis with arm-based data as inputs. We now need to assume prior distributions for the placebo group log-hazard rate in each trial, for this we use an extremely wide Student-t prior with 3 degrees of freedom centered on \\(-3\\), because when exponentiated \\(e^-3 \\approx 0.05\\) corresponds to a plausible yearly event rate.\n\nbrmfit3 = brm(data=sglt2,\n              formula = cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment,\n              family = poisson(link=\"log\"),\n              prior = prior(class=\"b\", coef=\"treatmentSGLT2inhibitor\",\n                            student_t(3, 0, 2.5)) +\n                prior(class=\"b\",  coef=\"studyCANVAS\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCANVASMR\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCREDENCE\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyDECLARE–TIMI58\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyEMPAMREGOUTCOME\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studySCORED\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyVERTISCV\",  student_t(3, -3, 5)),\n              control=list(adapt_delta=0.99),\n              refresh = 0) \n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\n\nsummary(brmfit3)\n\n Family: poisson \n  Links: mu = log \nFormula: cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment \n   Data: sglt2 (Number of observations: 14) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nstudyCANVAS                -3.49      0.04    -3.57    -3.41 1.00     3031\nstudyCANVASMR              -3.44      0.05    -3.55    -3.34 1.00     3923\nstudyCREDENCE              -3.07      0.05    -3.16    -2.98 1.00     3794\nstudyDECLARE–TIMI58        -3.69      0.03    -3.75    -3.64 1.00     2813\nstudyEMPAMREGOUTCOME       -3.14      0.04    -3.22    -3.07 1.00     2654\nstudySCORED                -2.83      0.04    -2.90    -2.76 1.00     3842\nstudyVERTISCV              -3.15      0.04    -3.22    -3.08 1.00     3269\ntreatmentSGLT2inhibitor    -0.13      0.03    -0.18    -0.08 1.00     1672\n                        Tail_ESS\nstudyCANVAS                 2762\nstudyCANVASMR               2624\nstudyCREDENCE               2879\nstudyDECLARE–TIMI58         2698\nstudyEMPAMREGOUTCOME        2745\nstudySCORED                 2786\nstudyVERTISCV               2880\ntreatmentSGLT2inhibitor     2412\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nbrmfit3 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws()\n\n# A tibble: 1 × 10\n  variable      mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 hazard ratio 0.877  0.876 0.0242 0.0241 0.838 0.918  1.00    1672.    2412.\n\n\nThis brms code performs a random-effects meta-analysis with arm-based data as inputs.\n\nbrmfit4 = brm(data=sglt2 %&gt;% mutate(trtno = 1L*(treatment==\"SGLT2 inhibitor\")),\n              formula = cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment + (0+trtno|study),\n              family = poisson(link=\"log\"),\n              prior = prior(class=\"b\", coef=\"treatmentSGLT2inhibitor\", student_t(3, 0, 2.5)) +\n                prior(class=\"b\",  coef=\"studyCANVAS\",   student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCANVASMR\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCREDENCE\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyDECLARE–TIMI58\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyEMPAMREGOUTCOME\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studySCORED\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyVERTISCV\",  student_t(3, -3, 5)) +\n                prior(class=\"sd\", normal(0,1)),\n              control=list(adapt_delta=0.99),\n              refresh = 0) \n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.2 seconds.\n\n\nAs before, we can easily obtain inference about the mean effect of the SGLT-2 inhibitors across studies or the predicted effect in a new study drawn from the same distribution of studies as the studies under analysis.\n\nbrmfit4 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws() %&gt;%\n  dplyr::select(variable, median, q5, q95) %&gt;%\n  knitr::kable(digits=3)\n\n\n\n\nvariable\nmedian\nq5\nq95\n\n\n\n\nhazard ratio\n0.869\n0.799\n0.932\n\n\n\n\n\n\nbrmfit4 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor + \n                                          rnorm(n=length(b_treatmentSGLT2inhibitor), \n                                                mean=0, sd=sd_study__trtno))) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws() %&gt;%\n  dplyr::select(variable, median, q5, q95) %&gt;%\n  knitr::kable(digits=3)\n\n\n\n\nvariable\nmedian\nq5\nq95\n\n\n\n\nhazard ratio\n0.87\n0.73\n1.024",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#results",
    "href": "src/02ab_meta_analysis_trtdiff.html#results",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "5.5 Results",
    "text": "5.5 Results\nLet us combine all the results including also a standard frequentist analysis. As we can see, particularly when using log-hazard-ratios as our analysis inputs, the results of the different models are extremely similar. There seems to be substantial evidence that on average there is a benefit on MACE from SGLT-2 inhibitors in type 2 diabetes patients.\nThe uncertainty from random effects models is greater - especially when predicting for a new trial. When using arm-based inputs, the prior we put on the results for each arm appears to influence there results - perhaps we should make those rates exchangeable between trials to make a more data-informed decision on those rates?\n\nlibrary(meta)\n\nfrequentist_ma = metagen(data = sglt2_hr,\n                         sm = \"HR\",\n                         TE=log_hr,\n                         seTE=log_hr_stderr, \n                         studlab=trial)\n\nbind_rows(\n  brmfit1 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Fixed effects - log-hazard ratios\"),\n   brmfit2 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - log-hazard ratios;inference on mean\"),\n  brmfit2 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept +\n                                            rnorm(n=length(b_Intercept), \n                                                  mean=0, \n                                                  sd=sd_studyno__Intercept))) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - log-hazard ratios;inference for new trial\"),\n  brmfit3 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws()  %&gt;%\n    mutate(Method = \"Fixed effects - rates by arm\"),\n  brmfit4 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - rates by arm; inference on mean\"),\n  brmfit4 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor + \n                                            rnorm(n=length(b_treatmentSGLT2inhibitor), \n                                                  mean=0, sd=sd_study__trtno))) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - rates by arm; inference for new trial\"),\n  tibble(Method = c(\"Fixed effects - frequentist\", \"Random effects - frequentist\"),\n         variable=\"hazard ratio\",\n         median=exp(c(frequentist_ma$TE.fixed, frequentist_ma$TE.random)), \n         q5=exp(c(frequentist_ma$lower.fixed, frequentist_ma$lower.random)), \n         q95=exp(c(frequentist_ma$upper.fixed, frequentist_ma$upper.random)))) %&gt;%\n  dplyr::select(Method, variable, median, q5, q95) %&gt;%\n  mutate(method_class = case_when(\n    Method==\"Fixed effects - frequentist\"~1L,\n    Method==\"Fixed effects - log-hazard ratios\" ~ 2L,\n    Method==\"Fixed effects - rates by arm\"~3L,\n    Method==\"Random effects - frequentist\"~4L,\n    Method==\"Random effects - log-hazard ratios;inference on mean\"~5L,\n    Method==\"Random effects - rates by arm; inference on mean\"~6L,\n    Method==\"Random effects - log-hazard ratios;inference for new trial\"~7L,\n    Method==\"Random effects - rates by arm; inference for new trial\"~8L,\n    TRUE~9L)) %&gt;%\n  arrange(desc(method_class)) %&gt;%\n  mutate(method_class = factor(method_class, levels=8L:1L, labels=gsub(\" - \", \"\\n\", Method))) %&gt;%\n  ggplot(aes(x=median, y=method_class, xmin=q5, xmax=q95)) +\n  geom_vline(xintercept=1, linetype=2) +\n  geom_point(shape=19) +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10() +\n  xlab(\"Hazard ratio for MACE (&lt;1 indicates\\nhazard rate reduction vs. placebo)\") +\n  theme_bw(base_size = 14)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#conclusion",
    "href": "src/02ab_meta_analysis_trtdiff.html#conclusion",
    "title": "5  Meta-analysis to estimate treatment effects",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nAs we saw, brms makes it extremely easy to fit various meta-analysis models. Looking at these from a regression perspective makes it easier to see their connections and makes the extension of meta-analysis to meta-regression seem like just one little logical step.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html",
    "href": "src/02ac_meta_analysis_strata.html",
    "title": "6  Use of historical control data with stratification",
    "section": "",
    "text": "6.1 Background\nAssume we wish to evaluate in a Phase III study, the effectiveness of an investigational treatment by comparing it to a placebo in a population with a rare disease. The study focused on evaluating the treatment’s efficacy in different age groups from the age range 2 to under 18 years old. The primary endpoint of the study is the change in a Patient Reported Outcome (PRO) total score at the end of the follow-up period (12 months).\nThe statistical hypothesis being tested for the primary endpoint is that there is no difference in the change from baseline in the PRO total score between the treatment group and the placebo group.\nA Bayesian linear regression model will be applied, adjusting for the age classified into 3 groups (2 to under 5, 5 to under 13, and 13 to under 18) and baseline score. The age group of 5 to under 13 years old will serve as the reference category, allowing the use of priors on the intercept and coefficients for treatment to represent the mean change in the score at 12 months in this subgroup of patients.\nIn order to inform the model, informative priors will be included for the mean change in the PRO score at 12 months in each age subgroup. These priors will be derived from historical studies.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#data",
    "href": "src/02ac_meta_analysis_strata.html#data",
    "title": "6  Use of historical control data with stratification",
    "section": "6.2 Data",
    "text": "6.2 Data\nAt the design stage of the trial, data from two historical studies were available for the control group.\n\nStudy 1: A natural history study of patients with the disease.\nStudy 2: Control arm of a randomized study evaluating an alternative therapy.\n\nThese historical studies provide valuable information for the primary evaluation of the treatment effect in the trial of interest. Casting the historical data into an informative prior on the control response is expected to increase the precision in the estimation of the treatment effect for the Phase III study of interest.\nThe data set obtained from these two studies are summary statistics for the PRO score at baseline (BASE), the mean change from baseline (CHGm), and is standard error (CHGse) stratified by age group (AGEGRP).\n\nhdata &lt;- tibble::tribble(\n  ~Study   , ~AGEGRPN, ~AGEGRP,           ~n, ~BASE, ~CHGm, ~CHGse,\n  \"1. STUDY 1\" , \"1\" , \"Age &gt;=2 to &lt;5\"   , 28, 16.8,  -0.7,  1.00,\n  \"1. STUDY 1\" , \"2\" , \"Age &gt;=5 to &lt;13\"  , 50, 26.7,  -1.1,  0.73,\n  \"1. STUDY 1\" , \"3\" , \"Age &gt;=13 to &lt;18\" , 23, 18.5,  -1.1,  1.27,\n  \"2. STUDY 2\" , \"1\" , \"Age &gt;=2 to &lt;5\"   , 42, 19.9,   0.2,  0.66)\n\nbase_mean &lt;- with(hdata, round(weighted.mean(BASE, n), 2))\nhdata &lt;- hdata |&gt;\n    mutate(\n        # make the age group a factor and set the middle age group to be the reference\n        age_group = factor(AGEGRPN, c(\"2\", \"1\", \"3\")),\n        # add a centered baseline covariate\n        cBASE = BASE - base_mean,\n        # back-calculate the sd\n        CHGsd = CHGse * sqrt(n),\n        # make Study a factor for better handling\n        Study = factor(Study),\n        # make a variable 'label' for later use in plots\n        label  = paste0(Study, \"/\", age_group),\n        label2 = paste0(Study, \"/\", age_group, \". \", AGEGRP),\n        # include active treatment indicator (in this case, is set to 0 as all patients are 'control')\n        active = 0L)\n\nhdata |&gt;\n    select(Study, AGEGRP, n, BASE, cBASE, CHGm, CHGse, CHGsd) |&gt;\n    kable()\n\n\n\n\nStudy\nAGEGRP\nn\nBASE\ncBASE\nCHGm\nCHGse\nCHGsd\n\n\n\n\n1. STUDY 1\nAge &gt;=2 to &lt;5\n28\n17\n-4.7\n-0.7\n1.00\n5.3\n\n\n1. STUDY 1\nAge &gt;=5 to &lt;13\n50\n27\n5.2\n-1.1\n0.73\n5.2\n\n\n1. STUDY 1\nAge &gt;=13 to &lt;18\n23\n18\n-3.0\n-1.1\n1.27\n6.1\n\n\n2. STUDY 2\nAge &gt;=2 to &lt;5\n42\n20\n-1.6\n0.2\n0.66\n4.3",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#model-description",
    "href": "src/02ac_meta_analysis_strata.html#model-description",
    "title": "6  Use of historical control data with stratification",
    "section": "6.3 Model description",
    "text": "6.3 Model description\nThe primary endpoint will be analyzed using a Bayesian regression model with the mean change from baseline in the PRO score as the response \\(Y\\) and with treatment \\(Z\\) (\\(Z = 1\\) is treatment, \\(Z = 0\\) is control). The basic analysis model for the Phase III model can be written as\n\\[Y|\\mu,\\sigma \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\]\n\\[\\mu = \\beta_0 + \\beta_1 \\, x_1 + \\beta_2 \\, x_2 + \\beta_3 \\, cBASE + \\theta \\, Z\\]\nwhere:\n\n\\(\\beta_0\\) is the mean change from baseline in the score on the control arm for the 5 to 13 years old age group,\n\\(x_1,x_2\\) are indicators for the Age &gt;=2 to &lt;-5 and Age &gt;=13 to &lt;18 groups,\n\\(\\beta_3\\) is the covariate for the centered baseline score,\n\\(\\theta\\) is the treatment effect comparing mean change from baseline for the treatment arm vs control arm,\n\\(\\sigma\\) is the sample standard deviation.\n\nInference for the primary analysis focuses on the posterior distributions of the treatment effect comparing mean change from baseline in the PRO score on the treatment arm vs control arm, \\(\\theta\\).\nTo form priors, the mean change from baseline as well the standard errors from each historical study and age subgroup are combined using the meta-analytic predictive approach (MAP, Neuenschwander et al 2010). While the MAP approach is mostly used for an intercept only model, the use of covariates complicates the analysis here. However, this complication can be resolved by considering the equivalent meta-analytic (MAC) combined approach. Recall, that whereas the MAP approach is a 2-step procedure (derived MAP prior from historical data, then apply to current data), the MAC approach performs a joint analysis of the historical data and the current data. Both approaches result in exactly the same results (Schmidli et al 2014). Thus, it is useful to first consider the respective MAC model. For this we expand the basic model shown above to also include a random intercept effect, which accounts for between-study heterogeneity. Denoting with \\(i\\) the study, we thus model the data as\n\\[Y_i|\\mu_i,\\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma^2)\\]\n\\[\\mu_i = b_i  + \\beta_1 \\, x_1 + \\beta_2 \\, x_2 + \\beta_3 \\, cBASE + \\theta \\, Z\\]\n\\[b_i|\\beta_0,\\tau \\sim \\mbox{Normal}(\\beta_0, \\tau^2).\\]\nThe key assumption of this model is that the covariate effects for age, centered baseline score and treatment effect are pooled accross all studies. That is, no between-study heterogeneity is admitted in these effects; heterogeneity is only accounted for in the intercept term representing the mean change from baseline for control of the reference age group with a baseline value as used for centering (21.45). Note that since we do only include data on control for the historical studies, we will infer the treatment effect \\(\\theta\\) for the new Phase III study without any influence from the other studies.\nHowever, in practice we will still want to run the primary analysis in a 2-step MAP approach. The desire here is to be able to evaluate the properties of the MAP prior derived from the historical data in advance. The challenge becomes then that we have to use the information from fitting the above model to the historical data for the entire model posterior and not merley the intercept only. Therefore, the MAP prior becomes multi-variate, since the MAP prior includes the covariate effects.\nHence, the model is fitted first to the historical data under the assumption of a known sampling standard deviation (accounting for the reported standard errors).\n\nAs priors for \\(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\theta\\) weakly informative \\(\\mbox{Normal}(0,s^2)\\) distributions are used, with \\(s = 5\\). The choice of \\(5\\) follows from the observation that the historical data has approximatly a sampling standard deviation of \\(5\\) (the pooled estimate from the historical data is 5.11 ). For the between-trial standard deviation \\(\\tau\\) a half-normal prior with scale \\(s / 4\\) is used. This is considered a plausible conservative, weakly-informative prior, as values of the ratio \\(\\tau/\\sigma \\ge 1/4\\) are generally considered to represent substantial between-trial variability (Neuenschwander et al 2010) and the proposed prior will generally place more than half of the prior probability on this area.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#implementation",
    "href": "src/02ac_meta_analysis_strata.html#implementation",
    "title": "6  Use of historical control data with stratification",
    "section": "6.4 Implementation",
    "text": "6.4 Implementation\nUsing brms we now specify the MAP model step by step. We first define the model:\n\n# Set up a model formula for use in 'brms'\nhist_model &lt;- bf(CHGm | se(CHGse) ~ 1 + age_group + cBASE + active + (1 | Study),\n                 family=gaussian, center=FALSE)\n# Display prior model across all parameters\nget_prior(hist_model, hdata)\n\n                prior class       coef group resp dpar nlpar lb ub       source\n               (flat)     b                                             default\n               (flat)     b     active                             (vectorized)\n               (flat)     b age_group1                             (vectorized)\n               (flat)     b age_group3                             (vectorized)\n               (flat)     b      cBASE                             (vectorized)\n               (flat)     b  Intercept                             (vectorized)\n student_t(3, 0, 2.5)    sd                                   0         default\n student_t(3, 0, 2.5)    sd            Study                  0    (vectorized)\n student_t(3, 0, 2.5)    sd  Intercept Study                  0    (vectorized)\n\n\nNote that we do set explicitly the option center=FALSE in the function call to bf. This avoids the otherwise automatic centering done by brms, which is needed here as we require full control over the parametrization of the model.\nWith the model (and data) being defined, we are left to specify the model priors. With the help of the call\n\n## Fit historical data with between-study heterogeneity  -----------------------\n# For β_0,β_1,β_2,β_3,θ weakly informative N(0,5^2) distributions are used.\n# For the group level effect of the study, we use τ^2=(5.0/4)^2.\n# Set then sd = 5 for later use\nsd &lt;- 5.0\nsd_tau &lt;- sd/4\nhist_prior &lt;- prior_string(glue::glue(\"normal(0, {sd})\"), class=\"b\") +\n    prior_string(glue::glue(\"normal(0, {sd_tau})\"), class=\"sd\", coef=\"Intercept\", group=\"Study\")\n\nNow we are ready to run the model in brms:\n\n# Apply brms model using historical data hdata and the model defined in hist_model\nmap_mc_brms &lt;- brm(hist_model, \n                   data = hdata, \n                   prior = hist_prior,\n                   seed = 234324, refresh = 0,\n                   control = list(adapt_delta = 0.99))\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.5 seconds.\n\nmap_mc_brms\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CHGm | se(CHGse) ~ 1 + age_group + cBASE + active + (1 | Study) \n   Data: hdata (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~Study (Number of levels: 2) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.80      0.63     0.03     2.33 1.00     1452      969\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     -1.13      1.92    -4.95     2.77 1.00     1208     1621\nage_group1     1.13      2.71    -4.40     6.44 1.00     1225     1611\nage_group3     0.39      2.64    -4.73     5.71 1.00     1369     1837\ncBASE          0.06      0.31    -0.55     0.69 1.00     1258     1761\nactive        -0.08      4.94    -9.91     9.51 1.00     3201     2483\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#results",
    "href": "src/02ac_meta_analysis_strata.html#results",
    "title": "6  Use of historical control data with stratification",
    "section": "6.5 Results",
    "text": "6.5 Results\nOnce the MAP prior is obtained in MCMC form a model check for it is recommended. In brms model diagnostic functions are directly available and essentially expose the functionality found in the bayesplot R package. A suitable bayesplot plot in this situation could be an intervals plot as:\n\n## Posterior predictive check for the mean change in Score at 12mo by age group -------\npp_check(map_mc_brms, type=\"intervals\") +\n  scale_x_continuous(\"Study / Strata\", \n                     breaks = 1:nrow(hdata), \n                     labels = hdata$label2,\n                     trans = \"reverse\") +\n  ylab(\"Mean change from baseline\") +\n  xlab(NULL) +\n  coord_flip() +\n  hline_0(linetype=2) +\n  ggtitle(\"Posterior predictive check\") +\n  theme(plot.background = element_rect(fill = \"white\"),\n        legend.position=\"right\",\n        # suppress vertical grid lines for better readability of intervals\n        panel.grid.major.y = element_blank())\n\nUsing all posterior draws for ppc type 'intervals' by default.\n\n\n\n\n\n\n\n\n\nThe call of the pp_check function is forwarded to the respective ppc_* functions for posterior predictive checks from bayesplot (depending on the type argument). The plots are designed to compare the posterior predictive distribution to the observed data rather than comparing mean estimates to one another. Thus, the outcome of each study/age group in the historical data set is sampled according to the fitted model and the resulting predictive distribution of the outcome (change in score) is compared to the observed outcome. The intervals predictive probability check summarises the predictive distributions using a light color for an outer credible interval range and a darker line for an inner credible interval. The outer defaults to a 90% credible interval (prob_outer argument) while the inner uses a 50% credible interval (prob argument). The light dot in the middle is the median of the predictive distribution and the dark dot is the outcome \\(y\\). As we can observe, the outcomes \\(y\\) for all age groups in historical studies all are contained within outer credible intervals of the predictive distributions for the simulated replicate data \\(y_{rep}\\). We can conclude that the model is consistent with the data.\nOnce the model has been checked for plausibility, we can proceed and derive the main target of the MAP analysis, which is the MAP prior in parametric form. The MAP priors are stored in the post_coefs_mix object. Note that since we will be predicting for a new study, we sample a random intercept using the between study variation.\n\n# For the simple case of this case study for which only a random\n# effect is assigned to the intercept, we can use the brms prediction\n# functions. We are seeking a sample of the intercept with\n# between-trial heterogeneity, but without having other terms\n# included in the linear predictor. Hence, we can predict the mean\n# change from baseline for a new study arm which is has the covariate\n# values set to the reference\n\nnew_study_ref &lt;- tibble(Study=\"new_study\", age_group=\"2\", cBASE=0, CHGse=0, active=0L)\nrv_study_sim_inter &lt;- rvar(posterior_epred(map_mc_brms,\n                                           newdata=new_study_ref,\n                                           allow_new_levels=TRUE,\n                                           sample_new_levels=\"gaussian\"))\nrv_study_sim_inter\n\nrvar&lt;4000&gt;[1] mean ± sd:\n[1] -1.1 ± 2.2 \n\n# Get random draws from the posterior of each parameter\n# after fitting historical data\nhist_post_draws &lt;- as_draws_rvars(map_mc_brms)\n\n\n# Whenever one also models the covariate effects to be varying between\n# studies, one needs to smaple the random effect directly, since brms\n# always simulates the linear predictor by convention. In this case\n# one needs to sample the random intercept directly from the posterior.\n\n# Add variability to the intercept to capture random between study variation\nrv_study_sim_inter_alt &lt;- with(hist_post_draws,\n                               rvar_rng(rnorm, 1, b_Intercept, sd_Study__Intercept, ndraws=ndraws(hist_post_draws)))\n\n# Get draws for the fixed effects from the model posterior\nhist_post_coefs &lt;- subset_draws(hist_post_draws,\n                                variable=\"^b_\",\n                                regex=TRUE) |&gt;\n  as_draws_matrix()\n\n# Replace the intercept draws with the intercept and added variability\nhist_post_coefs[,\"b_Intercept\"] &lt;- as_draws_matrix(rv_study_sim_inter)[,1]\n\n# Get a descriptive statistics for posterior draws by coefficient\nsummarize_draws(hist_post_coefs) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nb_Intercept\n-1.11\n-1.12\n2.16\n1.98\n-4.56\n2.53\n1\n1395\n2185\n\n\nb_age_group1\n1.13\n1.15\n2.71\n2.64\n-3.34\n5.54\n1\n1225\n1611\n\n\nb_age_group3\n0.39\n0.36\n2.64\n2.59\n-3.92\n4.79\n1\n1369\n1837\n\n\nb_cBASE\n0.06\n0.06\n0.31\n0.31\n-0.46\n0.57\n1\n1258\n1761\n\n\nb_active\n-0.08\n-0.04\n4.94\n4.86\n-8.19\n7.98\n1\n3201\n2483\n\n\n\n\n# Summarize as mixture multi-variate normal  MAP distribution\npost_coefs_mix &lt;- mixfit(sample = hist_post_coefs, type = \"mvnorm\", Nc = 3)\nprint(post_coefs_mix)\n\nEM for Multivariate Normal Mixture Model\nLog-Likelihood = -31931\n\nMultivariate normal mixture\nOutcome dimension: 5\nMixture Components:\n                               comp1   comp2   comp3  \nw                               0.4068  0.3653  0.2279\nm[b_Intercept]                 -1.6714 -1.0509 -0.1817\nm[b_age_group1]                 1.9530  0.9640 -0.0592\nm[b_age_group3]                 1.0223  0.1458 -0.3458\nm[b_cBASE]                      0.1528  0.0212 -0.0556\nm[b_active]                     0.2407 -0.3209 -0.2476\ns[b_Intercept]                  1.9893  1.5619  2.8476\ns[b_age_group1]                 2.6607  2.2126  2.9926\ns[b_age_group3]                 2.7526  2.4530  2.4833\ns[b_cBASE]                      0.3133  0.2761  0.3143\ns[b_active]                     5.1088  4.9198  4.6240\nrho[b_age_group1,b_Intercept]  -0.8653 -0.9432 -0.5990\nrho[b_age_group3,b_Intercept]  -0.7765 -0.7996 -0.5198\nrho[b_cBASE,b_Intercept]       -0.8167 -0.8602 -0.5286\nrho[b_active,b_Intercept]       0.0156  0.0289 -0.0121\nrho[b_age_group3,b_age_group1]  0.8778  0.7894  0.8255\nrho[b_cBASE,b_age_group1]       0.9196  0.9125  0.9271\nrho[b_active,b_age_group1]     -0.0151 -0.0144 -0.0285\nrho[b_cBASE,b_age_group3]       0.8637  0.8029  0.8447\nrho[b_active,b_age_group3]      0.0317 -0.0054 -0.0147\nrho[b_active,b_cBASE]           0.0027 -0.0248 -0.0355\n\n# notice the considerable correlations present in the posterior\nround(cor(hist_post_coefs), 2)\n\n             b_Intercept b_age_group1 b_age_group3 b_cBASE b_active\nb_Intercept         1.00        -0.80        -0.70   -0.74     0.00\nb_age_group1       -0.80         1.00         0.84    0.92     0.00\nb_age_group3       -0.70         0.84         1.00    0.85     0.02\nb_cBASE            -0.74         0.92         0.85    1.00     0.00\nb_active            0.00         0.00         0.02    0.00     1.00\n\n\nAnd a comparison of the fitted density vs the histogram of the MCMC sample is available as:\n\nplot(post_coefs_mix)\n\nDiagnostic plots for mixture multivariate normal densities are experimental.\nPlease note that these are subject to changes in future releases.\n\n\n$mixpairs\n\n\n\n\n\n\n\n\n\n\n# Create summary statistics for the posteriors draws of coefficients\nhist_post_coefs_df &lt;- as_draws_df(hist_post_coefs)\nhist_post_coefs_df[,1:5]\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 4,000 × 5\n   b_Intercept b_age_group1 b_age_group3 b_cBASE b_active\n         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1      -2.80         3.09         2.93   0.424    -5.78 \n 2      -1.61        -1.35        -0.711  0.154     2.07 \n 3      -0.352        0.419       -3.26  -0.0461   -3.52 \n 4       1.97        -2.83        -2.24  -0.465     1.13 \n 5      -0.154       -1.13        -1.40  -0.116    -0.732\n 6      -2.38         4.09         3.57   0.346     1.80 \n 7      -3.08         4.67         2.69   0.145    -4.13 \n 8       0.652       -0.224       -0.208  0.303    -7.01 \n 9      -0.913        1.68         0.173  0.225    -7.50 \n10      -0.900        0.535        0.821  0.0112   -1.57 \n# ℹ 3,990 more rows\n\nggpairs(hist_post_coefs_df[1:1000,1:5])\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\n\n\n\n\n\n\nThese MAP priors summaries for the coefficients of the model are displayed in the table below. Note that since the historical data did not contain data in both the treatment and control arms, the parameter related to the treatment effect (b_active) will remain with a weakly informative prior.\n\nmap_coef_table &lt;- hist_post_coefs_df |&gt;\n  summarise_draws(median, ~quantile2(.x, probs = c(0.025, 0.975))) |&gt;\n  rename(lower = q2.5,\n         upper = q97.5)\n\nOften it is of interest to evaluate the properties of the MAP prior at the design stage. This is complicated in this example due to the requirement to assume a certain baseline value for the used covariates. However, note that these evaluations can be repeated once the trial has enrolled the study population given that only baseline data is needed for this. Here we assume a baseline score of 20:\n\nagegroups &lt;- c(\"Age &gt;=2 to &lt;5\",\n               \"Age &gt;=5 to &lt;13\",\n               \"Age &gt;=13 to &lt;18\")\n# Set a value for the baseline score\nbase_score_new &lt;- 20\n# Create summary statistics for the posteriors draws of means across subgroups\nnew_study_base20 &lt;- tibble(Study=\"new_study\",\n                           age_group=c(\"1\", \"2\", \"3\"),\n                           cBASE=base_score_new-base_mean,\n                           CHGse=0,\n                           active=0L)\n\nmap_resp_table &lt;- new_study_base20 |&gt;\n    tidybayes::add_epred_rvars(map_mc_brms, allow_new_levels=TRUE, sample_new_levels=\"gaussian\", value=\"MAP\") |&gt;\n    mutate(summarise_draws(MAP, ~quantile2(.x, probs = c(0.025, 0.5, 0.975))),\n           MAP_ess=ess(mixfit(draws_of(MAP)[,1], Nc=3), sigma=5),\n           treatment = \"Control\") |&gt;\n    rename(median= q50,\n           lower = q2.5,\n           upper = q97.5) |&gt;\n  select(MAP, treatment, MAP_ess, median, lower, upper)\nmap_resp_table |&gt; kable()\n\n\n\n\nMAP\ntreatment\nMAP_ess\nmedian\nlower\nupper\n\n\n\n\n-0.088 ± 1.4\nControl\n19\n-0.07\n-3.0\n2.8\n\n\n-1.222 ± 2.5\nControl\n19\n-1.25\n-6.2\n3.9\n\n\n-0.831 ± 1.8\nControl\n19\n-0.84\n-4.2\n2.7\n\n\n\n\n\nAt this point we can use these MAP priors in the analysis of the pivotal trial as outlined in the following section.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#analysis-of-a-new-study",
    "href": "src/02ac_meta_analysis_strata.html#analysis-of-a-new-study",
    "title": "6  Use of historical control data with stratification",
    "section": "6.6 Analysis of a new study",
    "text": "6.6 Analysis of a new study\nAssume now that the trial reads out and you have to analyze the study using the MAP prior that were derived. For this exercise will use a simulated data set, see code below for reference.\n\n\nShow the code\n# Simulate summary data for 1 trial\n#\n# @param n1 The sample size in the 3 strata of the treatment arm. A vector of length 3\n# @param n0 The sample size in the 3 strata of the control arm. A vector of length 3\n# @param e1 The mean change from baseline at month 12 for the 3 strata of the treatment arm. A vector of length 3\n# @param e0 The mean change from baseline at month 12 for the 3 strata of the control arm. A vector of length 3\n# @param s1 The se of the response in the 3 strata of the treatment arm. A vector of length 3\n# @param s0 The se of the response in the 3 strata of the control arm. A vector of length 3\n# @param s1 The baseline score in the 3 strata of the treatment arm. A vector of length 3\n# @param s0 The baseline score in the 3 strata of the control arm. A vector of length 3\n#\n# @return A data frame with simulated trial data\n#\nsimulate_1_trial_ipd &lt;- function(n1 = c(39, 18, 18),\n                                 n0 = c(26, 12, 12),\n                                 e1 = c( 5,  5,  5),\n                                 e0 = c( 0,  0,  0),\n                                 s1 = rep(5, 3),\n                                 s0 = rep(5, 3),\n                                 bs1 = c( 33,33,33),\n                                 bs0 = c( 33,33,33),\n                                 base_center=base_mean) {\n    # Arm 1: Treatment\n    # Arm 0: Control\n    \n    # AGEGRPN 1: Age &gt;=2 to &lt;5\n    # AGEGRPN 2: Age &gt;=5 to &lt;13\n    # AGEGRPN 3: Age &gt;=13 to &lt;18\n    \n    # Simulate individual patient data  -------\n    d1 &lt;- sapply(1:3, function(i) rnorm(n = n1[i], mean = e1[i], sd = s1[i]))\n    d0 &lt;- sapply(1:3, function(i) rnorm(n = n0[i], mean = e0[i], sd = s0[i]))\n    \n    # Simulate individual patient data for the baseline score -------\n    b1 &lt;- sapply(1:3, function(i) rnorm(n = n1[i], mean = bs1[i], sd = s1[i]))\n    b0 &lt;- sapply(1:3, function(i) rnorm(n = n0[i], mean = bs0[i], sd = s0[i]))\n    \n    agegr &lt;- c(\"Age &gt;=2 to &lt;5\", \"Age &gt;=5 to &lt;13\", \"Age &gt;=13 to &lt;18\")\n    AGEGROUP &lt;- c(rep(agegr, n1), rep(agegr, n0))\n    strataN &lt;- c(rep(1:3, n1), rep(1:3, n0))\n  \n    BASE &lt;- c(unlist(b1), unlist(b0))\n    y &lt;- c(unlist(d1), unlist(d0))\n    \n    ### Put generated values into a data frame -----------------------------------\n    out &lt;- data.frame(AGEGRPN = strataN,\n                      AGEGRP  = AGEGROUP,\n                      age_group = relevel(as.factor(strataN), \"2\"),\n                      active    = rep(c(1, 0), c(sum(n1), sum(n0))),\n                      BASE = BASE,\n                      cBASE= BASE - base_center,\n                      CHG  = y\n                      )\n  out\n}\n\n\n\nnew_dat &lt;- simulate_1_trial_ipd(\n  n1 = c(39, 18, 18),\n  n0 = c(26, 12, 12),\n  e1 = c( 2,  1,  0),\n  e0 = c(-4, -3, -2),\n  s1 = rep(5, 3),\n  s0 = rep(5, 3),\n  bs1 = c( 33,33,33),\n  bs0 = c( 33,33,33),\n  base_center=base_mean)\nnew_dat &lt;- new_dat |&gt; \n  mutate(Study = \"New\")\n\nhead(new_dat) |&gt; kable()\n\n\n\n\nAGEGRPN\nAGEGRP\nage_group\nactive\nBASE\ncBASE\nCHG\nStudy\n\n\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n27\n5.8\n2.1\nNew\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n27\n5.1\n2.0\nNew\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n34\n12.4\n8.5\nNew\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n42\n20.8\n6.7\nNew\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n30\n9.0\n8.4\nNew\n\n\n1\nAge &gt;=2 to &lt;5\n1\n1\n39\n17.6\n-2.2\nNew\n\n\n\n\n\nWe first calculate simple summary statistics for the simulated data:\n\nnew_dat |&gt; \n    summarise(BASE = mean(BASE),\n              cBASE = mean(cBASE),\n              CHGm  = mean(CHG),\n              CHGsd = sd(CHG),\n              n     = n(),\n              CHGse = CHGsd / sqrt(n),\n              .by=c(Study, AGEGRPN, AGEGRP, age_group, active)) |&gt;\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy\nAGEGRPN\nAGEGRP\nage_group\nactive\nBASE\ncBASE\nCHGm\nCHGsd\nn\nCHGse\n\n\n\n\nNew\n1\nAge &gt;=2 to &lt;5\n1\n1\n34\n12\n2.76\n5.7\n39\n0.91\n\n\nNew\n2\nAge &gt;=5 to &lt;13\n2\n1\n34\n13\n2.22\n4.2\n18\n1.00\n\n\nNew\n3\nAge &gt;=13 to &lt;18\n3\n1\n35\n13\n0.91\n6.3\n18\n1.50\n\n\nNew\n1\nAge &gt;=2 to &lt;5\n1\n0\n33\n12\n-3.43\n4.5\n26\n0.89\n\n\nNew\n2\nAge &gt;=5 to &lt;13\n2\n0\n34\n12\n-3.04\n5.2\n12\n1.51\n\n\nNew\n3\nAge &gt;=13 to &lt;18\n3\n0\n32\n11\n-0.61\n4.4\n12\n1.26\n\n\n\n\n\nWe must define the model to be used. Note that now, since we have individual patient level data, we use only CHG as response variable. Hence, the sampling standard deviation is estimated from the data (requiring us to set a prior for this parameter as well). It is obviously important that we must use the very same model specification as before, since otherwise we could not use the derived MAP prior easily:\n\njoint_model_ipd &lt;- bf(CHG ~ 1 + age_group + cBASE + active,\n                      family=gaussian, center=FALSE)\n\nAnd then use brm to put it altogether. To use the previously derived priors, we need to use two options in the brm funciton. The prior option indicates that a multivariate normal prior will be used, while the stanvars will pass the actual prior distribution, which is a mixture distribution.\n\n# Fit with historical data priors -------------------------------------------\nmcmc_seed &lt;- 573391\n# use the MAP prior for the coefficients and encode with the gamma\n# prior on the sigma that we anticipate an sd of 5\nmap_prior &lt;- prior(mixmvnorm(prior_mix_w, prior_mix_m, prior_mix_sigma_L), class=b) +\n    prior(gamma(5,1), class=sigma)\nbfit_stage2_ipd &lt;- brm(formula  = joint_model_ipd,\n                       prior    = map_prior,\n                       stanvars = mixstanvar(prior_mix = post_coefs_mix),\n                       data     = new_dat,\n                       seed    = mcmc_seed,\n                       refresh = 0,\n                       backend = \"cmdstanr\")\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\nbfit_stage2_ipd\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CHG ~ 1 + age_group + cBASE + active \n   Data: new_dat (Number of observations: 125) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     -2.16      1.36    -5.04     0.16 1.00     1693     2140\nage_group1     0.37      0.83    -1.26     2.01 1.00     2174     2478\nage_group3     0.00      0.98    -1.94     1.91 1.00     2581     2772\ncBASE         -0.00      0.07    -0.14     0.14 1.00     1967     2630\nactive         4.09      0.92     2.35     5.88 1.00     3438     3067\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.31      0.35     4.67     6.02 1.00     3551     2696\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHaving fit the model, the last step is to use it to test the statistical hypothesis:\n\nhypothesis(bfit_stage2_ipd, \"active &gt; 0\", alpha = 0.05)\n\nHypothesis Tests for class b:\n    Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (active) &gt; 0      4.1      0.92      2.6      5.6       3999         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#conclusion",
    "href": "src/02ac_meta_analysis_strata.html#conclusion",
    "title": "6  Use of historical control data with stratification",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nThis case study demonstrated how a random-effects meta-analysis model has been implemented with brms. The model was implemented in a two-stage approach where first the model is fit with historical data to obtain meta-analytic-predictive priors. Later, when data from the study of interest is available, the model is fit using the study data and MAP priors.\nThe model enables borrowing between historical studies and age groups and hence a more informative MAP prior.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#exercises",
    "href": "src/02ac_meta_analysis_strata.html#exercises",
    "title": "6  Use of historical control data with stratification",
    "section": "6.8 Exercises",
    "text": "6.8 Exercises\n\nFit a model where only weakly informative priors are considered to analyze the data from the new study. This will help providing insights of the impact of using the historical data.\nTo add robustification to the MAP prior, non-informative components can added to the mixture distributions to protect against prior-data conflicts (Schmidli et al 2014). Fit a model where the MAP priors are robustified where the weight for the\nnon-informative component is 20%. The non-informative component is normally distributed with the mean 0 and variance \\(\\sigma^2\\).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02ac_meta_analysis_strata.html#solution-to-exercises",
    "href": "src/02ac_meta_analysis_strata.html#solution-to-exercises",
    "title": "6  Use of historical control data with stratification",
    "section": "6.9 Solution to exercises",
    "text": "6.9 Solution to exercises\n\nget2sidedCI &lt;- function(h1side, h2side) {\n  xx = h2side$hypothesis |&gt;\n    select(Hypothesis, CI.Lower, CI.Upper) |&gt;\n    rename(CI.95.Lower=CI.Lower,\n           CI.95.Upper=CI.Upper)\n  data.frame(h1side$hypothesis, xx)\n}\nget1sidehyp2sidedCI = function(brm_res) {\n  h1side = hypothesis(brm_res, \"active &gt; 0\", alpha=0.025)\n  h2side = hypothesis(brm_res, \"active &gt; 0\", alpha=0.05)\n  \n  xx = h2side$hypothesis |&gt;\n    select(Hypothesis, CI.Lower, CI.Upper) |&gt;\n    rename(CI.95.Lower=CI.Lower,\n           CI.95.Upper=CI.Upper)\n  data.frame(h1side$hypothesis, xx)\n}\n\n\n\nShow the code\n## Fit Non-Informative Prior using default priors -----------------------\nhead(new_dat)\nbfit_trial_ipd_noninf_flat &lt;- brm(joint_model_ipd,\n                                  seed = mcmc_seed, \n                                  refresh = 0,\n                                  data = new_dat)\n\n## Fit Non-Informative Prior using wide Normal priors ------------------------\ntrial_prior_mix &lt;- mixmvnorm(flat = c(1, rep(0, 5), diag(sd^2, 5)))\njoint_model_ipd &lt;- bf(CHG ~ 1 + age_group + BASE + active,\n                      family=gaussian, center=FALSE)\nbfit_trial_ipd_noninf_norm &lt;- brm(joint_model_ipd,\n                                  prior = mix_prior,\n                                  stanvars = mixstanvar(prior_mix = trial_prior_mix),\n                                  seed = mcmc_seed, refresh = 0,\n                                  backend = \"cmdstanr\",\n                                  data = new_dat)\n\n## Fit with historical data MAP Priors -------------------------------------------\nbfit_stage2_ipd &lt;- brm(joint_model_ipd,\n                       prior = mix_prior,\n                       stanvars = mixstanvar(prior_mix = post_coefs_mix),\n                       seed = mcmc_seed, refresh = 0, backend = \"cmdstanr\")\n                       data = new_dat)\n\n## Fit with historical data MAP Priors with robustification ----------------------\nps = seq(0, 1, 0.2)\nhp_ipd =  hn_ipd = list()\nfor (i in 1:length(ps)){\n  p = ps[i]\n  rob_post_coefs_mix &lt;- mixcombine(flat = trial_prior_mix, \n                                   MAP = post_coefs_mix, \n                                   weight=c(p, 1 - p))\n  \n  bfit_stage2_rob_ipd &lt;- brm(joint_model_ipd,\n                             prior = mix_prior,\n                             stanvars = mixstanvar(prior_mix = rob_post_coefs_mix),\n                             seed = mcmc_seed, refresh = 0, backend = \"cmdstanr\")\n                             data = new_dat)\n  hp_ipd[[i]] = get1sidehyp2sidedCI(bfit_stage2_rob_ipd)\n  hn_ipd[[i]] = glue::glue(\"MAP prior {p*100}% robust\")\n}\n\nh1_ipd_noninf_flat = get1sidehyp2sidedCI(bfit_trial_ipd_noninf_flat)\nh1_ipd_noninf_norm = get1sidehyp2sidedCI(bfit_trial_ipd_noninf_norm)\nh1_ipd_map         = get1sidehyp2sidedCI(bfit_stage2_ipd)\n\nh1n_ipd = unlist(hn_ipd)\nh1_ipd_rob = do.call(rbind, lapply(hp_ipd, function(x) x))\n\nres = data.frame(type = c(\"Non-informative prior. brm default\",\n                          \"Non-informative prior. wide normal prior\",\n                          \"MAP prior\",\n                          h1n_ipd),\n                 bind_rows(\n                   h1_ipd_noninf_flat,\n                   h1_ipd_noninf_norm,\n                   h1_ipd_map,\n                   h1_ipd_rob\n                 ))\nres",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Use of historical control data with stratification</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html",
    "href": "src/02l_single_arm_pos.html",
    "title": "7  Probability of success from a single arm trial",
    "section": "",
    "text": "7.1 Background",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#background",
    "href": "src/02l_single_arm_pos.html#background",
    "title": "7  Probability of success from a single arm trial",
    "section": "",
    "text": "7.1.1 Probability of success\nFor pharmaceutical companies it is important to realistically assess the probability of success (PoS) for drug development projects. Together with other information and considerations (e.g. costs of trials, projected timelines, unmet need for new therapies for an indication, competitive landscape, …) this allows for informed decision making about things such as which projects to pursue at all and how much at-risk investment to make into each of them.\nThere are a number of ways to define success for PoS calculations, but at Novartis the definition of success is to get approval meeting a target product profile (TPP). It is assumed that if the drug is approved with a label as good or better than the TPP targets, it will meet an important unmet need for patients, be successful on the market and the sales forecasts (obtained assuming this TPP) would be achieved. The Novartis PoS framework combines industry benchmark information with data on a drug and team assessments of other risks. This is most easily done when we have directly relevant data, e.g. with a Phase 2 study that is very similar (in terms of population, endpoint, duration and treatment groups) to the planned Phase 3 trial(s). On the other hand, this is harder based on a single arm study that differs from Phase 3 (or worse without Phase 2 data).\n\n\n7.1.2 Assurance\nAssurance is a popular related concept to PoS. It describes the probability of a successful trial.\n\n\n7.1.3 Background of the hypothetical example\nA hypothetical drug called NVS101 is already approved as a treatment for some systemic diseases. In a single arm trial in patients with a different systemic disease that leads to anemia, most patients that received NVS101 for 12 weeks were able to achieve an acceptable red blood cell count without needing a blood transfusion. We are now considering to start a pivotal randomized placebo controlled Phase III trial, where the primary endpoint would be having an acceptable red blood cell count without needing a blood transfusion over a 1-year period.\nWe wish to assess the PoS of such a pivotal study. I.e. the probability to get approval meeting the TPP of an odds ratio of 8.5 (given an assumed placebo responder rate of 15% this corresponds to a targeted 60% for the drug).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#data",
    "href": "src/02l_single_arm_pos.html#data",
    "title": "7  Probability of success from a single arm trial",
    "section": "7.2 Data",
    "text": "7.2 Data\nThe Novartis PoS framework provides us with a prior for the log-odds ratio of response on drug compared with a placebo given a number of project features. For a drug that is past its first approval (a so-called “life-cycle management” situation) such as NVS101, a TPP target of a odds ratio of 8.5 (given an assumed placebo responder rate of 15% this corresponds to a targeted 60% for the drug), we obtain a mixture prior for the log-odds ratio:\n\nmixture_prior &lt;- RBesT::mixnorm(\"Null component\"=c(0.08289228, 0, 0.91992526),\n                                \"TPP component\"=c(0.91710772, 2.14006616, 0.91992526))\n\nmixture_prior[,] |&gt; \n  as_tibble() |&gt;\n  mutate(Property = c(\"Mixture weight\", \n                      \"Mean of mixture component\", \n                      \"SD of mixture component\")) |&gt;\n  relocate(Property) |&gt;\n  gt() |&gt;\n  fmt_number(decimals=3) |&gt;\n  tab_spanner(columns=c(\"Null component\", \"TPP component\"), \n              label=\"Components of the mixture prior\")\n\n\n\n\n\n\n\nProperty\nComponents of the mixture prior\n\n\nNull component\nTPP component\n\n\n\n\nMixture weight\n0.083\n0.917\n\n\nMean of mixture component\n0.000\n2.140\n\n\nSD of mixture component\n0.920\n0.920\n\n\n\n\n\n\n\nNovartis associates can obtain this using the Novartis internal pos R package using the pos::benchmark_prior() function and applying the pos::as_rbest_mixnorm() function to the result, which converts the prior to a mixture prior object in the RBesT package.\nWhen we plot this mixture distribution, it looks as follows:\n\nplot(mixture_prior) + \n  xlab(\"Log-odds ratio for response\\non drug compared with placebo\") +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\nThe hypothetical single arm trial data for NVS101 are given below:\n\nnvs101_singlearm &lt;- tibble(trial=\"NVS101 Proof of Concept\",\n                           treatment=\"NVS101\",\n                           `trial duration` = \"12 weeks\",\n                           responders=8,\n                           patients=10) |&gt;\n  mutate(proportion=responders/patients,\n         long_study=0L)\n\nnvs101_singlearm |&gt;\n  gt() |&gt;\n  fmt_number(columns=\"proportion\", decimals=2)\n\n\n\n\n\n\n\ntrial\ntreatment\ntrial duration\nresponders\npatients\nproportion\nlong_study\n\n\n\n\nNVS101 Proof of Concept\nNVS101\n12 weeks\n8\n10\n0.80\n0\n\n\n\n\n\n\n\nLet us assume that there has been a placebo controlled Phase 3 trial in this indication for another drug EG-999, but no other trials with the same endpoint. The data for the hypothetical other drug are given below:\n\neg999_ph3 &lt;- tibble(trial=\"EG-999 Phase 3\",\n                    `trial duration` = \"52 weeks\",\n                    treatment=c(\"EG-999\", \"placebo\"),\n                    responders=c(19,2),\n                    patients=c(26, 13)) |&gt;\n  mutate(proportion=responders/patients,\n         long_study=1L)\n\neg999_ph3 |&gt;\n  gt() |&gt;\n  fmt_number(columns=\"proportion\", decimals=2)\n\n\n\n\n\n\n\ntrial\ntrial duration\ntreatment\nresponders\npatients\nproportion\nlong_study\n\n\n\n\nEG-999 Phase 3\n52 weeks\nEG-999\n19\n26\n0.73\n1\n\n\nEG-999 Phase 3\n52 weeks\nplacebo\n2\n13\n0.15\n1\n\n\n\n\n\n\n\nSuperficially, this extra information makes the NVS101 single arm data look promising, because the responder proportion on NVS101 is much higher than on placebo and even higher than on the EG-999 active treatment group. However, this is a non-randomized across trial comparison and it is not immediately clear how to quantify the strength of evidence for the efficacy of NVS101.\nSince we are not really interested in EG-999, the data we will use from the EG-999 Phase 3 trial will only be the placebo group. Thus, our the data we will use are the following:\n\nour_data &lt;- bind_rows(nvs101_singlearm,\n                      eg999_ph3 |&gt; filter(treatment != \"EG-999\")) |&gt;\n  mutate(treatment=factor(treatment, levels=c(\"placebo\", \"NVS101\")))\n\nour_data |&gt;\n  gt() |&gt;\n  fmt_number(columns=\"proportion\", decimals=2)\n\n\n\n\n\n\n\ntrial\ntreatment\ntrial duration\nresponders\npatients\nproportion\nlong_study\n\n\n\n\nNVS101 Proof of Concept\nNVS101\n12 weeks\n8\n10\n0.80\n0\n\n\nEG-999 Phase 3\nplacebo\n52 weeks\n2\n13\n0.15\n1",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#model-description",
    "href": "src/02l_single_arm_pos.html#model-description",
    "title": "7  Probability of success from a single arm trial",
    "section": "7.3 Model description",
    "text": "7.3 Model description\n\n7.3.1 Idea\nWith this available information and data, what do we do to come to a reasonable belief about the efficacy of NVS101 and the PoS of the drug development project? One approach that is foreseen in the Novartis PoS framework is to conduct an expert elicitation that relies on experts making judgements about the totality of the available evidence.\nAnother approach is to analyze the available data. However, we do not have direct data on\n\nhow NVS101 compares to a placebo group within the same trial (the proof of concept study was a single arm trial),\nhow much the true placebo group responder proportion varies from trial to trial, and\nhow much the true responder proportion changes with trial duration (for either NVS101 or placebo).\n\nThus, we will still require prior judgements on these questions. However, it could be the case that these questions are easier to make judgements about for experts than if we asked them directly about the efficacy of a drug compared with placebo.\n\n\n7.3.2 Model specification\nwe first express a mixed effects logistic regression model ignoring whether we have data to tell us something about all model parameters. We assume a binomial outcome \\(Y_{ij} \\sim \\text{Bin}(\\pi_{ij}, N_{ij})\\) for arm \\(j\\) of trial \\(i\\). Our model specifies that\n\\[\\begin{align}\n\\text{logit}(\\pi_{ij}) =& \\; \\beta_0  \\\\\n& + \\beta_1 \\times 1\\{\\text{treatment}_{ij} = \\text{NVS101}\\} \\\\\n& + \\beta_2 \\times\n1\\{\\text{duration} = \\text{52 weeks} \\} \\\\\n&+ \\beta_3 \\times\n1\\{\\text{treatment}_{ij} = \\text{NVS101 \\& duration} = \\text{52 weeks}\n\\}  \\\\\n& + \\theta_i,\n\\end{align}\\]\nwhere \\(\\theta_i \\sim N(0, \\tau^2)\\) is a random trial effect. Or to express this as a brms-formula:\n\nmodel_formula &lt;- bf(responders | trials(patients) ~ 1 + treatment + long_study + long_study:treatment + (1 | trial),\n                    # short notation for the rhs of this formula is\n                    # 1 + treatment*long_study + (1 | trial)\n                    center = FALSE, \n                    family = binomial(link=\"logit\"))\n\nNote that we use the option center=FALSE here in order to fully control the parametrization of the linear predictor. By default brms centers the design matrix and treats the intercept separately from the remaining covariates. While this is often desirable to do, this is not appropiate in this case where we want to control all details of the model and intend to set priors deliberately.\n\n\n7.3.3 Specification of prior distributions\nBefore we saw the EG-999 Phase 3 study, what might we have believed about the placebo group of such a study? To answer this might be a time consuming process that may require a review of the available literature and/or eliciting the judgements of experts. Conveniently, the sample size section of the clinical trial protocol of the EG-999 Phase 3 study contained a statement that the expected placebo responder rate might be in the range of 5% to 40%. We will interpret this to indicate a 80% prediction interval. The authors of the protocol do not really say so clearly, but we speculate that we cannot rely on this being a 95% prediction interval. Thus, if we assume a normal distribution for the predictive distribution, we get a normal distribution on the logit-scale with mean ( logit(0.4) + logit(0.05) )/2 \\(\\approx\\) -1.67 and standard deviation ( logit(0.4) - logit(0.05) ) / 2 / qnorm(0.9) \\(\\approx\\) 1.\n\nNote that the logit and inv_logit are convenient functions from the RBesT package for converting from probabilities to logit-probabilities and vice versa (in base R you can use qlogis and plogis instead).\n\nThe resulting prior distribution looks as shown in the figure below.\n\ntibble(mean=-1.67, sd=1.0) |&gt;\n  ggplot(aes(xdist = dist_normal(mu = mean, sigma = sd))) +\n  stat_halfeye(fill=\"darkorange\", alpha=0.5) +\n  scale_x_continuous(breaks=seq(-4,2,1), \n                     labels=paste0(seq(-4,2,1), \"\\n(=\", \n                                   round(inv_logit(seq(-4,2,1)),2)*100,\"%)\")) +\n  xlab(\"Average placebo log-odds\") +\n  ylab(\"Prior density\")\n\n\n\n\n\n\n\n\n\nNOTE: The ggdist and distributional packages are an amazing combo for understanding distributions. ggdist extends ggplot2 to help us with visualizing distributions (see here for more information). distributional lets us work with distributions including many commonly used ones, truncated distributions (e.g. half-normal as dist_truncated(dist_normal(mu = 0, sigma=2.5), lower=0)), distributions of transformed random variables (e.g. dist_transformed(dist=dist_normal(0, 0.5), transform=exp, inverse=log) |&gt; mean()) and mixture distributions (e.g. dist_mixture(dist_normal(0, 1), dist_normal(1.5, 5), weights = c(0.5, 0.5))). With the distributional package, we can also obtain information on distributions such as their mean, median, quantiles, variance, skewness and so on (e.g. dist_student_t(df=3, mu = 0, sigma = 1) |&gt; median()).\n\nNow that we have a prior for the average true response proportion, we consider how much the true proportion might vary from study to study. To set a prior distribution on the between trial SD, we asked experts about the narrowest and widest random effects distributions they consider plausible. When discussing with our experts, we fixed the random effects mean to a plausible value to make it easier for them to express their prior belief on a scale they are comfortable on. We then assumed that the same distribution width is approximately plausible for other mean values, too.\nFor a hypothetical scenario with an average true reponse proportion of 15%, clinical experts expressed that it would be unlikely that a few studies would not end up having a true responder proportion of 20% and that it is possible that variability could be so high that some could reach 40%.\nThus, we considered that for the narrowest plausible random effects distribution (conditional on the the middle of the distribution being 15%), there should be 2.5% probability that studies could vary to 20% or higher. Similarly, for the widest plausible distribution, we assigned 2.5% probability that studies would vary to 40% or more. Normal distributions on the logit scale that fulfill these criteria are shown in the figure below.\n\ntibble(mu=qlogis(0.15), sigma=c(0.18, 0.68) ) %&gt;% \n  mutate(res=map2(mu, sigma, \\(mu,sigma) tibble(x=seq(-2.5,0.5,0.001), y=dnorm(seq(-2.5,0.5,0.001), mu, sigma)))) %&gt;%\n  unnest(res) %&gt;%\n  mutate(grouping = factor(case_when(\n    (sigma==0.18 & x&gt;qlogis(0.2)) ~ 1L,\n    sigma==0.18 ~ 2L,\n    (x&gt;qlogis(0.4)) ~ 3L,\n    TRUE ~ 4L), levels=rev(1L:4L))) %&gt;%\n  ggplot(aes(x=x, y=y, ymin=0, ymax=y, col=grouping,  fill=grouping)) +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        legend.position=\"none\") +\n  geom_vline(xintercept=qlogis(0.15), lty=2, col=\"darkred\") +\n  #stat_slab(slab_alpha=0.5, n=10000) +\n  geom_ribbon(alpha=0.4) +\n  coord_cartesian(xlim=c(-2.2, 0)) +\n  scale_x_continuous(breaks=qlogis(seq(0.05, 0.5, 0.05)),\n                     labels=paste0(round(qlogis(seq(0.05, 0.5, 0.05)), 2), \"\\n=\", 100*seq(0.05, 0.5, 0.05), \"%\")) +\n  geom_text(aes(x=x, y=y, label=label),\n             data=tibble(x=c(-1.62, -1.4, -1.1, -0.9), y=c(2, 0.8, 0.6, 0.4), \n                         label=c(\"SD of 0.18\",\n                                 paste0(\"P(probability&gt;=20%)=\",round(100-100*pnorm(q=qlogis(0.2), qlogis(0.15), 0.18),1), \"%\"),\n                                 \"SD of 0.68\",\n                                 paste0(\"P(probability&gt;=40%)=\",round(100-100*pnorm(q=qlogis(0.4), qlogis(0.15), 0.68),1), \"%\")),\n                         mu=0, sigma=c(0.18, 0.18, 0.68, 0.68), grouping=factor(c(2L, 1L, 4L, 3L), levels=rev(1L:4L))), size=6, hjust = 0) +\n  xlab(\"True logit-response-probability for new trial\\ngiven average trial is -1.73 (=15%)\") +\n  ylab(\"Prior density\") +\n  scale_fill_manual(values=rev(c(\"darkred\", \"#E69F00\", \"darkblue\", \"#56B4E9\"))) +\n  scale_color_manual(values=rev(c(\"darkred\", \"#E69F00\", \"darkblue\", \"#56B4E9\"))) +\n  ggtitle(\"Narrowest & widest plausible random effects\\ndistribution conditional on a mean of logit(0.15)\")\n\n\n\n\n\n\n\n\nNow, we translate these beliefs into a prior for the between-trial SD. Given that we consider 0.18 and 0.68 as on the edge of what is plausible for the between-trial SD, we pick a log-normal distribution for the between-trial SD that has these two values approximately as its 2.5th and 97.5th percentiles. For such a normal distribution on the logit-scale centered on logit(0.15), this implies a standard deviation of (logit(0.2)-logit(0.15)) / qnorm(0.975) \\(\\approx\\) 0.18 to (logit(0.4)-logit(0.15)) / qnorm(0.975) \\(\\approx\\) 0.68. This can be described by a log-normal prior distribution on the standard deviation with location -1.06 and scale 0.35, as we can see by observing that dist_lognormal(mu = -1.06, sigma = 0.35) |&gt; quantile(p=c(0.025, 0.975)) |&gt; unlist() = 0.174, 0.688.\n\ntibble(mu=-1.06, sigma=0.35) |&gt;\n  ggplot(aes(xdist = dist_lognormal(mu=mu, sigma=sigma))) +\n  geom_vline(xintercept=0.18, col=\"#E69F00\", linewidth=2, lty=2) +\n  geom_vline(xintercept=0.68, col=\"#56B4E9\", linewidth=2, lty=2) +\n  geom_text(data=tibble(mu=c(0.3, 0.8), \n                        sigma=c(0.96, 0.5), \n                        label=c(\"SD of 0.18\", \"SD of 0.68\")),\n            aes(x=mu, y=sigma, label=label),\n            size=6, col=c(\"#E69F00\", \"#56B4E9\")) +\n  stat_halfeye(n=10000, fill=\"red\", alpha=0.6) +\n  coord_cartesian(xlim=c(0, 1.0)) +\n  ylab(\"Prior density\") +\n  xlab(\"Between trial SD\")\n\n\n\n\n\n\n\n\nWhen asked how much lower the responder proportion over 52 vs. 12 weeks would be, the clinicians judge it likely that the proportion would decline a little bit. Their rationale was that while the disease does not progress fast enough to change substantially over 9 months, a longer duration offers additional opportunities for a low red blood cell count to occur (even if by chance) that might lead to a blood transfusion.\nThe resulting prior distribution looks as shown in the figure below.\n\ntibble(mean=-0.1, sd=.1) |&gt;\n  ggplot(aes(xdist = dist_normal(mu = mean, sigma = sd))) +\n  stat_halfeye(fill=\"#e7298a\", alpha=0.5) +\n  scale_x_continuous(breaks=log(1+seq(-0.3, 0.2, 0.1)), \n                    labels=paste0(abs(round(seq(-0.3, 0.2, 0.1) * 100,0)), \n                                  \"%\\n\", \n                                  c(rep(\"lower\\nodds\", 4), rep(\"higher\\nodds\", 2)))) +\n  xlab(\"Log-odds of response over 52 compared with 12 weeks\") +\n  ylab(\"Prior density\")\n\n\n\n\n\n\n\n\nFinally, the clinicians were unsure whether a 52 week vs. a 12 week study would result in a higher or lower log-odds-ratio for drug vs. placebo. However, they expressed that the effect would be relatively small, because the drug should not be disease modifying (i.e. treatment effect should not grow over time) and there is no reason to speculate that the effect on red blood cell counts would somehow decline over time, either. On this basis, they expressed the following prior distribution:\n\ntibble(mean=0, sd=.1/1.96) |&gt;\n  ggplot(aes(xdist = dist_normal(mu = mean, sigma = sd))) +\n  stat_halfeye(fill=\"royalblue\", alpha=0.5) +\n  scale_x_continuous(breaks=log(1+seq(-0.3, 0.2, 0.1)), \n                    labels=paste0(abs(round(seq(-0.3, 0.2, 0.1) * 100,0)), \n                                  \"%\\n\", \n                                  c(rep(\"lower\\nodds\", 4), rep(\"higher\\nodds\", 2)))) +\n  xlab(\"Change in log-odds ratio of drug vs. placebo\\nwhen increasing trial duration from 12 to 52 weeks\") +\n  ylab(\"Prior density\")\n\n\n\n\n\n\n\n\nThus, we can set out prior distributions as below. To see what particular coefficients are named, get_prior(model_formula, our_data) is a useful command.\n\nbrms_prior &lt;- prior(class=b, coef=Intercept, normal(-1.67, 1.0)) + # prior average placebo effect \n  prior(class=sd, coef=Intercept, group=trial, lognormal( -1.06, 0.35)) + # prior on the between-study SD in  placebo log-odds\n  prior(class=b, coef=long_study, normal(-0.1, 0.1)) + # prior for effect of long-study (26 vs. 12 weeks) on responder rate\n  prior(class=b, coef=\"treatmentNVS101:long_study\", normal(0, 0.1/1.96)) + # Prior on the log-odds-ratio for drug vs. placebo when switching from shor to long study\n  prior(mixnorm(map_w, map_m, map_s), class=b, coef=treatmentNVS101) #  Syntax for specifying mixture prior\n\n\nNOTE: Thanks to the new functionality in the RBesT package, we were able to just write prior(mixture_prior, class=b, coef=treatmentNVS10) to specify a mixture prior. Alternatively, we could have written prior(mixnorm(map_w, map_m, map_s), class=b, coef=treatmentNovDrug). map_w, map_m and map_s would specify the weights, means and standard deviations of the mixture components, respectively. If we did not have this RBesT functionality available, we would have to manually write down the log-probability-density-function of the mixture, which involves weighted sums on the probability scale e.g. using the log_sum_exp function. For further details on the mixstanvars feature, please refer to its online manual.\n\n\n\n7.3.4 Model fitting and predictions\n\nbrmfit1 &lt;- brm(formula = model_formula,\n               data = our_data,\n               # By doing 11,000 samples per chain (default 4 chains) incl. 1000\n               # warmup samples, we'll get 4 * 10,000= 40,000 samples, which is\n               # what we'll need in the PoS app.\n               #iter = 11000, warmup=1000, \n               # Now we specify our prior distributions:\n               prior = brms_prior,\n               # Note that we define \"map=mixture_prior\" below. This\n               # matches the prefix \"map\" defined in the previous\n               # prior definition statement:\n               stanvars = mixstanvar(map=mixture_prior),\n               control = control_args,\n               seed = 5674574,\n               silent = 2, \n               refresh = 0\n               )\n\nWe can now generate predictions for a new Phase 3 trial for NVS101 compared with placebo of 52 week duration. While the predictions for each arm of the new trial are by default for the true log-odds of response, we can derive all desired quantities from this:\n\n# we are using the inv_logit function on rvars for which we need to\n# prepare the funnction to handle rvar arguments\nrvar_inv_logit &lt;- rfun(inv_logit)\n\nnew_ph3_trial &lt;- tibble(trial=\"New trial\",\n                        treatment=c(\"placebo\", \"NVS101\"),\n                        long_study=1,\n                        patients=c(25, 50))\n\nph3_predictions &lt;- new_ph3_trial |&gt;\n    tidybayes::add_linpred_rvars(brmfit1,\n                                 allow_new_levels=TRUE,\n                                 sample_new_levels=\"gaussian\",\n                                 value=\"logodds\") |&gt;\n    mutate(proportion=rvar_inv_logit(logodds))\n\n# One can have tibbles contain columns of rvars, which one can\n# recognize by the reported mean and standard deviation of the\n# sample they represent:\nph3_predictions\n\n# A tibble: 2 × 6\n  trial     treatment long_study patients      logodds   proportion\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;rvar[1d]&gt;   &lt;rvar[1d]&gt;\n1 New trial placebo            1       25  -1.6 ± 0.68  0.19 ± 0.10\n2 New trial NVS101             1       50   1.0 ± 0.78  0.71 ± 0.14",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#results",
    "href": "src/02l_single_arm_pos.html#results",
    "title": "7  Probability of success from a single arm trial",
    "section": "7.4 Results",
    "text": "7.4 Results\nHere is a plot of the predicted Phase 3 placebo and NVS101 responder proportion.\n\nwith(ph3_predictions,\n     tibble(proportion_NVS101  = proportion[2],\n            proportion_placebo = proportion[1])) |&gt;\n    tidybayes::unnest_rvars() |&gt;\n    ggplot(aes(x=proportion_placebo, y=proportion_NVS101)) +\n    geom_hex() +\n    scale_fill_continuous() +\n    coord_cartesian(xlim=c(0,1), ylim=c(0,1))\n\nWarning: Computation failed in `stat_binhex()`.\nCaused by error in `compute_group()`:\n! The package \"hexbin\" is required for `stat_bin_hex()`.\n\n\n\n\n\n\n\n\n\nAdditionally, we can look at various measures of a treatment effect:\n\nph3_predictions |&gt;\n    ggplot(aes(xdist=proportion)) +\n    stat_slabinterval() +\n    facet_wrap(~treatment, labeller=label_both, scales=\"free\") +\n    xlab(\"Proportion\") + ylab(NULL)\n\n\n\n\n\n\n\nwith(ph3_predictions,\n     tibble(measure=c(\"logodds\", \"lograte\", \"proportion_diff\"),\n            posterior=c(diff(logodds),\n                        diff(log(proportion)),\n                        diff(proportion)),\n            ref=c(1,1,0)\n            )) |&gt;\n    ggplot(aes(xdist=posterior)) +\n    stat_slabinterval() +\n    geom_vline(aes(xintercept=ref), lty=2, col=\"darkred\") +\n    facet_wrap(~measure, scales=\"free_x\") +\n    ylab(NULL)\n\n\n\n\n\n\n\n\nLet’s say the target product profile (TPP) for the drug demands a risk difference of &gt;= 45%. We can simulate Phase III outcomes to find the probability of meeting this TPP and having a significant study, which the Novartis internal PoS app would do for us.\nNote that the relativley compact code below does indeed perform a full trial simulation for possible outcomes for every draw of the posterior. The use of rvars hides essentially that we are handling entire posteriors here. However, note that we have to wrap the call to the fisher.test funcntion into rdo such that the expression is automatically evaluated for each draw of the posterior.\n\npos_simulations &lt;- new_ph3_trial |&gt;\n    tidybayes::add_predicted_rvars(brmfit1,\n                                   allow_new_levels=TRUE,\n                                   sample_new_levels=\"gaussian\",\n                                   value=\"pred\") |&gt;\n    mutate(predrate=pred/patients) |&gt;\n    select(treatment, pred, predrate, patients) |&gt;\n    pivot_wider(names_from=\"treatment\", values_from=c(\"pred\", \"predrate\", \"patients\")) |&gt;\n    mutate(riskdiff_crit = predrate_NVS101 - predrate_placebo &gt;= 0.45,\n           pvalue = rdo(fisher.test(x=matrix(c(pred_NVS101,  patients_NVS101  - pred_NVS101,\n                                               pred_placebo, patients_placebo - pred_placebo ),\n                                             2, 2 ) )[[\"p.value\"]] ),\n           `Achieving &gt;= 45% risk difference` = E(riskdiff_crit),\n           `Achieving significance (p&lt;=0.05)` = E(pvalue &lt;= 0.05),\n           `Achieving significance & TPP target` = E(riskdiff_crit * (pvalue &lt;= 0.05))\n           )\n\npos_simulations |&gt;\n    select(starts_with(\"Achieving\")) |&gt;\n    pivot_longer(cols=everything(), names_to=\"Outcome\", values_to = \"Probability\") |&gt;\n    gt() |&gt;\n    fmt_number(decimals=3)\n\n\n\n\n\n\n\nOutcome\nProbability\n\n\n\n\nAchieving &gt;= 45% risk difference\n0.673\n\n\nAchieving significance (p&lt;=0.05)\n0.952\n\n\nAchieving significance & TPP target\n0.673\n\n\n\n\n\n\n\nAs we can see, achieving the TPP risk difference is the greater hurdle than achieving statistical significance.\nAs part of the Novartis PoS framework, we would now account for the possibility of not obtaining an approval despite a positive Phase 3 study, a failure in Phase 3 due to a safety issue, and other risks (e.g. relating to drug manufacturing or market access).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#conclusion",
    "href": "src/02l_single_arm_pos.html#conclusion",
    "title": "7  Probability of success from a single arm trial",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIn the hypothetical case study, there was not a lot of data, but what there was appeared to be promising. Thus, one would not want to ignore it. In order to make use of it, we had to use informative prior distributions for several parameters of a logistic regression model. In this kind of situation, where we cannot let data inform our prior distributions, there is little choice, if we want an answer using the available data.\nGiven how our prior judgments directly affect the inference, we want ot ensure that the prior judgements reflect our best understanding of the available scientific evidence. For this purpose a formal prior elicitation (or predictions for some parameters from historical data) are options, but when those are not possibler, we should still try to ensure that prior choices are plausible to the whole team.\nNote that in this example, there was also an underlying laboratory measurement (longitudinal measurements of red blood cell counts). It might have been more efficient to work with these directly, but it would have been necessary to translate any results into predictions for the Phase III responder endpoint. Furthermore, the published external competitor data might not report these laboratory outcomes in sufficient detail.\nSingle arm studies are not ideal. However, when treatment effects are large compared to how much placebo group outcomes vary, we can still learn about drug efficacy from them at the cost of additional assumptions. As part of our case study, we tried to be explicit about our uncertainties and to transparently describe our prior judgements, as well as the rationale behind them. If someone disagrees with some of our prior distributions, they can then do a sensitivity analysis.\nA Bayesian approach is also well-suited for PoS calculations, because it lets us easily propagate uncertainties through different steps of a PoS analysis.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#exercises",
    "href": "src/02l_single_arm_pos.html#exercises",
    "title": "7  Probability of success from a single arm trial",
    "section": "7.6 Exercises",
    "text": "7.6 Exercises\nTo be added",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02l_single_arm_pos.html#references",
    "href": "src/02l_single_arm_pos.html#references",
    "title": "7  Probability of success from a single arm trial",
    "section": "References",
    "text": "References\n\n\nAllison, David B., Kishore M. Gadde, William Timothy Garvey, Craig A.\nPeterson, Michael L. Schwiers, Thomas Najarian, Peter Y. Tam, Barbara\nTroupin, and Wesley W. Day. 2012. “Controlled-Release\nPhentermine/Topiramate in Severely Obese Adults: A Randomized Controlled\nTrial (EQUIP).” Obesity 20 (2): 330–42. https://doi.org/10.1038/oby.2011.330.\n\n\nCombescure, C, DS Courvoisier, G Haller, and TV Perneger. 2012.\n“Meta-Analysis of Two-Arm Studies: Modeling the Intervention\nEffect from Survival Probabilities.” Statistical Methods in\nMedical Research 25 (2): 857–71. https://doi.org/10.1177/0962280212469716.\n\n\nDias, S., N. J. Welton, A. J. Sutton, and A. Ades. 2014.\n“NICE DSU Technical Support Document 2: A Generalised\nLinear Modelling Framework for Pairwise and Network Meta-Analysis of\nRandomised Controlled Trials.” Technical report. NICE\nDecision Support Unit.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari,\nand Donald B Rubin. 2014. Bayesian Data Analysis. Chapman\nTexts in Statistical Science Series. Third edit. https://doi.org/10.1007/s13398-014-0173-7.2.\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The\nPrior Can Often Only Be Understood in the Context of the\nLikelihood.” Entropy 19 (October). https://doi.org/10.3390/e19100555.\n\n\nHasselblad, Vic. 1998. “Meta-Analysis of Multitreatment\nStudies.” Medical Decision Making 18 (1): 37–43. https://doi.org/10.1177/0272989x9801800110.\n\n\nJanjigian, Yelena Y., Kohei Shitara, Markus Moehler, Marcelo Garrido,\nPamela Salman, Lin Shen, Lucjan Wyrwicz, et al. 2021. “First-Line\nNivolumab Plus Chemotherapy Versus Chemotherapy Alone for Advanced\nGastric, Gastro-Oesophageal Junction, and Oesophageal Adenocarcinoma\n(CheckMate 649): A Randomised, Open-Label, Phase 3 Trial.”\nThe Lancet 398 (July): 27–40. https://doi.org/10.1016/S0140-6736(21)00797-2.\n\n\nLilienthal, Jona, Sibylle Sturtz, Christoph Schürmann, Matthias Maiworm,\nChristian Röver, Tim Friede, and Ralf Bender. 2023. “Bayesian\nRandom‐effects Meta‐analysis with Empirical Heterogeneity Priors for\nApplication in Health Technology Assessment with Very Few\nStudies.” Research Synthesis Methods, December. https://doi.org/10.1002/jrsm.1685.\n\n\nLu, G., and A. E. Ades. 2004. “Combination of Direct and Indirect\nEvidence in Mixed Treatment Comparisons.” Statistics in\nMedicine 23 (20): 3105–24. https://doi.org/10.1002/sim.1875.\n\n\nNeuenschwander, Beat, and Heinz Schmidli. 2020. “Use of Historical\nData.” https://CRAN.R-project.org/package=RBesT.\n\n\nPiepho, H. P., E. R. Williams, and L. V. Madden. 2012. “The Use of\nTwo-Way Linear Mixed Models in Multitreatment Meta-Analysis.”\nBiometrics 68 (4): 1269–77. https://doi.org/10.1111/j.1541-0420.2012.01786.x.\n\n\nPiironen, Juho, and Aki Vehtari. 2017. “Sparsity Information and\nRegularization in the Horseshoe and Other Shrinkage Priors.”\nElectronic Journal of Statistics 11 (January): 5018–51. https://doi.org/10.1214/17-EJS1337SI.\n\n\nRöver, Christian, Ralf Bender, Sofia Dias, Christopher H. Schmid, Heinz\nSchmidli, Sibylle Sturtz, Sebastian Weber, and Tim Friede. 2021.\n“On Weakly Informative Prior Distributions for the Heterogeneity\nParameter in Bayesian Random-Effects Meta-Analysis.” Research\nSynthesis Methods 12 (January): 448–74. https://doi.org/10.1002/jrsm.1475.\n\n\nSimpson, Daniel P., Håvard Rue, Thiago G. Martins, Andrea Riebler, and\nSigrunn H. Sørbye. 2014. “Penalising Model Component Complexity: A\nPrincipled, Practical Approach to Constructing Priors.”\nStatistical Science 32 (February): 1–28. https://doi.org/10.1214/16-STS576.\n\n\nStan. 2024. “Prior Choice Recommendations.” 2024. https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations.\n\n\nTurner, Rebecca M, Dan Jackson, Yinghui Wei, Simon G Thompson, and\nJulian P T Higgins. 2015. “Predictive Distributions for\nBetween-Study Heterogeneity and Simple Methods for Their Application in\nBayesian Meta-Analysis.” Statistics in Medicine 34:\n984–98. https://doi.org/10.1002/sim.6381.\n\n\nWarren, Fiona C., Keith R. Abrams, and Alex J. Sutton. 2014.\n“Hierarchical Network Meta-Analysis Models to Address Sparsity of\nEvents and Differing Treatment Classifications with Regard to Adverse\nOutcomes.” Statistics in Medicine 33 (14): 2449–66. https://doi.org/10.1002/sim.6131.\n\n\nZhang, Yan Dora, Brian P. Naughton, Howard D. Bondell, and Brian J.\nReich. 2022. “Bayesian Regression Using a Prior on the Model Fit:\nThe R2-D2 Shrinkage Prior.” Journal of the American\nStatistical Association 117: 862–74. https://doi.org/10.1080/01621459.2020.1825449.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability of success from a single arm trial</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html",
    "href": "src/02b_dose_finding.html",
    "title": "8  Dose finding",
    "section": "",
    "text": "8.1 Background",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#what-role-does-dose-response-modeling-play-in-drug-development",
    "href": "src/02b_dose_finding.html#what-role-does-dose-response-modeling-play-in-drug-development",
    "title": "8  Dose finding",
    "section": "8.2 What role does dose response modeling play in drug development?",
    "text": "8.2 What role does dose response modeling play in drug development?\nIn clinical drug development, one key question is what dose of a drug should be used to treat a disease. Ideally, we would want a dose that achieves (nearly) optimal efficacy, while at the same time minimizing side effects to optimize the benefit-risk balance. In many therapeutic areas the relationship between dose and efficacy is characterized in Phase IIb clinical trials that randomize patients to received either one of several doses of a drug or a matching placebo.\nIn such studies, it would be inefficient to try a lot of doses and just look at the performance of each dose in isolation. Instead, we know that biologically there should be some smooth function that describes the relationship between dose and efficacy. We can exploit this knowledge to make the analysis of such studies more efficient. There are a number of methods for analyzing such trials that exploit that the true expected difference to placebo will follow some smooth function of the dose. However, we typically do not know what smooth function best approximates the true underlying dose response function. Obvious candidate functions include monotone functions that eventually plateau, and functions that initially achieve a maximum and then decline.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#data-pathway-asthma-example",
    "href": "src/02b_dose_finding.html#data-pathway-asthma-example",
    "title": "8  Dose finding",
    "section": "8.3 Data: PATHWAY asthma example",
    "text": "8.3 Data: PATHWAY asthma example\nWe will use the PATHWAY trial as an example. This was a placebo-controlled randomized trial of three different tezepelumab dosing regimens compared with placebo in severe asthma. Two of the tezepelumab dosing regimens were given every 4 weeks. We will treat the third regimen of 280 mg every 2 weeks as if it had been 560 mg every 4 weeks. The primary endpoint of PATHWAY was the annualized rate of asthma exacerbations. The plot below shows the published estimates with 95% confidence intervals per dose. Many standard dose response modeling approaches do not require access to individual patient data - which in this case is not publically available -, but can be conducted given estimates and standard error for each dose.\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8979476)\n\n\n# This is the PATHWAY DRF data by group\npathway = tibble(dose = c(0, 70, 210, 280*2),\n                 group = c(\"placebo\", \"tezepelumab 70 mg q4w\",\n                           \"tezepelumab 210 mg q4w\", \"tezepelumab 280 mg q2w\"),\n                 est = log(c(0.67, 0.26, 0.19, 0.22)),\n                 stderr = c(0.10304, 0.17689, 0.22217, 0.19108))\n\n# Simple plot of the data\npathway %&gt;%\n  ggplot(aes(x=dose, y=est, label=str_wrap(group, 12), col=group,\n             ymin=est-stderr*qnorm(0.975), ymax=est+stderr*qnorm(0.975))) +\n  geom_errorbar(width=10) +\n  geom_point() +\n  scale_y_continuous(breaks=log(seq(0.1,1,0.1)), labels=seq(0.1,1,0.1)) +\n  ylab(\"Annualized rate (95% CI)\") +\n  xlab(\"Tezepelumab dose (mg/month)\") +\n  geom_text_repel(nudge_x=c(25, 30, 60, -30), segment.color = NA)\n\n\n\n\n\n\n\n\nOr to show this as rate ratios compared with the placebo group:\n\n# This is the PATHWAY DRF log-rate-ratios vs. placebo\npathway_deltas = tibble(dose= c(0, 70, 210, 280*2),\n                        group = c(\"reference level\",\n                                  \"tezepelumab 70 mg q4w vs. placebo\",\n                                  \"tezepelumab 210 mg q4w vs. placebo\", \n                                  \"tezepelumab 280 mg q2w vs. placebo\"),\n                        logRR= c(0,log(c(1-0.61, 1-0.71, 1-0.66))),\n                        .lower90 = log(c(NA_real_, 1-0.75, 1-0.82, 1-0.79)), # Paper reports 90% CIs\n                        .upper90 = log(c(NA_real_, 1-0.39, 1-0.53, 1-0.47)),\n                        stderr = (.upper90-.lower90)/2/qnorm(0.95),\n                        .lower = logRR-stderr*qnorm(0.975), # Get 95% CIs\n                        .upper = logRR+stderr*qnorm(0.975))\n# Plot log-rate ratios\npathway_deltas %&gt;%\n  ggplot(aes(x=dose, y=logRR, \n             label=str_wrap(group, 12), col=group,\n             ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0) +\n  geom_errorbar(width=10) +\n  #geom_errorbar(width=10, aes(ymin=.lower90, ymax=.upper90)) +\n  geom_point() +\n  scale_y_continuous(breaks=log(seq(0.1,1,0.1)), labels=seq(0.1,1,0.1)) +\n  scale_x_continuous(limits=c(0,600)) +\n  ylab(\"Exacerbation rate ratio (95% CI)\") +\n  xlab(\"Tezepelumab dose (mg/month)\") +\n  geom_text_repel(nudge_x=c(25, 30, 60, -30), nudge_y=c(-0.15, 0, 0, 0), \n                  segment.color = NA)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#model-description",
    "href": "src/02b_dose_finding.html#model-description",
    "title": "8  Dose finding",
    "section": "8.4 Model description",
    "text": "8.4 Model description\nTo conduct dose response analyses under model uncertainty a number of approaches such as the Multiple Comparison Procedures and Modeling (MCP-Mod), the generalized MCP-Mod approach and Gaussian process based approaches are popular. Within the generalized MCP-Mod framework model averaging is an attractive approach for dealing with model uncertainty that has performed well in simulation studies. Here, we take a Bayesian approach to model averaging in the MCP-Mod framework, which Gould (2018) called BMA-Mod (BMA stands for Bayesian model averaging).\nWhile more complex mechanistic pharmacokinetic/pharmacodynamic (PK/PD) models are often important, here, we discuss directly modelling the relationship between dose and some clinical outcome in patients.\nBMA-Mod, just as MCP-Mod, uses a set of candidate models that might describe dose response relationships. In the subsections that follow, we describe each model.\n\n8.4.1 The likelihood function into which we put the dose response model\nNo matter whether we use summary data consiting of estimates with standard errors with each treatment group, or whether we have individual patient data (IPD) for a continuous, binary, count, time-to-event or ordinal outcome, our regression model will be one of the dose response models we will describe in the subsequent sections.\nWhat will differ is the likelihood function, into which we insert the dose response function. In the PATHWAY example, we will use that the estimates for log-mean-event-rate for each dose level \\(i=1,\\ldots,I\\) are approximately indpendent with \\(\\hat{\\theta}_i \\sim N(\\theta_i, \\text{SE}(\\hat{\\theta}_i))\\) distributions. Thus, we can use a Gaussian likelihood with known standard deviation. In case of IPD, we would instead use suitable IPD-likelihoods. This only requires minimal changes in the code for the examples.\n\n\n8.4.2 The sigmoid-Emax and the Emax model\nA very popular dose response model is the sigmoid-Emax model. This model assumes that the outcome for each dose group can be described by a function of the form \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\text{E}_\\text{max} \\times \\frac{\\text{dose}^h}{\\text{dose}^h + \\text{ED}_{50}^h}.\\] This is a monotone curve that starts at no difference to placebo at dose 0 (the expected placebo response is \\(\\text{E}_0\\)) and plateaus at a maximum effect of size \\(\\text{E}_\\text{max} \\in (-\\infty, \\infty)\\). 50% of the maximum effect is reached at the dose \\(\\text{ED}_{50} \\in (0, \\infty)\\). The steepness of the curve is determined by the so-called Hill parameter \\(h \\in (0, \\infty)\\). For \\(h=1\\), we have the simple Emax model, for \\(h&gt;1\\) the dose response curve is steeper around \\(\\text{ED}_{50}\\) (and plateaus faster) than for a simple Emax model, while for \\(h&lt;1\\) the curve is less steep. Typical values of \\(h\\) are smaller than 5 and usually at most 3, so assigning a prior that puts most of the prior weight for \\(\\log h\\) between \\(-\\log 5\\) and \\(\\log 5\\) makes sense.\n\n\n\n\n\n\n\n\n\nThe Emax model is a simple model with some biological plausibility. It is the equation that results from assuming the drug concentrations at the site of drug action are proportional to the administered dose, that drug binds to and unbinds from a receptor at a rate proportional to the drug concentration, and that drugs effects are proportional to receptor occupancy. And, it assumes that all these relationships are exactly the same for all patients. All of these assumptions can be relaxed/avoided when individual patient data (and even better, if also the necessary data for PK/PD modeling) are available.\n\n\n8.4.3 The modified beta model\nAnother possible dose response model is the modified beta model. \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\text{E}_{\\text{max }2} \\times \\frac{(\\delta_1 + \\delta_2)^{\\delta_1 + \\delta_2}}{\\delta_1^{\\delta_1} + \\delta_2^{\\delta_2}} \\times (\\frac{\\text{dose}}{S})^{\\delta_1} \\times (1-\\frac{\\text{dose}}{S})^{\\delta_2}\\] This model is capable of describing a non-monotone dose response.\n\n\n8.4.4 Exponential model, and linear and quadratic functions of dose or log-dose\nThe exponential model \\[f(\\text{dose}; \\text{parameters}) = b_1 + b_2 (\\exp(b_3 \\times \\text{dose})-1)\\] with \\(b_3&lt;0\\) was also discussed by Gould.\nGould also considered linear and quadratic functions of dose or log-dose. I.e. \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta \\times \\text{dose},\\] \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta \\times \\log (\\text{dose}),\\] \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta_1 \\times \\text{dose} + \\beta_2 \\times \\text{dose}^2\\] or \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta_1 \\times \\log (\\text{dose}) + \\beta_2 \\times (\\log (\\text{dose}))^2.\\] These models are perhaps better suited towards modeling the dose-response relationship within the range of tested dose and would not necessarily be expected to extrapolate well beyond the data range. Nevertheless, they may be useful for providing a more flexible set of models.\n\n\n8.4.5 Bayesian Model averaging\nIf we have multiple models that are similarly plausible given our prior information and the data we observed, then picking a single model out of these ignores the model uncertainty. For this reason, model averaging is known to be a good approach for dealing with model uncertainty. It assigns greater weight to more plausible models and can - given enough evidence - give all weight to a single clearly best model (and also give near-zero weight to clearly inappropriate models). So, - given enough data - it effectively results in model selection, but as long as we only have limited information multiple models will contribute to our inference.\nGould proposes to use a weighted combination of the predictions for each dose level from each of the candidate models and suggests to base the posterior model weigths on predictive likelihood weights.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#implementation-and-results",
    "href": "src/02b_dose_finding.html#implementation-and-results",
    "title": "8  Dose finding",
    "section": "8.5 Implementation and results",
    "text": "8.5 Implementation and results\nWe will now fit the various dose response models introduced in the statistical model section using brms. We will start with the sigmoid-Emax model and look at the things brms let us do easily with a single fitted model. We will not repeat that level of exploration for other models, but you may wish to do so in practice.\n\n8.5.1 Fitting dose response models with brms: sigmoid-Emax model\nWe use the capabilities of brms for fitting non-linear models. For example, a sigmoid-Emax model can be fitted as follows:\n\n# Fitting a sigmoid Emax model\nbrmfit1 &lt;-  brm( bf( est | se(stderr) ~ E0 + Emax * dose^h/(dose^h + ED50^h),\n                    nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)),\n                    E0 ~ 1, logED50 ~ 1, logh ~ 1, Emax ~ 1, \n                    nl=TRUE, family=gaussian()),\n                data=pathway,\n                prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                          prior(normal(0,1), nlpar=\"logh\"),\n                          prior(normal(0,1), nlpar=\"Emax\"),\n                          prior(normal(4,2), nlpar=\"logED50\")),\n                control=list(adapt_delta=0.999),\n                seed=3624)\n\nNote that with individual patient data, we would not write est | se(stderr) ~, but would instead use outcome ~ and bf() + negbinomial() (and similarly, e.g. + gaussian() for continuous data). With the settings above, you might still experience a divergent transition or two and you may wish to increase adapt_delta to 0.9999 or 0.99999, or alternatively try to come up with a way to reparameterize the model.\n\n# Summarize model fit\nsummary(brmfit1)\n\nWarning: There were 1 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * dose^h/(dose^h + ED50^h) \n         h ~ exp(logh)\n         ED50 ~ exp(logED50)\n         E0 ~ 1\n         logED50 ~ 1\n         logh ~ 1\n         Emax ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept         -0.42      0.10    -0.61    -0.21 1.00     2050     2044\nlogED50_Intercept     2.73      1.29    -0.10     4.99 1.00     1251     1246\nlogh_Intercept        0.00      0.95    -1.82     1.85 1.00     1397     1605\nEmax_Intercept       -1.28      0.30    -2.05    -0.83 1.00     1502     1168\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo what does the model fit look like? We have two options for looking at this. The easiest and fastest uses the conditional_effects function from brms.\n\nplot(conditional_effects(brmfit1, \"dose\"), \n     points = TRUE)\n\n\n\n\n\n\n\n\nThe downside is that in a controlled trial we are primarily interested in the contrast with placebo and that we would like to see the confidence intervals around the point estimates for each dose. We can obtain that with a little bit more code:\n\n# We are going to predict the annualized rate for all doses from 0,1,2,...600.\n# We are setting stderr=0 to avoid predicting with sampling variability.\ndr_curve_data = tibble(dose=as.integer(seq(0,600)), stderr=0)\n\n# Now we get the predictions and turn this into a dataset with one row per dose per MCMC sample\ndr_curve_pred &lt;- tidybayes::add_predicted_draws(dr_curve_data, brmfit1) %&gt;%\n  ungroup() %&gt;%\n  transmute(dose, .sample = .draw, pred = .prediction)\n\n# Now we get the rate ratio by subtracting the annualized placebo rate for each MCMC sample  \ndr_curve_pred = dr_curve_pred %&gt;%\n  left_join(dr_curve_pred  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;%\n  dplyr::select(dose, .sample, logRR)\n\n# Now we get median prediction, 50 and 95% credible intervals \n# and plot those with the original data overlaid.\np1 = dr_curve_pred  %&gt;%\n  dplyr::select(-.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"red\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"red\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkred\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"SigMod Emax model fit\"), col=\"darkred\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(p1)\n\n\n\n\n\n\n\n\nWe can also plot the dose response curve for each MCMC sample:\n\n dr_curve_pred  %&gt;%\n  ggplot(aes(x=dose, y=logRR, group=factor(.sample))) +\n  geom_hline(yintercept=0)  +\n  geom_line(col=\"darkred\", alpha=0.1, size=0.25) +\n  geom_point(data=pathway_deltas %&gt;% mutate(.sample=NA_integer_)) +\n  geom_errorbar(data=pathway_deltas %&gt;% mutate(.sample=NA_integer_), \n                aes(ymin=.lower, ymax=.upper), width=10) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\n\n\n\n\n\n\n\nNote, that this also makes it really easy to determine a distribution for the lowest dose that achieves a 50% relative rate reduction:\n\ndr_curve_pred  %&gt;%\n  filter(logRR&lt;log(0.5) | (logRR&gt;=log(0.5) & dose==600)) %&gt;%\n  group_by(.sample) %&gt;%\n  summarize(dose = min(dose), logRR=max(logRR)) %&gt;%\n  mutate(`Has 50% RRR`=(logRR&lt;log(0.5))) %&gt;%\n  ggplot(aes(x=dose, y=\"Dose with\\n&gt;50% RRR\")) +\n  stat_halfeye() +\n  coord_cartesian(xlim=c(0,200)) +\n  xlab(\"Tezepelumab dose [mg/month]\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nAs the plot below shows the data resulted in a substantially different posterior distribution compared with the prior distribution for the logED50 parameter and the Emax parameter. These parameters are reasonably well-identified even with just 3 doses - although the ED50 would be better estimated, if doses with approximately 50% of the Emax (maximum relative rate reduction) had been included in the study. In contrast, the log-Hill-parameter is poorly identified, when there are very few doses (as in this study), which is why the posterior distribution is more or less the prior distribution for the logED50 parameter.\n\nbrmfit1 %&gt;%\n  spread_draws(b_logED50_Intercept, b_logh_Intercept, b_Emax_Intercept) %&gt;%\n  pivot_longer(cols=c(\"b_logED50_Intercept\", \"b_logh_Intercept\", \"b_Emax_Intercept\")) %&gt;%\n  ggplot(aes(x=value)) +\n  theme_bw(base_size=16) +\n  geom_density() +\n  geom_line(data=tibble(name=c(rep(\"b_logED50_Intercept\",151),\n                               rep(\"b_logh_Intercept\",71),\n                               rep(\"b_Emax_Intercept\",61)),\n                        xval = c(seq(-5,10,0.1),\n                                 seq(-3,4,0.1),\n                                 seq(-3,3,0.1)),\n                        density=\n                          case_when(str_detect(name,\"ED50\") ~ dnorm(xval,mean=4,sd=2),\n                                    str_detect(name,\"logh\") ~  dnorm(xval,mean=0,sd=1),\n                                    TRUE ~ dnorm(xval,mean=0,sd=1))),\n            aes(x=xval, y=density), col=\"red\") +\n  geom_text(data=tibble(name=\"b_Emax_Intercept\", density=0.6, xval=1),\n            aes(x=xval, y=density, label=\"prior\"), col=\"red\", size=8) +\n  geom_text(data=tibble(name=\"b_Emax_Intercept\", density=1, xval=0.5),\n            aes(x=xval, y=density, label=\"posterior\"), col=\"black\", size=8) +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n8.5.2 Fitting dose response models with brms: modified beta model\nWe now fit the modified beta model that can capture non-monotone dose response relationships.\n\n# Try a modified beta model\nbrmfit2 &lt;-  brm( bf( est | se(stderr) ~ E0 + Emax*(delta1+delta2)^(delta1+delta2)/(delta1^delta1*delta2^delta2)*(dose/850)^delta1*(1-dose/850)^delta2,\n                   E0 ~ 1, delta1 ~ 1, delta2 ~ 1, Emax ~ 1,\n                   nl=TRUE, \n                   family = gaussian()),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(lognormal(0,1), nlpar=\"delta1\", lb=0),\n                         prior(lognormal(0,1), nlpar=\"delta2\", lb=0)),\n               control=list(adapt_delta=0.999),\n               save_pars = save_pars(all = TRUE),\n               seed = 7304)\n\nLet us summarize the model fit:\n\nsummary(brmfit2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * (delta1 + delta2)^(delta1 + delta2)/(delta1^delta1 * delta2^delta2) * (dose/850)^delta1 * (1 - dose/850)^delta2 \n         E0 ~ 1\n         delta1 ~ 1\n         delta2 ~ 1\n         Emax ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept        -0.43      0.10    -0.63    -0.23 1.00     1535     2122\ndelta1_Intercept     0.45      0.22     0.13     0.98 1.00     1303     1812\ndelta2_Intercept     0.75      0.42     0.16     1.75 1.00     1304     1577\nEmax_Intercept      -1.30      0.21    -1.73    -0.90 1.00     1640     1769\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo what does the model fit look like? We could, again, just use plot(conditional_effects(brmfit2, \"dose\"), points = TRUE) or get a slightly more tailored plot.\n\n# Now we get the predictions and turn this into a dataset with one row per dose per MCMC sample\ndr_curve_pred2 &lt;- tidybayes::add_predicted_draws(dr_curve_data, brmfit2) %&gt;%\n  ungroup() %&gt;%\n  transmute(dose, .sample = .draw, pred = .prediction)\n\n# Now we get the rate ratio by subtracting the annualized placebo rate for each MCMC sample  \ndr_curve_pred2 = dr_curve_pred2 %&gt;%\n  left_join(dr_curve_pred2  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;%\n  dplyr::select(dose, .sample, logRR)\n\n# Now we get median prediction, 50 and 95% credible intervals \n# and plot those with the original data overlaid.\np2 = dr_curve_pred2  %&gt;%\n  dplyr::select(-.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"royalblue\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"royalblue\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkblue\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"modified-beta model fit\"), col=\"darkblue\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(p2)\n\n\n\n\n\n\n\n\n\n\n8.5.3 Perform Bayesian model averaging\nNow we have two fitted model. We can see both of them together below:\n\np1 + p2\n\n\n\n\n\n\n\n\nIn practice, we do not really know, whether one of these is the true data generating model. In fact, realistically both models are going to be at least somewhat misspecified, because we made no attempt to fully model the full complexity of the underlying biological system. So, really, the question is which of the two models provides the better approximation. At a glance both seem to fit the data somewhat decently, so it seems unlikely that we can completely rule one of the two models out. That is where Bayesian model averaging comes in.\nFirst, we need to compare how well each model fits. As it turns out, when you just have 4 data points (due to us using summary data), it is problematic to just use the “standard” brms::loo function to perform approximate leave-one-out cross-validation (LOO-CV) based on the posterior likelihood as implemented in the loo package, as can be seen by running\n\nloo(brmfit1)\n\nWarning: Found 1 observations with a pareto_k &gt; 0.7 in model 'brmfit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo      1.0 0.3\np_loo         1.3 0.5\nlooic        -2.1 0.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 0.9]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3     75.0%   852     \n   (0.7, 1]   (bad)      1     25.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 0      0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nand\n\nloo(brmfit2)\n\nWarning: Found 4 observations with a pareto_k &gt; 0.7 in model 'brmfit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo     -0.4 0.4\np_loo         2.7 0.7\nlooic         0.7 0.8\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 0.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     0      0.0%   &lt;NA&gt;    \n   (0.7, 1]   (bad)      2     50.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 2     50.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nInstead, we actually fit each model leaving each dose group out. We could probably do something more sophisticated by simulating consistent individual patient data (and then performing approximate LOO-CV) or parametric sampling from the normal distribution around the point estimates.\n\n(loo_exact_brmfit1 &lt;- kfold(brmfit1, folds = \"loo\"))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 3.1 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.5 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\n\n\nBased on 4-fold cross-validation.\n\n           Estimate  SE\nelpd_kfold     -0.5 1.3\np_kfold         2.8 1.9\nkfoldic         1.0 2.7\n\n(loo_exact_brmfit2 &lt;- kfold(brmfit2, folds = \"loo\"))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.0 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.4 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.5 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 2.0 seconds.\n\n\n\nBased on 4-fold cross-validation.\n\n           Estimate  SE\nelpd_kfold     -2.1 1.7\np_kfold         4.4 2.2\nkfoldic         4.2 3.4\n\n\nNow, we can compare the models via\n\nloo_compare(loo_exact_brmfit1, loo_exact_brmfit2)\n\n        elpd_diff se_diff\nbrmfit1  0.0       0.0   \nbrmfit2 -1.6       0.8   \n\n\n\n8.5.3.1 Approach 1 (manual without brms)\nLet us say that we a-priori assign a 75% probability to the SigEmax model and 25% to the modified beta model.\n\n# follows: \n# Gould, A. L. (2019). BMA‐Mod: A Bayesian model averaging strategy for determining dose‐response relationships in the presence of model uncertainty. Biometrical Journal, 61(5), 1141-1159.\n# https://dx.doi.org/10.1002/bimj.201700211\nprior_weights = c(0.75,0.25)\nposterior_weigths = c(prior_weights[1]*exp(loo_exact_brmfit1$estimates[\"elpd_kfold\", \"Estimate\"]), \n                      prior_weights[2]*exp(loo_exact_brmfit2$estimates[\"elpd_kfold\", \"Estimate\"]))\n(posterior_weigths = posterior_weigths/sum(posterior_weigths))\n\n[1] 0.93629621 0.06370379\n\n\nAs we can see, using leaving-one-dose-group out cross-validation, suggests that the SigEmax model should be given most of the weight. A-posteriori, we assign 93.6% weight to the SigEmax model and 6.4% weight to the modified beta model. However, if we had done approximate LOO-CV with individual patient data, the modified beta model might have been given more weight.\nWhen we average the predictions of the two models with these weights, we get the dose response curve below.\n\nbma_curve = dr_curve_pred %&gt;%\n  inner_join(dr_curve_pred2, by=c(\".sample\", \"dose\")) %&gt;%\n  mutate(logRR = posterior_weigths[1]*logRR.x + posterior_weigths[2]*logRR.y) %&gt;%\n  dplyr::select(-logRR.x, -logRR.y, -.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95))\n\n\npbma = bma_curve %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"orange\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"orange\", alpha=0.5) +\n  geom_line(col=\"darkorange\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"Bayesian model averaging\"), col=\"darkorange\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose\")\n\nplot( ( (p1 + theme(axis.title.x=element_blank()) + ggtitle(\"Sigmoid Emax model\")) | \n          (p2 + theme(axis.title.y=element_blank(),\n                      axis.title.x=element_blank()) + \n             ggtitle(\"Modified beta model\"))) / \n        (pbma + ggtitle(\"Bayesian model averaging\") | plot_spacer()) )\n\n\n\n\n\n\n\n\n\n\n8.5.3.2 Approach 2: Using brms functions\nBefore, we needed to do a lot of manual work to get model-averaged predictions, but the below using brms::pp_average would be a lot simpler, if we could rely on the approximate LOO-CV.\n\n8.5.3.2.1 Approach 2a: Not appropriate here\n\nbma_brms_curve = dr_curve_data %&gt;%\n  dplyr::select(-stderr) %&gt;%\n  bind_cols(as_tibble(\n    t(pp_average(brmfit1, brmfit2, newdata=dr_curve_data, \n                 weights=\"loo\", # Not a good option in this case.\n                 summary=F)),\n    .name_repair=vctrs::vec_as_names(repair=\"unique\", quiet =T))) %&gt;%\n  pivot_longer(cols=starts_with(\"V\"), names_to=\".sample\", values_to=\"pred\") %&gt;%\n  mutate(.sample = as.integer(str_extract(.sample, \"[0-9]+\")))\n\n# Forming differences to placebo by merging with the placebo (dose=0) predictions for each sample\nbma_brms_curve = bma_brms_curve %&gt;%\n  left_join(bma_brms_curve  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;% # Calculate log rate ratio (difference on log scale vs. placebo)\n  dplyr::select(dose, .sample, logRR)\n\n\n\n8.5.3.2.2 Approach 2b: Using leave-one-dose-out CV\nBecause we want to avoid using approximate LOO-CV in this case, we first replace the internally calculated approximate LOO-CV results with our own\n\nbrmfit1$criteria$loo &lt;- loo_exact_brmfit1\nbrmfit2$criteria$loo &lt;- loo_exact_brmfit2\n(w_dose &lt;- model_weights(brmfit1, brmfit2, weights = \"loo\"))\n\n brmfit1  brmfit2 \n0.830486 0.169514 \n\n\nWe can then do the following:\n\nbmapreds1 &lt;- posterior_epred(brmfit1, newdata = dr_curve_data)\nbmapreds2 &lt;- posterior_epred(brmfit2, newdata = dr_curve_data)\npe_avg &lt;- bmapreds1 * w_dose[1] + bmapreds2 * w_dose[2]\nrownames(pe_avg) &lt;- 1:nrow(pe_avg)\n\n# log-rate predictions for each dose\npe_avg = pe_avg %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;%\n  bind_cols(dr_curve_data %&gt;% dplyr::select(dose)) %&gt;%\n  pivot_longer(cols=-dose, names_to = \".sample\", values_to=\"pred\") \n\n# form difference to placebo (log-rate-ratio) and plot\n pbma2 = pe_avg %&gt;%\n  left_join(y=pe_avg %&gt;% filter(dose==0) %&gt;% dplyr::select(-dose), \n            by=\".sample\") %&gt;%\n  mutate(logRR=pred.x-pred.y) %&gt;%\n  dplyr::select(-.sample, -pred.x, -pred.y) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"green\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"darkgreen\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkgreen\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"BMA (LOO weights)\"), col=\"darkgreen\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(pbma2)\n\n\n\n\n\n\n\n\n\n\n\n8.5.3.3 Why did we do this form of Bayesian model averaging?\nWhy are we doing it like this? Can we not just fit a single model like this (as an example where we average a sigmoid-Emax and a modified beta model)? \\[\n\\begin{aligned}\nf(\\text{dose}; \\text{parameters}) = & \\text{E}_0 + w \\times \\text{E}_{\\text{max }1} \\times \\frac{\\text{dose}^h}{\\text{dose}^h + \\text{ED}_{50}^h} \\\\\n  & + (1-w) \\times \\text{E}_{\\text{max }2} \\times \\frac{(\\delta_1 + \\delta_2)^{\\delta_1 + \\delta_2}}{\\delta_1^{\\delta_1} + \\delta_2^{\\delta_2}} \\times (\\frac{\\text{dose}}{S})^{\\delta_1} \\times (1-\\frac{\\text{dose}}{S})^{\\delta_2}\n\\end{aligned}\n\\] brms certainly lets us specify such a model (see below). But, part of the reason why we do not use it, is that the model parameters are really badly identified: you can still obtain a similarly good fit at each observed dose level by making one of the two models fit better for one dose (and worse for another dose), if the other model in turn is made to fit worse for that dose (and better for the other dose).\nIf you run the code below, you will see how much the sampler struggles to fit this model, if we try to fit it. That is the case, even if we make things easier and fix the model weights to 50:50.\n\n# The model below does not work so well\nbrmfit3 &lt;-  brm( bf( est | se(stderr) ~ E0 + 0.5 * Emax * dose^h/(dose^h + ED50^h) + 0.5*Emax2*(delta1+delta2)^(delta1+delta2)/(delta1^delta1*delta2^delta2)*(dose/850)^delta1*(1-dose/850)^delta2,\n                   E0 ~ 1, ED50 ~ 1, h ~ 1, Emax ~ 1,\n                   #modelwgt ~ 1,\n                   delta1 ~ 1, delta2 ~ 1, Emax2 ~ 1,\n                   nl=T),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(lognormal(0,1), nlpar=\"h\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(lognormal(4,2), nlpar=\"ED50\"),\n                         #prior(beta(4,1), nlpar=\"modelwgt\", lb=0, ub=1),\n                         prior(normal(0,1), nlpar=\"Emax2\"),\n                         prior(lognormal(0,1), nlpar=\"delta1\", lb=0),\n                         prior(lognormal(0,1), nlpar=\"delta2\", lb=0)),\n               control=list(adapt_delta=0.999))\n\nIf you run this code, it produces a lot of severe warnings (which you should not ignore) about divergent transitions after warmup and a very high largest R-hat. Additionally, “Bulk Effective Samples Size” and “Tail Effective Samples Size (ESS)” are too low, and a lot of transitions after warmup that exceeded the maximum treedepth.\n\n\n\n8.5.4 Going beyond the default MCP-Mod models\nSo far, a lot of what we did could also be done in a frequentist manner using the MCP-Mod approach. However, with brms it is easy to make our models more complex when necessary. For example, we assumed that a 280 mg q2w dose is equivalent to a 560 mg q4w dose. This is of course only the case in terms of the total amount of drug injected over the trial period. However, the two dose regimens differ in terms of their blood concentrations over time as shown in the plot below.\n\nsim_1st_order_pk = function(injected=c(1,rep(0,28*5)), dose=1, thalf=28*24, V=10){\n  k = log(2)/thalf  \n  concentration = rep(0, length(injected))\n  for (i in 1:length(injected)){\n    if (i&gt;1) {\n      concentration[i] = concentration[i-1] - k*concentration[i-1] + dose*injected[i]/V\n    } else {\n      concentration[i] = dose*injected[i]/V\n    }\n  }\n  return(concentration)\n}\n\nplot_data = tibble(regimen=\"q4wk\",\n                   dose=280,\n                   concentration = sim_1st_order_pk(rep(c(1,rep(0, 28*24-1)),12), dose=280)) %&gt;%\n  mutate(hour=1:n()) %&gt;%\n  bind_rows(tibble(regimen=\"q2wk\",\n       dose=280,\n       concentration = sim_1st_order_pk(rep(c(1,rep(0, 14*24-1)),24), dose=280)) %&gt;%\n         mutate(hour=1:n())) %&gt;%\n  bind_rows(bind_rows(tibble(regimen=\"q4wk\",\n       dose=560,\n       concentration = sim_1st_order_pk(rep(c(1,rep(0, 28*24-1)),12), dose=560)) %&gt;%\n         mutate(hour=1:n()))) %&gt;%\n  mutate(dose_regimen = paste0(dose, \" mg \", regimen)) \n\nplot_data %&gt;%\n  ggplot(aes(x=hour, y=concentration, col=dose_regimen)) +\n  geom_line() +\n  coord_cartesian(ylim=c(0,120)) +\n  scale_x_continuous(breaks=seq(0,12)*28*24, labels=seq(0,12)) +\n  geom_text(data=. %&gt;% filter( (hour==8025 & str_detect(dose_regimen, \"280 mg q2\")) |\n                               (hour==500 & str_detect(dose_regimen, \"560 mg q4\")) |\n                               (hour==5000 & str_detect(dose_regimen, \"280 mg q4\"))),\n            aes(label=dose_regimen), nudge_y=c(-20,25, 60), angle=c(0,-75,0)) +\n  xlab(\"Time in trial (months)\") +\n  ylab(\"Drug concentration [mg/L]\")\n\n\n\n\n\n\n\n\nThese differences in pharmacokinetic profiles likely lead to at least somewhat different efficacy outcomes. For example, if efficacy is more driven by the peak concentration achieved in each dosing interval, 280 mg q2w might be less effective than a 560 mg q4w regiment would have been. On the other hand, if efficacy is more determined by the minimum concentration maintained throughout time, then 280 mg q2w could be more effective than 560 mg q4w. If the main determinant of efficacy is the average drug concentration over time, and peaks and troughs do not matter (at least within the concentrations seen), then the two dosing regimens might be exactly identical.\nSo, we could update our previous sigmoid Emax model and add an extra term that describes how the 280 mg q2w dose might relate to q4w doses. Note that assuming a monotone concentration response, a 280 mg q2w regimen cannot be worse than a 280 mg q4w regimen - this is also illustrated by the plot showing that the concentrations of 280 mg q2w are (unsurprisingly) always above those of 280 mg q4w. This gives us a lower bound for the efficacy of 280 mg q2w. Similarly, its peak concentrations stay below 2.5 times the peak concentrations of 280 mg q2w, which gives us an upper bound. We will specify a log-normal prior with mean 0 and SD 0.67 for a regimen factor that indicates what q4w dose a q2w dose is equivalent to.\n\nbrmfit3 &lt;- brm( bf( est | se(stderr) ~ E0 + Emax * (dose*dosefactor)^h / ((dose*dosefactor)^h + ED50^h),\n                   nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)), nlf(dosefactor ~ exp((dose==560)*regimen)),\n                   E0 ~ 1, logED50 ~ 1, logh ~ 1, Emax ~ 1, regimen ~ 1, \n                   nl=T, family=gaussian()),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(normal(0,1), nlpar=\"logh\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(normal(4,2), nlpar=\"logED50\"),\n                         prior(normal(0,0.67), nlpar=\"regimen\")),\n               control=list(adapt_delta=0.999),\n               seed=2023,\n               save_pars = save_pars(all = TRUE))\n\nLet’s summarize the model results:\n\nsummary(brmfit3)\n\nWarning: There were 3 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * (dose * dosefactor)^h/((dose * dosefactor)^h + ED50^h) \n         h ~ exp(logh)\n         ED50 ~ exp(logED50)\n         dosefactor ~ exp((dose == 560) * regimen)\n         E0 ~ 1\n         logED50 ~ 1\n         logh ~ 1\n         Emax ~ 1\n         regimen ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept         -0.42      0.10    -0.61    -0.22 1.00     2624     2529\nlogED50_Intercept     2.76      1.34    -0.33     5.09 1.00     1552     1553\nlogh_Intercept        0.00      0.98    -1.81     1.98 1.00     1388     1769\nEmax_Intercept       -1.30      0.33    -2.11    -0.84 1.00     1429     1348\nregimen_Intercept    -0.05      0.68    -1.36     1.25 1.00     2553     2452\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we can see, we can fit such a model, but the marginal posterior for the log-regimen factor is still a N(0, 0.67) distribution. Thus, the data do not inform this parameter very well - which should not surprise us. However, if we had information external to the PATHWAY trial, such as the elicited judgments of experts, we could incorporate it via this type of model.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#conclusion",
    "href": "src/02b_dose_finding.html#conclusion",
    "title": "8  Dose finding",
    "section": "8.6 Conclusion",
    "text": "8.6 Conclusion\nAs we could see brms makes it surprisingly easy to fit non-linear dose response models and to perform model averaging. Taking a Bayesian approach to dose response modeling is attractive for a number of reasons: * using prior information e.g. on expected placebo outcomes, the maximum plausible treatment effect and typical dose response patterns, * (weakly-)informative priors avoid issues with maximum likelihood estimation such as infinitely steep sigmoid Emax models (common when the lowest tested dose has the best point estimate), * the convenience of obtaining predictions about things that are transformations of the model parameters (e.g. the dose with at least a certain effect), and * the straightforward way, in which we can extend the basic models to account for the specifics of our dose finding study.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#excercises",
    "href": "src/02b_dose_finding.html#excercises",
    "title": "8  Dose finding",
    "section": "8.7 Excercises",
    "text": "8.7 Excercises\nThe data below are from a non-clinical study published by Goodner and Horsfall in 1935 (Goodner, K. and Horsfall Jr, F.L., 1935. The protective action of type I antipneumococcus serum in mice: I. The quantitative aspects of the mouse protection test. The Journal of Experimental Medicine, 62(3), pp.359-374.). Mice were given injections of different amounts from a pneumococcus culture, which at the doses given would in the absence of treatment be invariably fatal, mixed together with different doses of type I antipenumococcus horse and rabbit sera. Over several days it was recorded whether each mouse survived or died. This is an example of a dose response curve that appears to have been accepted to be truly non-monotonic in the scientific literature.\n\nmice = tibble(\n  `Source of immune serum` = factor( c(rep(1L, 29*3), rep(2L, 32*3)),\n                                     levels=c(1L, 2L), \n                                     labels=c(\"rabbit\", \"horse\")),\n  `Amount of serum (cc.)` = rep(0.4*0.5^c(0L:5L, 0L:6L, 0L:7L, 0L:7L,\n                                          1L:5L, 0L:8L, 0L:8L, 0L:8L),\n                                each=3),\n  `Amount of culture (cc.)` = c(rep(0.4, 18), rep(0.2, 21), rep(0.1, 24), \n                                rep(0.05, 24), rep(0.4, 15), rep(0.2, 27), \n                                rep(0.1, 27), rep(0.05,27)),\n  Died = c(rep(1L, 18),\n            1L,rep(0L,5), 1L,1L,0L, 1L,0L,0L, 1L,1L,1L, 1L,0L,0L, 1L,1L,1L,\n            rep(0L, 14),1L, 0L,0L,0L, 1L,1L,0L, 1L,1L,1L,\n            rep(0L,24), \n            rep(1L, 15),\n            rep(1L, 8),0L, 1L,0L,0L, 1L,0L,0L, rep(1L,5),0L, 0L,rep(1L,5),\n            rep(1L,5), 0L,1L,rep(0L,9),rep(1L,4),0L, rep(1L,6),\n            rep(1L,5), rep(0L,12),1L, 0L,0L, rep(1L,7))) %&gt;%\n  mutate(Survived = 1L-Died)\n\nDisplayed in a fashion similar to the original article, the data look as in the figure below.\n\n\n\n\n\n\n\n\n\nAn alternative display showing the proportion of mice that survived versus the amount of serum given as displayed in the figure below.\n\n\n\n\n\n\n\n\n\nExcercises in increasing order of complexity:\n\nFit a sigmoid Emax model to the horse serum data of each individual mouse assuming a Bernoulli distribution after choosing reasonable weakly informative priors, but remember that 0 cc. of serum was stated to result in a death rate of near 100%. Perform model checking.\nFit a modified beta model to the horse serum data and also perform Bayesian model averaging. Does this result in a better fit?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html",
    "href": "src/02c_dose_escalation.html",
    "title": "9  Oncology dose escalation",
    "section": "",
    "text": "9.1 Background\nIn Oncology, a principal aim of a Phase-I trial is to study the relationship between dose and toxicity. Larger doses of the study drug are understood to increase the likelihood of severe toxic reactions known as dose-limiting toxicities (DLTs). The key estimand in such a trial is the maximum tolerated dose (MTD) or recommended dose (RD).\nThe class of trial designs that are typically employed are dose escalation designs. In order to protect patient safety, small cohorts of patients are enrolled sequentially, beginning with low dose levels of the study treatment, and monitored for DLTs. Once a dose level is found to be safe, the dose level may be escalated, and the subsequent cohort enrolled at a higher dose.\nWhereas traditional designs such as the 3+3 design are based on algorithmic rules governing the decision for the subsequent cohort based on the outcome at the current dose, Bayesian model-based designs have proven to provide greater flexibility and improved performance for estimating the MTD, while protecting patient safety.\nIn the model-based paradigm for dose escalation, one develops a model for the dose-toxicity relationship. As DLT data accumulates on-study, the model is used to adaptively guide dose escalation decisions. Bayesian approaches are well-suited for this setting, due in part to the limited amount of available data (Phase-I studies are generally small).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#data",
    "href": "src/02c_dose_escalation.html#data",
    "title": "9  Oncology dose escalation",
    "section": "9.2 Data",
    "text": "9.2 Data\nIn the simplest setting of a first-in-human (FIH) study of a new Oncology treatment, the relevant data are the doses, and associated counts of the number of patients evaluated and the number of patients with DLTs.\nFor dose levels indexed by \\(i=1,\\ldots, K\\), we observe:\n\\[ d_i = \\text{dose for the }i^\\text{th}\\text{ dose level} \\] \\[ n_i = \\text{number of evaluable patients treated at }d_i \\] \\[ Y_i = \\text{number of patients with a DLT at }d_i. \\]\nAs an example:\n\ndlt_data &lt;- dplyr::transmute(OncoBayes2::hist_SA, dose = drug_A, num_patients, num_toxicities)\nknitr::kable(dlt_data)\n\n\n\n\ndose\nnum_patients\nnum_toxicities\n\n\n\n\n1.0\n3\n0\n\n\n2.5\n4\n0\n\n\n5.0\n5\n0\n\n\n10.0\n4\n0\n\n\n25.0\n2\n2",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#model-description",
    "href": "src/02c_dose_escalation.html#model-description",
    "title": "9  Oncology dose escalation",
    "section": "9.3 Model description",
    "text": "9.3 Model description\nNeuenschwander, Branson, and Gsponer (2008) proposed the following simple Bayesian logistic regression model (BLRM).\n\\[ Y_i \\, | \\, \\pi(d_i) \\, \\sim \\, \\mathrm{Binom}(n_i, \\pi(d_i))\\, \\, \\text{for }i=1,\\ldots,K \\] \\[ \\pi(d_i) \\, | \\, \\alpha, \\beta = \\mathrm{logit}^{-1}\\Bigl( \\log\\alpha + \\beta\\log \\Bigl(\\frac{d_i}{d^*}\\Bigr) \\Bigr) \\] \\[ \\log\\alpha \\, \\sim \\, N(\\mathrm{logit}(p_0), s_\\alpha^2) \\] \\[ \\log\\beta \\, \\sim \\, N(m_\\beta, s_\\beta^2). \\]\nNote:\n\nA key modeling consideration is monotonicity of the dose-toxicity curve. The use of a lognormal prior for \\(\\beta\\) ensures the curve is strictly increasing \\((\\beta &gt; 0)\\).\nIn the second line, \\(d^*\\) is known as a reference dose, used to scale the doses \\(d_i\\).\nHyperparameters \\(p_0\\), \\(s_\\alpha\\), \\(m_\\beta\\), and \\(s_\\beta\\) may be chosen based on clinical understanding at trial outset. Typically, little information is available prior to the FIH study, so weakly informative priors are preferred.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#implementation",
    "href": "src/02c_dose_escalation.html#implementation",
    "title": "9  Oncology dose escalation",
    "section": "9.4 Implementation",
    "text": "9.4 Implementation\n\nlibrary(dplyr)\nlibrary(brms)\nlibrary(OncoBayes2)\nlibrary(bayesplot)\nlibrary(posterior)\nlibrary(knitr)\nlibrary(BOIN)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8794567)\n\nSuch a model is straightforward to implement in brms. Below, we use a nonlinear formula specification, in order to allow the prior for the intercept to be specified on the log scale.\n\n# Reference dose\ndref &lt;- 50\n\n# Standardize the covariate as in the model description\ndlt_data &lt;- mutate(dlt_data, std_dose = log(dose / dref))\n\n# Model formula in brms\nblrm_model &lt;- bf(num_toxicities | trials(num_patients) ~ logalpha + exp(logbeta) * std_dose, nl = TRUE) +\n  lf(logalpha ~ 1) +\n  lf(logbeta ~ 1)\n\n# Get the list of model parameters requiring prior specifications\nget_prior(blrm_model, data = dlt_data, family = \"binomial\")\n\n  prior class      coef group resp dpar    nlpar lb ub       source\n (flat)     b                           logalpha            default\n (flat)     b Intercept                 logalpha       (vectorized)\n (flat)     b                            logbeta            default\n (flat)     b Intercept                  logbeta       (vectorized)\n\n# Set the prior as described in the previous section\nblrm_prior &lt;- prior(normal(logit(0.33), 2), nlpar = \"logalpha\") +\n  prior(normal(0, 0.7), nlpar = \"logbeta\")\n\n# Compile and fit the model using brms\nblrm_fit &lt;- brm(blrm_model,\n                data = dlt_data,\n                family = \"binomial\",\n                prior = blrm_prior,\n                refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: binomial_logit_lpmf: Probability parameter[1] is -inf, but must be finite! (in '/tmp/Rtmp0749IU/model-a06d6ff209e3.stan', line 42, column 4 to column 50)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#results",
    "href": "src/02c_dose_escalation.html#results",
    "title": "9  Oncology dose escalation",
    "section": "9.5 Results",
    "text": "9.5 Results\n\n9.5.1 Inference for model parameters\nIt is simple to get summary statistics and graphical displays of the posterior distributions for the model parameters.\n\n# Summary statistics for log(alpha) and log(beta)\nposterior_summary(blrm_fit)\n\n                       Estimate Est.Error       Q2.5     Q97.5\nb_logalpha_Intercept  0.6881521 1.2999627 -1.7060120  3.372981\nb_logbeta_Intercept   0.4872278 0.5277053 -0.6747383  1.382841\nlprior               -3.1555746 1.0517939 -5.9296413 -2.194074\nlp__                 -6.3715219 1.0040109 -9.1454040 -5.384548\n\n\n\n# Posterior density estimates for log(alpha) and log(beta)\nbrms::mcmc_plot(blrm_fit, type = \"dens\", facet_args = list(ncol = 1))\n\n\n\n\n\n\n\n\nHowever, these parameters are not themselves the primary target of inference for decision making. Rather the dose-DLT curve \\(\\pi(d)\\) itself is estimated, in order to guide decision making about future dose levels.\n\n\n9.5.2 Inference for DLT rates\nEstimation of \\(\\pi(d)\\) is similarly straightforward. For posterior summary statistics of \\(\\pi(d_i)\\) at the observed dose levels \\(i=1,\\ldots,K\\), one can use the poserior::summarize_draws function as follows:\n\n# Posterior draws for pi(d)\ndlt_data %&gt;%\n  bind_cols(\n    posterior::summarize_draws(\n      posterior_linpred(blrm_fit, transform = TRUE),\n      c(\"mean\", \"sd\", \"quantile2\")\n    ) %&gt;% select(-variable)\n  ) %&gt;%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndose\nnum_patients\nnum_toxicities\nstd_dose\nmean\nsd\nq5\nq95\n\n\n\n\n1.0\n3\n0\n-3.912\n0.010\n0.019\n0.000\n0.045\n\n\n2.5\n4\n0\n-2.996\n0.024\n0.033\n0.000\n0.090\n\n\n5.0\n5\n0\n-2.303\n0.052\n0.053\n0.002\n0.160\n\n\n10.0\n4\n0\n-1.609\n0.124\n0.092\n0.015\n0.306\n\n\n25.0\n2\n2\n-0.693\n0.379\n0.188\n0.102\n0.709\n\n\n\n\n\nNext we illustrate estimation of \\(\\pi(d)\\) on a fine grid of dose levels, in order to visualize the posterior for the continuous curve.\n\ndose_grid &lt;- seq(0, dref, length.out = 500)\n\n# Posterior draws for pi(d)\ndlt_rate_draws &lt;- posterior_linpred(blrm_fit,\n                                    newdata = tibble(dose = dose_grid,\n                                                     std_dose = log(dose / dref),\n                                                     num_patients = 1),\n                                    transform = TRUE)\n\n# Visualization using bayesplot\nbayesplot::ppc_ribbon(y = rep(0, 500), yrep = dlt_rate_draws, x = dose_grid) +\n  guides(fill = \"none\", color = \"none\") +\n  labs(x = \"Dose\", y = \"P(DLT)\")\n\n\n\n\n\n\n\n\n\n\n9.5.3 Predictive inference for future cohorts\nAnother key quantity for understanding risk of toxicity for patients is the predictive distribution for DLTs in a future, unobserved cohort:\n\\[ \\Pr(\\tilde Y = y \\, | \\, y_{obs}) = \\int \\Pr(\\tilde Y = y \\, | \\, \\theta) \\, d\\pi(\\theta|y_{obs}). \\]\nOne can easily obtain such information from brms. Suppose we wished to know the predictive distribution for the DLT count in new cohorts of size 4 at a set of candidate dose levels, \\(d=10, 25, 50\\).\n\ncandidate_doses &lt;- tibble(\n  dose = c(10, 25, 50),\n  std_dose = log(dose / dref),\n  num_patients = 4\n)\n\npredictive_draws &lt;- posterior_predict(blrm_fit, newdata = candidate_doses)\n\ncount_freq &lt;- function(x, breaks = 0:4){\n  x &lt;- factor(x, breaks)\n  setNames(\n    prop.table(table(x)),\n    paste0(\"P(\", breaks, \" DLTs)\")\n  )\n}\n\nkable(bind_cols(\n  candidate_doses,\n  summarize_draws(predictive_draws, count_freq) %&gt;% select(-variable)\n), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndose\nstd_dose\nnum_patients\nP(0 DLTs)\nP(1 DLTs)\nP(2 DLTs)\nP(3 DLTs)\nP(4 DLTs)\n\n\n\n\n10\n-1.609\n4\n0.637\n0.271\n0.075\n0.013\n0.003\n\n\n25\n-0.693\n4\n0.237\n0.299\n0.248\n0.162\n0.054\n\n\n50\n0.000\n4\n0.082\n0.156\n0.220\n0.273\n0.270\n\n\n\n\n\n\n\n9.5.4 Dose escalation decisions\nOne common framework for guiding dosing decisions using a model-based dose escalation framework is known as Escalation With Overdose Control (EWOC). Decisions are based on an MTD threshold above which constitutes excessive toxicity \\(\\pi_{over}\\) (typically \\(\\pi_{over}\\)), and a so-called feasibility bound \\(c\\):\n\\[ \\text{EWOC satisfied at }d \\iff \\Pr( \\pi(d) &gt; \\pi_{over}) \\leq c \\]\nFurthermore, we often define another cutoff \\(\\pi_{targ}\\), and summarize the posterior probabilities for three intervals,\n\\[ \\Pr(d\\text{ is an underdose}) = \\Pr( \\pi(d) &lt; \\pi_{targ} ) \\] \\[ \\Pr(d\\text{ is in the target range}) = \\Pr( \\pi_{targ} \\leq \\pi(d) &lt; \\pi_{over} ) \\] \\[ \\Pr(d\\text{ is an overdose}) = \\Pr( \\pi(d) &gt; \\pi_{over} ),\\]\nand evaluate EWOC by checking if the last quantity exceeds \\(c\\).\nFor the set of candidate doses, the EWOC criteria can be evaluated as follows:\n\npi_target &lt;- 0.16 # motivated by 3+3 design (~ 1/6)\npi_over &lt;- 0.33 # motivated by 3+3 design (~ 1/3)\nc &lt;- 0.25\n\nraw_draws &lt;- posterior_linpred(blrm_fit, newdata = candidate_doses, transform = TRUE)\ndraws &lt;- posterior::as_draws_matrix(raw_draws)\ninterval_probs &lt;- posterior::summarize_draws(\n  draws,\n  list(\n    function(x){\n      prop.table(table(cut(x, breaks = c(0, pi_target, pi_over, 1))))\n    }\n  )\n)\n\nbind_cols(\n  select(candidate_doses, dose),\n  select(mutate(interval_probs, ewoc_ok = `(0.33,1]` &lt;= c), -variable)\n) %&gt;%\n  kable(digits = 3)\n\n\n\n\ndose\n(0,0.16]\n(0.16,0.33]\n(0.33,1]\newoc_ok\n\n\n\n\n10\n0.710\n0.254\n0.036\nTRUE\n\n\n25\n0.129\n0.309\n0.562\nFALSE\n\n\n50\n0.029\n0.108\n0.864\nFALSE",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#conclusion",
    "href": "src/02c_dose_escalation.html#conclusion",
    "title": "9  Oncology dose escalation",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nbrms is easily capable of handling this simple logistic regression model, and producing all the posterior summaries necessary for guiding a dose escalation trial with it. There are, of course, many extensions and complications to this basic methodology that we encounter frequently. The OncoBayes2 package is specifically tailored to this class of regression models, and covers all the typical use-cases. However, the approach described above for brms can be extended as well, for example to handle dose escalation for drug combinations, rather than single agents. See [Advanced Topics section on combination dose escalation][Combination dose escalation].",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#exercise",
    "href": "src/02c_dose_escalation.html#exercise",
    "title": "9  Oncology dose escalation",
    "section": "9.7 Exercise",
    "text": "9.7 Exercise\nAn alternative to designs based on the BLRM is the so-called Bayesian Optimal INterval (BOIN) design (see Yan et al, 2020). In this exercise we will explore how the estimation of the MTD differs between the BOIN approach vs the BLRM vs a flexible monotone-regression approach that can be easily implemented in brms.\nSuppose are planning a trial with the following provisional doses.\n\nprovisional_doses &lt;- tribble(\n  ~dose_id, ~dose, ~prob_dlt,\n         1,     1,      0.05,\n         2,     2,       0.1,\n         3,     4,       0.2,\n         4,     8,       0.3,\n         5,    16,       0.5\n)\n\nTo generate data for comparing the MTD estimation methods, we begin by simulating a trial acording to true DLT rates prob_DLT above, and according to BOIN decision rules for escalation and de-escalation. We starting with dose = 2, and enroll cohorts of size 4 until either 12 cohorts have been enrolled, or more than 12 patients have been enrolled at any one dose. Below is some code that simulates such a trial.\n\nlibrary(BOIN)\n\n# Function to simulate a BOIN trial\nsimulate_BOIN &lt;- function(starting_dose_id,\n                          provisional_doses,\n                          cohort_size,\n                          num_cohorts,\n                          num_patients_stop,\n                          target_dlt_rate = 0.25,\n                          low_dlt_rate = 0.6 * target_dlt_rate,\n                          high_dlt_rate = 1.4 * target_dlt_rate){\n  \n  BOIN_decisions &lt;- BOIN::get.boundary(\n    target = target_dlt_rate,\n    ncohort = num_cohorts,\n    cohortsize = cohort_size,\n    n.earlystop = num_patients_stop,\n    p.saf = low_dlt_rate,\n    p.tox = high_dlt_rate\n  )\n  \n  current_dose_id &lt;- starting_dose_id\n  \n  dlt_data &lt;- provisional_doses %&gt;%\n    mutate(\n      num_patients = 0,\n      num_toxicities = 0,\n      eliminated = FALSE\n    )\n  \n  cohort_history &lt;- dlt_data[0,]\n  \n  cohort_time &lt;- 1\n  while(cohort_time &lt;= num_cohorts){\n    \n    current_dose_data &lt;- dlt_data[current_dose_id,]\n    \n    true_dlt_rate &lt;- dlt_data$prob_dlt[current_dose_id]\n    num_toxicities &lt;- rbinom(1, cohort_size, true_dlt_rate)\n    \n    current_dose_data$num_patients &lt;- current_dose_data$num_patients + cohort_size\n    current_dose_data$num_toxicities &lt;- current_dose_data$num_toxicities + num_toxicities\n    \n    \n    new_data &lt;- mutate(current_dose_data,\n                       num_patients = cohort_size,\n                       num_toxicities = !! num_toxicities)\n    \n    cohort_history &lt;- rbind(\n      cohort_history,\n      new_data\n    )\n    \n    dlt_data[current_dose_id, ] &lt;- current_dose_data\n    \n    if(current_dose_data$num_patients &gt;= num_patients_stop) break\n    \n    boundaries &lt;- setNames(\n      BOIN_decisions$full_boundary_tab[-1, current_dose_data$num_patients],\n      NULL\n    )\n    \n    comparison &lt;- c(\n      escalate = current_dose_data$num_toxicities &lt;= boundaries[1],\n      stay = current_dose_data$num_toxicities &gt; boundaries[1] & current_dose_data$num_toxicities &lt; boundaries[2],\n      de_escalate = current_dose_data$num_toxicities &gt;= boundaries[2],\n      eliminate = current_dose_data$num_toxicities &gt;= boundaries[3]\n    )\n    \n    decision &lt;- names(comparison[max(which(comparison))])\n    \n    if(decision == \"stay\"){\n      next_dose_id &lt;- current_dose_id\n    } else if(decision == \"escalate\"){\n      next_dose_id &lt;- current_dose_id + 1\n    } else{\n      next_dose_id &lt;- current_dose_id - 1\n    }\n    \n    if(decision == \"eliminate\") dlt_data$eliminated[current_dose_id] &lt;- TRUE\n    \n    if(next_dose_id &gt; nrow(dlt_data)){\n      next_dose_id &lt;- nrow(dlt_data)\n    }\n    \n    if(dlt_data$eliminated[next_dose_id]){\n      next_dose_id &lt;- next_dose_id - 1\n    }\n    \n    if(next_dose_id &lt; 1) break\n    \n    current_dose_id &lt;- next_dose_id\n    cohort_time &lt;- cohort_time + 1\n    \n  }\n  \n  mtd &lt;- BOIN::select.mtd(\n    target = target_dlt_rate,\n    npts = dlt_data$num_patients,\n    ntox = dlt_data$num_toxicities,\n    p.tox = high_dlt_rate\n  )\n  \n  return(\n    brms:::nlist(\n      dlt_data,\n      cohort_history,\n      mtd\n    )\n  )\n}\n\n\n# simulate a trial\nwithr::with_seed(\n  -1423048045,\n  {\n    \n    sim &lt;- simulate_BOIN(\n      starting_dose_id = 2,\n      provisional_doses = provisional_doses,\n      cohort_size = 4,\n      num_cohorts = 12,\n      num_patients_stop = 13,\n      target_dlt_rate = 0.25\n    )\n\n  }\n)\n\nThe output of this call include:\n\nsim$dlt_data # simulated cohort-by-cohort data\nsim$mtd # isotonic regression to determine the MTD; p_est shows the result\n\nIn the BOIN design, the MTD is selected based on isotonic regression of beta-binomial posterior quantiles (see Yan et al, 2020).\nNow for the exercise:\n\nUse brms to fit a BLRM similar to these data with reference dose \\(d^* = 16\\) and the same prior as described in this section.\nIn brms, the mo() function is used to specify an ordinal covariate with a monotone relationship with the response. Fit a model in which the log odds of DLT (family = binomial(\"logit\")) is a monotone function of the dose_id variable defined in dlt_data (i.e. the covariate is an integer sequence indexing the provisional doses). How do these estimates compare to the estimates from the BLRM?\nCompare the estimates of DLT rate (e.g. posterior means and quantiles) by dose: for the models in #1 and #2, and as estimated by BOIN in sim$mtd$p_est. Does the BOIN MTD differ from the MTD that would result from taking the highest dose satisfying EWOC from models #1 and #2?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html",
    "href": "src/02cb_tte_dose_escalation.html",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "",
    "text": "10.1 Background\nThe aim of Oncology Phase-I trials is to identify a drug dose which is reasonable safe to use and efficacious at the same time. As cytotoxic drugs aim to kill cancer cells, it is expected that toxicity events are to some extent a surrogate for efficacy of the drug. Due to the life-threatening nature of the disease one thereby uses dose escalation trial designs to identify the maximum tolerated dose (MTD) or recommended dose (RD) rapidly. These trials enroll small patient cohorts at some drug dose and then assess at a dose escalation meeting (DEM) all safety events occurring over the time-course of a treatment cycle (often 4 weeks). The safety events classified as dose limiting toxicities (DLTs) then determine which dose levels are safe for use in the next patient cohort. In this way an increasing data set on the toxicity of the drug emerges as the trial continues.\nThere are many approaches on how to guide dose escalation trials at a DEM. Traditional designs such as the 3+3 design are based on algorithmic rules governing the decision for the subsequent cohort based on the outcome at the current dose. Specifically, 3 patients are enrolled onto a cohort and the drug dose is increased in case no DLT occurs, if 1 DLT occurs the drug dose is kept constant while it is decreased if more DLTs occur. Whenever two cohorts in sequence observe 1 out of 3, the 3+3 trial completes. This is in line with the goal of a 33% DLT rate at the MTD. It has been shown that these rule-based approaches have rather poor statistical properties and model based approaches have been proposed. In particular, Bayesian model-based designs have proven to provide greater flexibility and improved performance for estimating the MTD, while protecting patient safety. In the model-based paradigm for dose escalation, one develops a model for the dose-toxicity relationship. As DLT data accumulates on-study, the model is used to adaptively guide dose escalation decisions. Bayesian approaches are well-suited for this setting, due in part to the limited amount of available data (Phase-I studies are generally small).\nRecent, more targeted treatments, have challenged several aspects taken for granted for traditional cytotoxic therapies, such as\nThese challenges can be addressed by switching to time-to-first-event (time-to-DLT) modeling with piecewise-constant hazards, where the constant period is chosen to align with a period of interest. For instance, if the dose-toxicity relationship over 3 cycles is of interest, one might choose to model piecewise-constant hazards per cycle, or if we are interested in the dose-toxicity relationship of several regimens that contain within-patient ramp-up of dosing on a weekly basis, we might choose piecewise-constant hazards for each week.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#background",
    "href": "src/02cb_tte_dose_escalation.html#background",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "",
    "text": "Estimating the MTD over just one treatment cycle: since targeted therapies are typically safer than cytotoxic ones and are applied over longer time periods, estimating longer-term (multi-cycle) dose-toxicity relationships has become more important. However, we typically do not want to wait for the full time period (e.g., 3 cycles) to decide the dose for the next cohort. That is, after the first cycle, one may want to extrapolate to the dose-toxicity relationship over 3 cycles given a prior on how the conditional toxicity of the next cycles changes with respect to the first one. However, this is not easily feasible with methods that only use binary (DLT yes/no) data.\nConstant dosing regimen: For some targeted treatments it may be advantageous to vary the dosing regimen, for instance, by performing within-patient dose-ramp-up over several weeks to avoid cytokine release syndrome. However, classical binary cycle-1 methods such as the 3+3, BOIN or the Bayesian Logistic Regression Model (BLRM) are not able to directly incorporate time-varying dosing or hazard reductions due to ramp-up dosing.\nIgnoring dropout: In classical binary dose-DLT methods, where DLT yes/no was measured for each patient over the first treatment cycle, data from patients that dropped out during this cycle (for instance due to disease progression) is typically ignored. This is no longer an option with modern cancer treatments in Phase I that are both taken over longer time periods where dropout is more likely, as well as administered in last-line treatment populations with aggressive cancers / rapid dropout due to disease progression.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#required-libraries",
    "href": "src/02cb_tte_dose_escalation.html#required-libraries",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.2 Required libraries",
    "text": "10.2 Required libraries\nTo run the R code of this section please ensure to load these libraries and options first:\n\nlibrary(lubridate)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(posterior)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(here)\nlibrary(assertthat)\nlibrary(gt)\ntheme_set(theme_bw(12))\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8794567)\n# Table formatting helper function:\ngt_format &lt;- function(x) {\n  x |&gt; fmt_number(decimals=2) |&gt;\n    opt_interactive(\n      page_size_default = 5, \n      use_text_wrapping = F, \n      use_compact_mode = T\n    )\n}",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#example-trial",
    "href": "src/02cb_tte_dose_escalation.html#example-trial",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.3 Example trial",
    "text": "10.3 Example trial\nIn the following, we will assume a simple setting of a first-in-human (FIH) study of a new Oncology treatment. Here, we consider the dose-DLT relationship over a time horizon of 3 treatment cycles, a cycle length of 4 weeks, and daily (QD) dosing of a drug A at a pre-defined dose set with reference dose \\(\\tilde{d} = 50\\) (at the reference dose we anticipate the MTD a priori):\n\ncycle_lengths &lt;- rep(weeks(4), 3)\ndref_A &lt;- 50\ndoses &lt;- c(1, 2.5, 5, 10, 20, 30, 40, 45, 50)\n\ncycle_seq &lt;- seq_along(cycle_lengths)\ndose_seq  &lt;- seq_along(doses)\n\nWe will use the logarithm of the normalized dose (with respect to a reference dose \\(\\tilde{d}\\) ), \\(\\log\\left(\\tilde{d}_{j}\\right) =\n\\log\\left(d_{j} / \\tilde{d}\\right)\\), where \\(j\\) is the cycle index. We define a helper function to add this column as std_A, and whether the log-dose is finite as finite_A, which will become relevant in the case of drug combinations.\n\nadd_std_dose_col &lt;- function(dose_info, drug = \"A\", dref) {\n  dose_info |&gt; mutate(\n    \"std_{drug}\"    := log(get(paste0(\"dose_\", drug)) / dref), \n    \"finite_{drug}\" := 1L * (get(paste0(\"dose_\", drug)) != 0)\n  )\n}",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#data",
    "href": "src/02cb_tte_dose_escalation.html#data",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.4 Data",
    "text": "10.4 Data\nWe now need individual patient data (IPD) containing:\n\nThe doses planned to be administered in each of the three cycles,\nThe start and end dates of each cycle\nThe date of censoring (either at the end of the last cycle due to end of the observation period, or during the trial due to dropout)\nThe date of DLT\n\nAs an example, we create 3-cycle data in long format reflecting the data used in the BLRM dose escalation case study:\n\n# Create a single patient\nexample_ipd &lt;- tibble(\n  cycle_index = cycle_seq,\n  dose_A = 1,\n  num_toxicities = 0,\n  follow_up = as.numeric(cycle_lengths, \"days\")\n) \n\n# Create long-format individual-patient data (IPD) corresponding to the one \n# in the BLRM dose escalation case study\nipd &lt;- bind_rows(\n  example_ipd |&gt; slice(rep(row_number(), 3)),\n  example_ipd |&gt; mutate(dose_A = 2.5) |&gt; slice(rep(row_number(), 4)),\n  example_ipd |&gt; mutate(dose_A = 5)   |&gt; slice(rep(row_number(), 5)),\n  example_ipd |&gt; mutate(dose_A = 10)  |&gt; slice(rep(row_number(), 4)),\n  example_ipd |&gt; mutate(dose_A = 25, num_toxicities = 1) |&gt; slice(rep(1, 2))\n) |&gt; add_std_dose_col(drug = \"A\", dref = dref_A)\n\nipd |&gt; gt() |&gt; gt_format()\n\n\n\n\n\n\n\n\nWe define dosing schedules that we want to compute DLT probabilities for. Here, we choose schedules that are constant over all three cycles:\n\n# Dosing information per cycle\ndose_info_long &lt;- expand_grid(schedule_id = dose_seq, cycle = cycle_seq) |&gt;\n    mutate(dose_A = doses[schedule_id])\n\n# Cycle information\ncycle_info &lt;- tibble(\n  cycle = cycle_seq, \n  follow_up = as.numeric(cycle_lengths, \"days\")\n)\n\nWe also generate a reference dosing data set which is formatted like an analysis data set, but is only setup to evaluate the final model on the predefined set of doses. These will be needed to compute the conditional (or cumulative) DLT probabilities in (up to) cycles 1, 2 and 3:\n\ndose_ref_data &lt;- dose_info_long |&gt;\n    left_join(cycle_info, by=\"cycle\") |&gt;\n    mutate(num_toxicities=0L) |&gt;\n    add_std_dose_col(drug = \"A\", dref = dref_A)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#model-description",
    "href": "src/02cb_tte_dose_escalation.html#model-description",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.5 Model description",
    "text": "10.5 Model description\nFor each cycle \\(j\\), we model the probability of observing a DLT event conditional on entering cycle \\(j\\) as a function of the dose \\(d_{j}\\) being administered during this cycle.\nFor each patient \\(i \\in {1, \\ldots, N}\\), we observe either the DLT event time \\(T_i\\) or the censoring time \\(C_i\\), whichever occurs first. We denote this as \n\\[\n(U_i, \\delta_i), \\textrm{ where } U_i = \\min(T_i, C_i) \\textrm{ and } \\delta_i = 1(T_i \\leq C_i).\n\\]\nHere, \\(U_i\\) represents the observed time, and \\(\\delta_i\\) indicates whether a DLT occurred (\\(\\delta_i = 1\\)) or not (\\(\\delta_i = 0\\)).\nThe primary focus of the model is the risk for DLT events over the course of a sequence of cycles. Thus, we do not aim to model accurately the risk for an event within each cycle. While one could model the event and censoring times as interval censored observations, we disregard here the interval censoring and use a continuous time representation of the time to event process. The continuous time representation can be understood as an approximation of the interval censored process with time lapsing in full units of a cycle. As a consequence, the observed event time \\(T_i\\) and censoring times \\(C_i\\) are always recorded at the planned cycle completion time-point (time lapses in units of cycles). Here we assume for simplicity that each cycle \\(j\\) has the same duration \\(\\Delta t\\) such that any time being recorded is a multiple of \\(\\Delta t\\) and we denote time \\(t\\) being during a cycle \\(j\\) with \\(t \\in ((j-1) \\, \\Delta t, j \\, \\Delta t]\\) or equivalently \\(t \\in I_j\\) for brevity.\n\n10.5.1 Model definition\nTo define our model, we use the following basic definitions from survival analysis. The survival function \\(S(t)\\) and cumulative event probability \\(F(t)\\) are defined as\n\\[\\begin{align*}\nS(t) := & \\Pr(\\textrm{\"No event up to time t\"}) = \\Pr(T \\geq t) = 1 - F(t).\n\\end{align*}\\]\nWe now have two quantities that are of particular interest:\n\nThe cumulative DLT probability up to cycle \\(j\\) given a prescribed dosing \\(\\boldsymbol{d} = (d_{1}, d_{2}, d_{3})\\) over the three cycles: \\[\\Pr(\\textrm{\"DLT up to cycle j\"}|\\boldsymbol{d}) =\nF(t).\\]\nThe conditional DLT probability of cycle \\(j\\) given survival up to cycle \\(j-1\\) (i.e., not having observed a DLT event up to the end of cycle \\(j-1\\)): \\[\\begin{align*}\n   \\, &  \\Pr(\\textrm{\"DLT in cycle j\"}|\\textrm{\"No DLT up to cycle j-1\"}, d_j) \\\\\n= \\, &  \\Pr(T \\leq t + \\Delta t|T &gt; t, d_j)\\\\\n= \\, &  1 - \\exp\\left(-\\int_{t}^{t + \\Delta t} h(u) du \\right),\n\\end{align*}\\] where \\(h(u)\\) is the hazard function and \\(H(t) = \\int_{0}^{t} h(u) du\\) the cumulative hazard.\n\nGiven the drug dose \\(d_{j}\\) administered in cycle \\(j\\), we model the log-hazard \\(\\log(h_j(t|\\tilde{d}_{j}))\\) as \\[\n\\log(h_j(t|\\tilde{d}_{j})) = \\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right), \\textrm{ where } \\tilde{d}_{j} = \\frac{d_{j}}{\\tilde{d}}\n\\] and \\(\\tilde{d}\\) is the reference dose. Then (see Section 10.10 for the full derivation), the conditional DLT probability is closely related to the classic two-parameter Bayesian Logistic Regression Model (BLRM), since\n\\[\\begin{align}\n\\mathrm{cloglog}\\Pr(T \\leq t + \\Delta t|T &gt; t, d_j) & = \\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right) + \\log(\\Delta t),\n\\end{align}\\]\nis the same as for the BLRM in cycle one if we elapse time in cycles (then \\(\\Delta t = 1\\) and the \\(\\log(\\Delta t)\\) is zero), except for the link function changing from \\(\\mathrm{logit}\\) to \\(\\mathrm{cloglog}(x) = \\log(-\\log(1 - x))\\).\nThe cumulative DLT probability can also be derived, where \\(j_t\\) is the index of the cycle that time \\(t\\) lies in:\n\\[\\begin{align}\n\\mathrm{cloglog}\\Pr(T \\leq t|\\boldsymbol{d}) & = \\log\\left(\\sum_{j = 1}^{j_t} \\exp\\left(\\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right) + \\log(\\Delta t)\\right)\\right) = \\mathrm{cloglog} F(t|\\boldsymbol{d}).\n\\end{align}\\]\nLast, but not least, the likelihood for all patients \\(i\\) is then \\[\nL(U|\\alpha, \\beta) = \\prod_i \\underbrace{ f(T_i)^{\\delta_i} }_\\textrm{DLT\nevents} \\, \\underbrace{S(C_i)^{1 - \\delta_i}}_\\textrm{Censoring event} \\,.\n\\] In order to fit these models to time to first event data it is helpful to consider its representation as a Poisson regression. Since \\(f(t) =\nh(t) \\, S(t)\\) and \\(S(t) = \\exp\\left(-H(t)\\right)\\) from basic survival analysis, it follows that the likelihood contribution in absence of an event is \\(\\exp(-H(t))\\) while it is \\(h(t) \\,\n\\exp(-H(t))\\) whenever an event occurs.\nThe Poisson distribution for observing \\(k\\) events,\n\\[ \\Pr(k|\\lambda) = \\frac{\\lambda^k \\, \\exp(-\\lambda)}{k!}, \\]\nbecomes \\(\\exp(-\\lambda)\\) for no event (\\(k=0\\)) and \\(\\lambda \\,\n\\exp(-\\lambda)\\) for an event (\\(k=1\\)). Therefore, we can model the data using a Poisson regression approach whenever the hazard is constant over the time units we model, \\(h(t) = \\lambda\\). In this case the counts on each time interval are \\(0\\) or \\(1\\). Moreover, we use a \\(\\log\\) link function for the counting rate \\(\\lambda\\) and model with a linear function the \\(\\log\\) counting rate to ensure its positivity. Importantly, we also add to the linear predictor an offset equal to the \\(\\log\\) of the exposure time \\(\\log(\\Delta t)\\). The offset multiplies \\(\\lambda\\) with \\(\\Delta t\\) and thereby turns the constant counting rate \\(\\lambda\\) into a cumulative hazard \\(H(t) = \\lambda \\,\n\\Delta t\\). Thereby we obtain the desired likelihood.\nNote, that when using the Poisson approach in brms we pass the cumulative hazard \\(\\lambda \\, \\Delta t\\) as argument to the counting rate such that the likelihood term for an event becomes \\(\\lambda \\,\n\\Delta t \\, \\exp(-\\lambda \\, \\Delta t)\\), which is not quite equal to \\(f(t)\\) as required above, but is instead equal to \\(\\Delta t \\,\nf(t)\\). However, as \\(\\Delta t\\) is known it is merely a constant and is hence irrelevant for sampling the posterior.\n\n\n10.5.2 Model prior\nFor the above model, we must define a prior for the parameters \\(\\alpha\\) and \\(\\beta\\). The interpretation of these parameters depends on two reference values, the reference dose \\(\\tilde{d}\\) and reference time point \\(\\tilde{t}\\).\nThe reference dose \\(\\tilde{d}\\), with respect to which the dose is normalized, defines the meaning of the intercept parameter \\(\\alpha\\). Here we choose to define the meaning of the intercept by considering the reference time point \\(\\tilde{t}\\) such that for \\(d=\\tilde{d}\\) and \\(t=\\tilde{t}\\) we have that\n\\[\n\\mathrm{cloglog}\\Pr(T \\leq \\tilde{t}|\\tilde{d}) = \\alpha + \\log(\\tilde{t}).\n\\]\nHence, when setting the mean of \\(\\alpha\\) to \\(\\mathrm{cloglog}^{-1}(\\tilde{\\pi}) - \\log(\\tilde{t})\\) it becomes apparent that the intercept is equal to the rate of events \\(\\tilde{\\pi}\\) over the reference time period up to the reference time point \\(\\tilde{t}\\) whenever dose \\(\\tilde{d}\\) is given.\nIf we are interested in modeling treatment cycles, we might let the time unit elapse in full cycles and set the reference time point to the end of cycle 1 (\\(\\tilde{t} = 1\\)). Then, the interpretation of the prior is the same as for the classic BLRM for which the DLT probability over one cycle is modeled as a binomial logistic regression (\\(\\mathrm{logit}(\\pi(d)) = \\alpha + \\beta \\, \\log(d/\\tilde{d})\\)). The only difference is that the link function is now \\(\\mathrm{cloglog}\\) instead of \\(\\mathrm{logit}\\).\nGiven the similarity of the Poisson TTE model to the one cycle focused BLRM we will use the same rational for the slope parameter \\(\\beta\\). The slope parameter \\(\\beta\\) can be interpreted as the Hill coefficient of a sigmoid logistic curve. We can hence use mechanistic knowledge and prior in vivo and in vitro data to motivate its feasible range of values, and condense this prior knowledge into an appropriate prior for the slope. In an efficacy meta-analysis, “modeling showed the Hill parameters were concentrated near 1.0”, see\n\nThomas et al. (2014) doi:10.1080/19466315.2014.924876,\nBayesian Methods in Pharmaceutical Research (2020) doi:10.1201/9781315180212 and\nThomas et al. (2017) doi:10.1080/19466315.2016.1256229 for details.\n\nTherefore, in the absence of more specific knowledge (e.g. pre-clinical data, historical clinical data for the mechanism of action, etc.) we assume a prior for \\(\\log\\left(\\beta\\right)\\) with mean 0. Furthermore, we wish to avoid extremely steep or flat dose-response curves. Thereby, we parameterize the 95% central credible interval to allow for a 4 fold increase or decrease via setting\n\\[ \\log\\left(\\beta\\right) \\sim \\mbox{Normal}\\left(0,\n\\left(\\frac{\\log(4)}{1.96}\\right)^2\\right).\\]\nAs discussed above, the intercept \\(\\alpha\\) defines the reference event rate \\(\\tilde{\\pi}\\) at the reference dose \\(\\tilde{t}\\) over the time span up to the reference time \\(\\tilde{t}\\). By setting \\(\\tilde{\\pi}\\) to 20% we imply that the reference dose \\(\\tilde{d}\\) corresponds to our prior guess of the MTD. These considerations define the prior mean for \\(\\alpha\\). The standard deviation for the intercept is set to unity. This choice results from an extensive study of the model properties under various data scenarios. These data scenarios studied how the model guides a dose-escalation trial at an early stage. At an early stage of the trial one requires sufficient conservative dose recommendations from the safety model while at the same time one wishes to ensure that the model allows the trial to continue enrolling patients to low doses should a limited number of DLT events be observed during the early cohorts. The rationale behind the prior of\n\\[ \\alpha \\sim \\mbox{Normal}\\left(\\mathrm{cloglog}(\\tilde{\\pi} = 20\\%) - \\log\\left(\\tilde{t}\\right), 1^2\\right) \\]\nis that we have some level of confidence in the set of doses which we study in the trial to be reasonable. That is, the fact that we choose to study a concrete dose range is by itself prior knowledge driven by a multitude of (pre-clinical & clinical) considerations. This prior choice deliberately avoids that a limited number of DLT events at the start of the trial can entirely undermine our initial understanding of the drug safety profile. Only with sufficient data the model will stop a trial.\nFor a better intuition of the model prior, it is a helpful to study data scenarios (left out here) and to visualize the joint model prior rather than considering the prior on each parameter individually. This is covered in section Section 10.6.3.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#implementation",
    "href": "src/02cb_tte_dose_escalation.html#implementation",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.6 Implementation",
    "text": "10.6 Implementation\nFor the case of a single agent trial we can cast the problem into a Poisson regression framework as detailed above and define the brms model formula as follows. We use here a non-linear model formla syntax to enforce a positive slope:\n\ntte_model &lt;-\n    bf(num_toxicities | rate(follow_up) ~ interA + exp(slopeA) * std_A,\n       interA ~ 1,\n       slopeA ~ 1,\n       nl=TRUE, family=poisson()\n  )\n\nbrms helps us to get the list of model parameters requiring prior specifications:\n\nget_prior(tte_model, data = ipd)\n\n  prior class      coef group resp dpar  nlpar lb ub       source\n (flat)     b                           interA            default\n (flat)     b Intercept                 interA       (vectorized)\n (flat)     b                           slopeA            default\n (flat)     b Intercept                 slopeA       (vectorized)\n\n\nWe can for instance define a prior with 20% DLT probability after the first treatment cycle at the reference dose. For more details on the \\(\\beta\\) parameter prior, see the bite-size guidance on go/BLRM.\n\n## -4.83 approx cloglog(0.2) - log(7*4) for drug A\ntte_prior &lt;-\n  prior(normal(-4.83, 1), nlpar=interA, class=b, coef=Intercept) +\n  prior(normal(0, log(4)/1.96), nlpar=slopeA)\n\nWe then create the Stan model code\n\ntte_stanmodel &lt;- tte_model |&gt; make_stancode(\n  data = ipd, prior = tte_prior\n)\n\nand data:\n\ntte_standata &lt;- tte_model |&gt; make_standata(\n  data = ipd, prior = tte_prior\n)\n\nAfter inspecting the generated code and data for correctness, we compile the model without sampling it (chains = 0) yet:\n\ntte_brms_model &lt;- tte_model |&gt; brm(\n  data = ipd, prior = tte_prior, chains = 0, silent = 2\n)\n\n\n10.6.1 Computing conditional and cumulative DLT probabilities for a given dosing schedule\nWe define a function to compute the conditional and cumulative DLT probabilities over a given schedule, respectively. This function needs to take the hazards for each piecewise-constant interval and compute the respective DLT probability. We write the function in the same logic as the add_*_rvars functions from tidybayes such that we can easily complement analysis data sets with the probabilities of interest:\n\nadd_risk_rvars &lt;- function(newdata, model, time, .by) {\n    ## P(T =&lt; t) = 1 - P(T &gt; t) = inv_cloglog(log(H(t)))\n    ## &lt;=&gt; P(T =&lt; t) = 1 - P(T &gt; t) = inv_clog(H(t))\n    ## inv_cloglog(cll) = 1 - exp(-exp(cll))\n    ## =&gt; inv_clog = 1 - exp(-cl)\n    inv_clog &lt;- function(cl) { 1 - exp(-cl) }\n\n    ## data must be sorted by time to ensure that the cumulative sum\n    ## works correctly below\n    time_order &lt;- order(pull(newdata, {{.by}}), pull(newdata, {{time}}))\n    orig_order &lt;- seq_len(nrow(newdata))[time_order]\n    riskdata &lt;- newdata[time_order,]\n    H &lt;- rvar(posterior_epred(model,\n                              newdata=riskdata,\n                              allow_new_levels=TRUE,\n                              sample_new_levels=\"gaussian\"))\n    riskdata &lt;- mutate(riskdata,\n                       prob=inv_clog(cumsum(H[cur_group_rows()])),\n                       cprob=inv_clog(H[cur_group_rows()]),\n                       .by={{.by}})\n    riskdata[orig_order,]\n}\n\n\n\n10.6.2 Dose escalation decisions\nAs in the basic dose-escalation chapter, we will apply Escalation With Overdose Control (EWOC). I.e., we define a threshold above which doses are “excessively toxic” \\(\\pi_{over}\\) (typically \\(\\pi_{over} = 33\\%\\)), and a so-called feasibility bound \\(c\\) (typically \\(c = 25\\%\\)):\n\\[ \\text{EWOC satisfied at }d \\iff \\Pr( \\pi(d) &gt; \\pi_{over}) \\leq c \\] Furthermore, we often define another cutoff \\(\\pi_{targ}\\), and summarize the posterior probabilities for three intervals,\n\\[\\begin{align*} \\Pr(d\\text{ is an underdose}) &= \\Pr( \\pi(d) &lt; \\pi_{targ} ) \\\\\n\\Pr(d\\text{ is in the target range}) &= \\Pr( \\pi_{targ} \\leq \\pi(d) &lt; \\pi_{over} ) \\\\\n\\Pr(d\\text{ is an overdose}) &= \\Pr( \\pi(d) &gt; \\pi_{over} ),\n\\end{align*}\\]\nand evaluate EWOC by checking if the last quantity exceeds \\(c\\).\nFor the set of MCMC draws, we can compute median, mean, sd, 25% and 75% quantiles, the interval probability as well as the EWOC criteria using the summarize_draws function from the posterior package. Especially for the EWOC metric, we also would like to know if there are any pre-specified doses where the Monte Carlo Error could flip the decision whether a dose is considered safe enough (or not).\nWe test for this using a Z-test (one-sample location test) for the 75% quantile and its distance from the critical threshold \\(\\pi_{over}\\) using the Monte-Carlo Standard Error (MCSE) for the 75% quantile (mcse_q75). We define the EWOC metric to be sufficiently accurate if the probability of flipping the decision due insufficient accuracy (and hence large MCSE) is smaller than 2.5% certainty on whatever side of the decision threshold. In turn we are at least 97.5% certain that the decision is robust wrt to fluctuations implied by MCMC sampling variability. Here we assume that the statistic is normally distributed:\n\nsummarize_probs &lt;- function(draws) {\n  summarize_draws(\n    draws,\n    median, mean, sd, \n    ~quantile2(.x, probs = c(0.25, 0.75)), \n    ~prop.table(table(cut(.x, breaks = c(0, 0.16, 0.33, 1)))),\n    ~mcse_quantile(.x, probs = 0.75), \n    default_convergence_measures()\n  ) |&gt; mutate(\n    ewoc_ok = q75 &lt; 0.33,\n    stat = (q75 - .33) / mcse_q75,\n    ewoc_accurate =  abs(stat) &gt;= qnorm(0.975)\n  ) |&gt; select(-variable)\n}\n\nTo inspect the prior, we first sample the model ignoring the data:\n\ntte_model_prioronly &lt;- update(tte_brms_model, chains = 4, sample_prior = \"only\") \n\nNow we can compute the conditional and cumulative DLT probabilities implied by the prior,\n\nprior_risk &lt;- dose_ref_data |&gt;\n  add_risk_rvars(tte_model_prioronly, cycle, schedule_id)\n\nand display them as a table, i.e., the conditional DLT probabilities\n\nprior_risk |&gt;\n  mutate(summarize_probs(cprob)) |&gt;\n  select(-num_toxicities, -std_A, -prob, -cprob) |&gt;\n  gt() |&gt; gt_format()\n\n\n\n\n\n\n\n\nas well the cumulative probabilities\n\nprior_risk |&gt;\n  mutate(summarize_probs(prob)) |&gt;\n  select(-num_toxicities, -std_A, -prob, -cprob) |&gt;\n  gt() |&gt; gt_format()\n\n\n\n\n\n\n\n\n\n\n10.6.3 Visualizing the model prior\nWe can also visualize the prior in terms of the sampled curves, and see whether this distribution makes sense to us. We can, e.g., see, that the prior we defined above is quite sensible in terms of the dose-response curves it implies:\n\n10.6.3.1 Conditional DLT probability in cycle 1\n\n\nShow the code\nadd_plot_cols &lt;- function(data) {\n  data |&gt; mutate(\n    EWOC = factor(ewoc_ok, c(TRUE, FALSE), c(\"OK\", \"Not OK\")), \n    dose_A = factor(dose_A),\n    Cycle = cycle\n  )\n}\n\ndoses_dense &lt;- seq(0, 2*dref_A, length.out = 100 + 1)\ndose_info_dense &lt;- expand_grid(\n  schedule_id = seq_along(doses_dense), \n  cycle = cycle_seq\n  ) |&gt;\n  mutate(dose_A = doses_dense[schedule_id])\n    \ndose_ref_data_dense &lt;- dose_info_dense |&gt;\n  left_join(cycle_info, by=\"cycle\") |&gt;\n  mutate(num_toxicities=0L) |&gt;\n  add_std_dose_col(drug = \"A\", dref = dref_A)    \n  \nplot_cycle1_dist &lt;- function(model, schedules, doses) {\n  plot_data &lt;- schedules |&gt; \n    add_risk_rvars(model, cycle, schedule_id) |&gt;\n    mutate(summarize_probs(cprob)) |&gt;\n    filter(cycle == 1)\n    \n  plot_data_long &lt;- plot_data |&gt;\n    select(dose_A, cprob) |&gt;\n    unnest_rvars()\n  \n  plot_data_long |&gt;\n    ggplot(aes(dose_A, y = cprob, group = .draw)) +\n    geom_line( alpha=0.01) +\n    scale_x_continuous(breaks = doses) +\n    scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n    hline_at(0.33, linetype = I(2)) +\n    ggtitle(\n      \"Risk of a DLT in cycle 1\"\n    ) +\n    xlab(\"Dose [mg]\") +\n    ylab(\"P(DLT in cycle | survival to cycle start)\") +\n    theme(axis.text.x = element_text(angle = 90))\n}\n\nplot_cycle1_dist(tte_model_prioronly, dose_ref_data_dense, c(doses, 2*dref_A))\n\n\n\n\n\n\n\n\n\nIf one chooses a very wide prior on a parameter without considering the response scale, this can, result in priors that are not biologically justifiable. For instance, if we chose a much higher standard deviation for the prior of the slope \\(\\beta\\), this leads to a prior is bimodal in the sense that it implies either flat dose-response curves or step functions at the reference dose (which is easier to overlook when only plotting the point intervals):\n\n\nShow the code\ntte_prior_wide &lt;-\n  prior(normal(-4.83, 1), nlpar=interA, class=b, coef=Intercept) +\n  prior(normal(0, 3), nlpar=slopeA)\n\ntte_brms_model_wide_prioronly &lt;- tte_model |&gt; brm(\n  data = ipd, prior = tte_prior_wide, silent = 2, sample_prior = \"only\"\n)\n\nplot_cycle1_dist(tte_brms_model_wide_prioronly, dose_ref_data_dense, c(doses, 2*dref_A))\n\n\n\n\n\n\n\n\n\nNevertheless, it is easier to visualize the distributiom of the estimated dose-response curves for the pre-specified doses using the stat_pointinterval stat from the ggdist package:\n\n\n10.6.3.2 Conditional DLT probability in each of the 3 cycles\n\nplot_cprob &lt;- function(model, schedules) {\n  schedules |&gt; \n    add_risk_rvars(model, cycle, schedule_id) |&gt;\n    mutate(summarize_probs(cprob)) |&gt;\n    add_plot_cols() |&gt;\n    ggplot(aes(dose_A, ydist = cprob, colour = EWOC)) +\n    facet_wrap(~Cycle, labeller = label_both) + \n    stat_pointinterval(.width = 0.5) +\n    scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n    coord_cartesian(ylim = c(0, 0.5)) +\n    hline_at(0.33, linetype = I(2)) +\n    ggtitle(\n      \"Conditional risk for one DLT per cycle\", \n      \"Risk is conditional on survival at the same dose up to cycle start.\\nShown is the median (dot) and central 50% CrI (line).\"\n    ) +\n    xlab(\"Dose [mg]\") +\n    ylab(\"P(DLT in cycle | survival to cycle start)\")\n}\n\nplot_cprob(tte_model_prioronly, dose_ref_data)\n\n\n\n\n\n\n\n\n\n\n10.6.3.3 Cumulative DLT probability over 3 cycles\n\nplot_prob &lt;- function(model, schedules) {\n  schedules |&gt;\n    add_risk_rvars(model, cycle, schedule_id) |&gt;\n    mutate(summarize_probs(prob)) |&gt;\n    add_plot_cols() |&gt;\n    ggplot(aes(dose_A, ydist = prob, colour = EWOC)) +\n    stat_pointinterval(.width = 0.5) +\n    facet_wrap(~Cycle, labeller = label_both) +\n    scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n    coord_cartesian(ylim = c(0, 0.5)) +\n    hline_at(0.33, linetype = I(2)) +\n    ggtitle(\n      \"Overall risk for DLT up to cycle\", \n      \"Given same dose over all cycles.\\nShown is the median (dot) and central 50% CrI (line).\"\n    ) +\n    xlab(\"Dose [mg]\") +\n    ylab(\"P(DLT up to cycle)\")\n}\n\nplot_prob(tte_model_prioronly, dose_ref_data)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#examining-the-model-posterior",
    "href": "src/02cb_tte_dose_escalation.html#examining-the-model-posterior",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.7 Examining the model posterior",
    "text": "10.7 Examining the model posterior\nWe now sample the model including the data:\n\ntte_model_posterior &lt;- update(tte_brms_model, chains = 4)\n\n\n10.7.1 Inference for model parameters\nIt is simple to get summary statistics and graphical displays of the posterior distributions for the model parameters:\n\nposterior_summary(tte_model_posterior)\n\n                     Estimate Est.Error       Q2.5     Q97.5\nb_interA_Intercept -4.2091746 0.8782811  -5.932148 -2.423622\nb_slopeA_Intercept  0.3480113 0.4682895  -0.662769  1.159865\nlprior             -2.4100443 1.0109607  -5.297765 -1.513230\nlp__               -7.7164309 1.0531914 -10.655005 -6.674749\n\n\n\nbrms::mcmc_plot(tte_model_posterior, type = \"dens\", facet_args = list(ncol = 1))\n\n\n\n\n\n\n\n\n\n\n10.7.2 Inference for conditional and cumulative DLT probabilities\nThe conditional and cumulative DLT probabilities are now higher, as expected from the data:\n\nplot_cprob(tte_model_posterior, dose_ref_data) \n\n\n\n\n\n\n\n\n\nplot_prob(tte_model_posterior, dose_ref_data)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#conclusion",
    "href": "src/02cb_tte_dose_escalation.html#conclusion",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.8 Conclusion",
    "text": "10.8 Conclusion\nbrms can handle time-to-event modelling for DLTs in an early Oncology dose escalation trial using Poisson regression with an offset for the follow-up, and produce all the posterior summaries necessary for guiding such trials. There are, of course, many possible extensions and complications to this methodology, such as hazards varying by treatment cycle (e.g., monotonically decreasing), or by prior treatment (e.g., decreasing hazard in case of prior ramp-up dosing), or drug combinations. The latter will be the topic of the exercise for this chapter where we showcase how to define custom link functions in brms to handle the combination case.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#exercise",
    "href": "src/02cb_tte_dose_escalation.html#exercise",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.9 Exercise",
    "text": "10.9 Exercise\nOften, investigational cancer drugs are tested on top of a standard-of-care (SoC) treatment. In this exercise, we will explore how we can extend the single-agent time-to-event model to a combination of investigational drug and SoC treatment.\nIn the following, we assume the SoC treatment also has a hazard rate when present, and that there is no drug-drug interactions, i.e., the hazards are additive.\nWe model the log-hazard \\(\\log\\left(h_{B,j}(t|\\tilde{d}_{B,j})\\right)\\) of the SoC treatment (called “drug B” here) in a more simplified way, i.e., either as present “at the reference dose” or absent: \\[\n\\log\\left(h_{B,j}(t|\\tilde{d}_{B,j})\\right) = \\begin{cases}\n  \\alpha_B & \\textrm{if } \\tilde{d}_{B,j} = 1  \\\\\n  -\\infty  & \\textrm{if } \\tilde{d}_{B,j} = 0\n\\end{cases}\n\\] First, we add a second component to the model:\n\n## Stan function to make combo2 non-linear link come to life. To avoid\n## issues with negative infinity (log(0)) whenever one of the drugs is\n## not present, we have to pass in this information.\n## 3. if drug_A is present (finiteA: indicator 1 = present, 0 = absent)\n## 4. if drug_B is present (finnitB: indicator 1 = present, 0 = absent)\ncombo2_tte_stan_code &lt;- \"\n// log(h) = log(exp(muA) + exp(muB))\nreal combo2_tte_log_inv_link(real muA, real muB, int finiteA, int finiteB) {\n  real log_h;\n  if(finiteA == 1 && finiteB == 1) {\n    log_h = log_sum_exp(muA, muB);\n  } else if(finiteA == 1 && finiteB != 1) {\n    log_h = muA;\n  } else if(finiteA != 1 && finiteB == 1) {\n    log_h = muB;\n  } else if(finiteA != 1 && finiteB != 1) {\n    // Need to use negative_infinity() instead of -std::numeric_limits&lt;double&gt;::infinity()\n    // to avoid autodiff issues\n    log_h = negative_infinity();\n  }\n  return log_h;\n}\n\"\n\ncombo2_tte_stanvar &lt;- stanvar(scode = combo2_tte_stan_code, block = \"functions\")\n\n## Define respective R function which is used for simulation of the\n## posterior in R. Note that the inputs are given as draws of matrices\n## which have the format of draws representing rows and columns\n## corresponding to observation rows in the modeling data set for\n## which the posterior is simulated.\ncombo2_tte_log_inv_link &lt;- function(muA, muB, finiteA, finiteB) {\n    N &lt;- ncol(muA) # number of observations\n    D &lt;- nrow(muA) # number of draws\n\n    ## flatten the matrices to vectors (column-major ordering)\n    muA &lt;- as.vector(muA)\n    muB &lt;- as.vector(muB)\n    finiteA &lt;- as.vector(finiteA)\n    finiteB &lt;- as.vector(finiteB)\n  \n    log_muAB &lt;- matrixStats::rowLogSumExps(cbind(muA, muB))\n    \n    log_h &lt;- case_when(\n        finiteA == 1 & finiteB == 1 ~ log_muAB,\n        finiteA == 1 & finiteB != 1 ~ muA,\n        finiteA != 1 & finiteB == 1 ~ muB,\n        finiteA != 1 & finiteB != 1 ~ rep.int(-Inf, times=D*N)\n    )\n\n    ## cast result into draws x observation matrix\n    matrix(log_h, D, N)\n}\n\nNote that the brms model corresponding to the combination model described above is quite different from the single-drug case due to the need for a custom non-linear link function:\n\ncombo2_tte_model  &lt;-\n    bf(num_toxicities | rate(follow_up) ~ combo2_tte_log_inv_link(muA, muB, finite_A, finite_B),\n       nlf(muA ~ interA + exp(slopeA) * std_A),\n       slopeA ~ 1,\n       interA ~ 1,\n       muB ~ 1,\n       nl=TRUE, loop=TRUE, family=poisson\n  )\n\nIn addition, we need the indicators finite_A and finite_B now, in order to numerically deal with cases where one or both of the drugs has zero 0 dose.\nNow for the exercise:\n\nUpdate the ipd data to include the SoC as drug B (hint: use drug name “B” and a reference dose of 1 for SoC present).\n\n\ncombo2_ipd &lt;- ipd |&gt; \n  mutate(dose_B = 1) |&gt; \n  add_std_dose_col(drug = \"B\", dref = 1)\n\n\nDefine a prior for the SoC treatment that corresponds to a DLT probability of 5% in the first cycle with a standard deviation of 1.\n\n\nget_prior(combo2_tte_model, data = combo2_ipd)\n\n  prior class      coef group resp dpar  nlpar lb ub       source\n (flat)     b                           interA            default\n (flat)     b Intercept                 interA       (vectorized)\n (flat)     b                              muB            default\n (flat)     b Intercept                    muB       (vectorized)\n (flat)     b                           slopeA            default\n (flat)     b Intercept                 slopeA       (vectorized)\n\n\n\n## -6.3 approx cloglog(0.05) - log(7*4) for the SoC\ncombo2_tte_prior &lt;-\n  prior(normal(-4.83, 1), nlpar=interA, class=b, coef=Intercept) +\n  prior(normal(0, log(4)/1.96), nlpar=slopeA) +\n  prior(normal(-6.3, 1), nlpar=muB, coef=Intercept)\n\n\nWhich dose of the investigational drug would still be sufficiently safe according to the standard EWOC criterion when administered on top of the SoC treatment?\n\n\ncombo2_tte_brms_model &lt;- combo2_tte_model |&gt; brm(\n  data = combo2_ipd, stanvars = combo2_tte_stanvar, \n  prior = combo2_tte_prior, silent = 2\n)\n\n\ncombo2_dose_ref_data &lt;- dose_ref_data |&gt; \n  mutate(dose_B = 1) |&gt; \n  add_std_dose_col(drug = \"B\", dref = 1)\n\n\ncombo2_risk &lt;- combo2_dose_ref_data |&gt;\n    add_risk_rvars(combo2_tte_brms_model, cycle, schedule_id)\n\n\ncombo2_risk |&gt;\n    mutate(summarize_probs(cprob)) |&gt;\n    select(-num_toxicities, -std_A, -prob, -cprob) |&gt;\n    gt() |&gt; gt_format()\n\n\n\n\n\n\n\n\n\ncombo2_risk |&gt;\n    mutate(summarize_probs(prob)) |&gt;\n    filter(cycle == 3, ewoc_ok) |&gt; \n    select(-num_toxicities, -std_A, -std_B, -prob, -cprob, -finite_A, -finite_B) |&gt;\n    gt() |&gt; gt_format()\n\n\n\n\n\n\n\n\nI.e., we can see that the highest dose that is safe according to the EWOC criterion until the end of cycle 3 would be a dose of 10 for drug A.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02cb_tte_dose_escalation.html#sec-onco-tte-appendix",
    "href": "src/02cb_tte_dose_escalation.html#sec-onco-tte-appendix",
    "title": "10  Time-to-event modelling in Oncology dose escalation",
    "section": "10.10 Appendix",
    "text": "10.10 Appendix\n\n10.10.1 Model derivation\nFor each cycle \\(j\\), we model the probability of observing a DLT event conditional on entering cycle \\(j\\) as a function of the dose \\(d_{j}\\) being administered during this cycle.\nFor each patient \\(i \\in {1, \\ldots, N}\\), we observe either the DLT event time \\(T_i\\) or the censoring time \\(C_i\\), whichever occurs first. We denote this as \n\\[\n(U_i, \\delta_i), \\textrm{ where } U_i = \\min(T_i, C_i) \\textrm{ and } \\delta_i = 1(T_i \\leq C_i).\n\\]\nHere, \\(U_i\\) represents the observed time, and \\(\\delta_i\\) indicates whether a DLT occurred (\\(\\delta_i = 1\\)) or not (\\(\\delta_i = 0\\)).\nThe primary focus of the model is the risk for DLT events over the course of a sequence of cycles. Thus, we do not aim to model accurately the risk for an event within each cycle. While one could model the event and censoring times as interval censored observations, we disregard here the interval censoring and use a continuous time representation of the time to event process. The continuous time representation can be understood as an approximation of the interval censored process with time lapsing in full units of a cycle. As a consequence, the observed event time \\(T_i\\) and censoring times \\(C_i\\) are always recorded at the planned cycle completion time-point (time lapses in units of cycles). Here we assume for simplicity that each cycle \\(j\\) has the same duration \\(\\Delta t\\) such that any time being recorded is a multiple of \\(\\Delta t\\) and we denote time \\(t\\) being during a cycle \\(j\\) with \\(t \\in ((j-1) \\, \\Delta t, j \\, \\Delta t]\\) or equivalently \\(t \\in I_j\\) for brevity.\n\n\n10.10.2 Definitions from Survival Analysis\nTo define our model, we use the following basic definitions from survival analysis. The survival function \\(S(t)\\) and cumulative event probability \\(F(t)\\) are defined as\n\\[\\begin{align*}\nS(t) := & \\Pr(\\textrm{\"No event up to time t\"}) \\\\\n= & \\Pr(T \\geq t) \\\\\n= & 1 - F(t),\n\\end{align*}\\]\nwhere \\(F(t) = \\Pr(T &lt; t)\\). The respective event densities \\(s(t)\\) and \\(f(t)\\) are \\[\ns(t) = \\frac{d}{dt}S(t)\n     = \\frac{d}{dt} \\int_t^\\infty f(u) du\n     = \\frac{d}{dt}[1 - F(t)]\n     = -f(t)\n\\] The hazard function \\(h(t)\\) is defined as \\[\nh(t) := \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Pr(t \\leq T &lt; t + \\Delta t)}{\\Delta t \\cdot S(t)}\n= \\lim_{\\Delta t \\rightarrow 0}\\frac{S(t) - S(t + \\Delta t)}{\\Delta t \\cdot S(t)}\n= \\frac{f(t)}{S(t)} = -\\frac{s(t)}{S(t)}\n\\]\nand the cumulative hazard function \\(H(t)\\) is given by \\[\nH(t) = \\int_0^t h(u) du = -\\log(S(t)).\n\\]\nWe now have two quantities that are of particular interest:\n\nThe cumulative DLT probability up to cycle \\(j\\) given a prescribed dosing \\(\\boldsymbol{d} = (d_{1}, d_{2}, d_{3})\\) over the three cycles: \\[\\Pr(\\textrm{\"DLT up to cycle j\"}|\\boldsymbol{d}) =\nF(t).\\]\nThe conditional DLT probability of cycle \\(j\\) given survival up to cycle \\(j-1\\) (i.e., not having observed a DLT event up to the end of cycle \\(j-1\\)): \\[\\begin{align*}\n   \\, &  \\Pr(\\textrm{\"DLT in cycle j\"}|\\textrm{\"No DLT up to cycle j-1\"}, d_j) \\\\\n= \\, &  \\Pr(T \\leq t + \\Delta t|T &gt; t, d_j)\\\\\n= \\, &  1 - \\exp\\left(-\\int_{t}^{t + \\Delta t} h(u) du \\right).\n\\end{align*}\\]\n\nIn the following, we will also use the complementary log-log link \\(\\mathrm{cloglog}\\), as well as its inverse \\(\\mathrm{cloglog}^{-1}:\\)\n\\[\\begin{align}\n\\mathrm{cloglog}(x) :=&\\ \\log(-\\log(1 - x)), \\textrm{ and} \\\\\n\\mathrm{cloglog}^{-1}(y) =&\\ 1 - \\exp(-\\exp(y)).\n\\end{align}\\]\n\n\n10.10.3 Model definition\nGiven the drug dose \\(d_{j}\\) administered in cycle \\(j\\), we model the log-hazard \\(\\log(h_j(t|\\tilde{d}_{j}))\\) as \\[\n\\log(h_j(t|\\tilde{d}_{j})) = \\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right), \\textrm{ where } \\tilde{d}_{j} = \\frac{d_{j}}{\\tilde{d}}\n\\] and \\(\\tilde{d}\\) is the reference dose. The conditional DLT probability over 1 cycle in this model is then\n\\[\\begin{align}\n\\Pr(T \\leq t + \\Delta t|T &gt; t, \\tilde{d}_{j}) & = 1 - \\exp\\left(-\\int_{t}^{t + \\Delta t} h(u) du \\right)\\\\\n& = 1 - \\exp\\left(-\\int_{t}^{t + \\Delta t} \\exp\\left(\\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right)\\right) du \\right) \\\\\n& = 1 - \\exp\\left(-\\Delta t \\exp\\left(\\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right)\\right) \\right) \\\\\n& = 1 - \\exp\\left(-\\exp\\left(\\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right) + \\log(\\Delta t)\\right) \\right).\n\\end{align}\\]\nThis is closely related to the classic two-parameter Bayesian Logistic Regression Model (BLRM), since the conditional probability of observing a DLT,\n\\[\\begin{align}\n\\mathrm{cloglog}\\Pr(T \\leq t + \\Delta t|T &gt; t, d_j) & = \\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right) + \\log(\\Delta t),\n\\end{align}\\]\nis the same as for the BLRM in cycle one if we elapse time in cycles (then \\(\\Delta t = 1\\) and the \\(\\log(\\Delta t)\\) is zero), except for the link function changing from \\(\\mathrm{logit}\\) to \\(\\mathrm{cloglog}\\).\nThe cumulative DLT probability can also be derived, where \\(j_t\\) is the index of the cycle that time \\(t\\) lies in:\n\\[\\begin{align}\n\\mathrm{cloglog}\\Pr(T \\leq t|\\boldsymbol{d}) & = \\log\\left(\\sum_{j = 1}^{j_t} \\exp\\left(\\alpha + \\beta \\log\\left(\\tilde{d}_{j}\\right) + \\log(\\Delta t)\\right)\\right) = \\mathrm{cloglog} F(t|\\boldsymbol{d}).\n\\end{align}\\]\nLast, but not least, the likelihood for all patients \\(i\\) is then \\[\nL(U|\\alpha, \\beta) = \\prod_i \\underbrace{ f(T_i)^{\\delta_i} }_\\textrm{DLT\nevents} \\, \\underbrace{S(C_i)^{1 - \\delta_i}}_\\textrm{Survival\nevents} \\,,\n\\] and the log-likelihood \\[\nLL(U|\\alpha, \\beta) = \\underbrace{\\sum_i \\log\\left(f(T_i)\\right) \\delta_i}_\\textrm{DLT\nevents} + \\underbrace{\\sum_i \\log\\left(S(C_i)\\right)\\left(1 - \\delta_i\\right)}_\\textrm{Survival\nevents} \\,.\n\\]",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event modelling in Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html",
    "href": "src/02e_multiple_imputation.html",
    "title": "11  Multiple imputation",
    "section": "",
    "text": "11.1 Background",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#background",
    "href": "src/02e_multiple_imputation.html#background",
    "title": "11  Multiple imputation",
    "section": "",
    "text": "11.1.1 Why multiple imputation?\nIn clinical trials, some data we would like to observed will inevitably be missing, or the data that we observe will not be of interest for our estimand (the quantity we really want to estimate with our clinical trial) due to some intercurrent event (https://database.ich.org/sites/default/files/E9-R1_Step4_Guideline_2019_1203.pdf). A common approach to deal with this situation is to perform some form of imputation for such values. We do this, because there is no realistic scenario under which an analysis of only the observed data (“complete case analysis”) targets a meaningful estimand in a valid way.\nWhen would a complete case analysis ever be appropriate? One scenario would be for a hypothetical “as-if-patients-had-been-able-to-complete-treatment” estimand, if we assume that patients stopping treatment or stopping trial participation happens completely at random and has nothing to do with patient characteristics and previous (efficacy and safety) outcomes for the patients. However, this is obviously rather implausible and even if it were the case, a pharmaceutical company would be hard pressed to convince the scientific community and regulatory authorities that it is so.\nAt least until about 2010, people would often impute a single value to replace any data that were missing or not of interest. One popular single value imputation approach was “last-observation-carried-forward” (LOCF), in which the last observed value of interest was used to as a single imputation for any future values. However, even if the imputed value is the best possible guess for the missing value, imputing a single value completely ignores the uncertainty about the unobserved value. This is where multiple imputation (MI) comes in. MI imputes not one single value, but multiple ones. This set of values represent a sample from a distribution that describes the uncertainty about the unobserved values.\n\n\n11.1.2 How do the inner workings of multiple imputation look like?\nIn general, an analysis using MI involves four steps (Rubin Donald B. “Multiple imputation for nonresponse in surveys”. Hoboken: Wiley; 2004.):\n\nA (Bayesian) imputation model is fit to the data and \\(M\\) samples are drawn from the posterior distribution of the model parameters given the data.\n\nThis imputation model could be a single model for all treatment groups without treatment group by covariate interactions, a model that has completely separate parameters for each treatment group, or a model that assumes that (some) parameters in different treatment groups are similar.\nIn a single model for all treatment groups, we can also allow for treatment by covariate interactions.\nGenerally, we need to ensure that imputation model is “congenial” to the analysis model. I.e. it needs to be able to capture all the complexity present in the analysis model. It will often be more complex than the analysis model, for example it might be a model for the longitudinal data of patients over time when the analysis model is only for the final study visit, or it might separately model on- and off-treatment records for patients.\n\nFor each of the \\(M\\) posterior samples \\(M=1,\\ldots,M\\) the missing data are simulated based on the sampled values from the posterior of the model parameters. What set of parameter values applies depends on the estimand of interest.\n\nUnder a treatment policy estimand implemented using a “jump-to-reference” (J2R) approach, we would impute missing values after treatment discontinuation in all treatment groups based on placebo group parameters.\nAlternatively, we could target a treatment policy estimand by imputing missing values after treatment discontinuation in each treatment group based on parameters that describe what happens in that treatment group after treatment discontinuation (based on observed off-treatment data).\nUnder a hypothetical “as-if-patients-had-been-able-to-complete-treatment” estimand, we would impute missing values for patients based on the parameters of the treatment group they were assigned to.\nWe will often use have patient specific latent random effect(s) that are used to reflect the correlation of observation from the same patient. In both of the scenarios described above, we would use the sampled values of the random effect(s) for each patients to simulate missing values for the patient. It usually computationally convenient to use latent normal random effects.\n\nEach of the \\(M\\) sets of partially directly observed and partially imputed data are analyzed separately. This is done using an analysis model that need not be identical to the imputation model.\nThe results of the \\(M\\) analyses are aggregated e.g. using Rubin’s rule (Rubin Donald B. “Multiple imputation for nonresponse in surveys”. Hoboken: Wiley; 2004.).\n\nThis all sounds pretty complicated, but it gets surprisingly easy with brms. Interestingly, brms let’s us specify some pretty complex imputation models for a wide range of situations that would otherwise require bespoke and error-prone code e.g. in Stan. We will illustrate that in the next sections.\n\n\n11.1.3 How many imputations to use?\nWhile older literature suggests as few as 3 to 5, or maybe 50 imputations, there is usually some further reduction in standard errors by using a larger number of observations such as 250. Additionally, it is desirable to make our results as independent of pure play of chance in our MCMC sampling (it is just akward when the interpretation of the results may change with the choice of a random number seed). Thus, in practice we would frequently use 1000 or even 2500 imputations, as long as the added runtime is not prohibitive.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#data",
    "href": "src/02e_multiple_imputation.html#data",
    "title": "11  Multiple imputation",
    "section": "11.2 Data",
    "text": "11.2 Data\nWe will need the following R packages in this Section:\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(ggrepel)\nlibrary(MASS)\nlibrary(emmeans)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(7894562)\n# we also disable normalization of the likelihood which accelerates Poisson models used\noptions(brms.normalize=FALSE)\n\nLet us consider some simulated data for a trial in chronic obstructive pulmonary disease (COPD). Patients are enrolled into this trial, if they had at least one COPD exacerbation requiring oral corticosteroids, antibiotics, emergency department visit or hospitalization, or leading to death. Patients are then randomized to receive a drug or placebo and followed for up to 1 year. However, some patients discontinue treatment before 1 year. Some proportion of these patients agrees to be followed until the end of the trial, while some proportion is lost to follow-up.\n\npatients &lt;- 2000 # Total number of patients randomized\nmultiple &lt;- 10 # Multiple of patients randomized to be simulated before applying inclusion criteria\nminmum_for_inclusion &lt;- 1 # Mininum number of events in previous year for inclusion in trial\nmean_rate &lt;- 1 # Mean event rate in simulated population before applying inclusion criteria\nkappa &lt;- 1.5 # Dispersion parameter of simulated population\nstudy_length &lt;- 1 # Length of simulated study\ndiscontinuation_rate &lt;- 0.2 # Exponential rate at which patients discontinue treatment\nloss_proportion &lt;- 0.5 # Proportion of discontinued patients that is lost to follow-up\nnum_imputations &lt;- 500 # number of imputations to use\nset.seed(67588)\n\n# Simulate true event rates for patient population that is screened\npatient_rates &lt;- rgamma(n=patients*multiple, shape=1/kappa, rate = 1/kappa)*mean_rate\n# Draw random number of events in previous year before trial\nprevious_year &lt;- rpois(n=patients*multiple, patient_rates)\n# Apply inclusion criteria\npatient_rates &lt;- patient_rates[previous_year&gt;=minmum_for_inclusion][1:patients]\nprevious_year &lt;- previous_year[previous_year&gt;=minmum_for_inclusion][1:patients]\n\n# Simulate data for patients that met inclusion criteria\ncount_data &lt;- tibble(patient=1:patients, \n                    BASE = previous_year,\n                    treatment = ifelse(patient&lt;=patients/2, 0L, 1L),\n                    discontinuation_time = rexp(n=patients, rate=discontinuation_rate),\n                    lost_to_followup = rbinom(n=patients,size=1, prob=loss_proportion),\n                    patient_follow_up = min(study_length, discontinuation_time),\n                    #drop_out = (drop_out_time&lt;study_length),\n                    patient_rate = patient_rates[patient]) %&gt;%\n  left_join(expand_grid(patient=1:patients,\n                        period=factor(1L:3L, levels=1L:3L, \n                                      labels=c(\"On-treatment\", \n                                               \"Post-treatment\", \n                                               \"Lost-to-follow-up\"))), \n            by = \"patient\", multiple = \"all\") %&gt;%\n  mutate(follow_up = case_when(\n    period==\"On-treatment\" ~ pmin(discontinuation_time, study_length),\n    period==\"Post-treatment\" ~ (1-lost_to_followup) * pmax(study_length-discontinuation_time, 0), \n    period==\"Lost-to-follow-up\" ~ lost_to_followup * pmax(study_length-discontinuation_time, 0),\n    TRUE ~ 1.0),\n    events = ifelse(period==\"Lost-to-follow-up\", NA_integer_,\n                    rpois(n=patients*3, \n                          lambda=follow_up*patient_rate*0.5^(treatment*(period==\"On-treatment\")))),\n    patient = factor(patient, levels=1:patients),\n    treatment = factor(treatment, levels=0L:1L),\n    logBASE = log(BASE))\n\ncount_data  %&gt;% \n  group_by(treatment, period) %&gt;% \n  summarize(Events=sum(events), \n            `Patient-years`=sum(follow_up), \n            `Event rate (/p-y)` = Events/`Patient-years`,\n            `Total events prev. year` = sum(BASE),\n            `Rate in previous year` = mean(BASE),\n            .groups=\"drop\") %&gt;%\n  knitr::kable(digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nperiod\nEvents\nPatient-years\nEvent rate (/p-y)\nTotal events prev. year\nRate in previous year\n\n\n\n\n0\nOn-treatment\n1505\n903.36\n1.67\n2182\n2.18\n\n\n0\nPost-treatment\n78\n48.18\n1.62\n2182\n2.18\n\n\n0\nLost-to-follow-up\nNA\n48.47\nNA\n2182\n2.18\n\n\n1\nOn-treatment\n753\n898.05\n0.84\n2162\n2.16\n\n\n1\nPost-treatment\n72\n50.63\n1.42\n2162\n2.16\n\n\n1\nLost-to-follow-up\nNA\n51.32\nNA\n2162\n2.16",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#model-description",
    "href": "src/02e_multiple_imputation.html#model-description",
    "title": "11  Multiple imputation",
    "section": "11.3 Model description",
    "text": "11.3 Model description\n\n11.3.1 Negative binomial regression as a Poisson random effects model\nOur example uses overdispersed count data that follows a negative binomial distribution. We will use the parameterization of the negative binomial distribution, which can be derived as a Poisson random effects model with independent and identically distributed (i.i.d.) random effects. These random effects follow a gamma distribution with shape parameter \\(1/\\kappa\\) and a rate parameter \\(1/\\kappa\\) \\[\\begin{equation*}\nU_i \\sim \\text{Gamma}(1/\\kappa, 1/\\kappa),\n\\end{equation*}\\] where \\(\\kappa&gt;0\\). The distribution of the \\(Y_i\\) for patient \\(i=1,\\ldots,N\\) conditional on the random patient effect \\(U_i=u_i\\) and on the observed follow-up time \\(T_i=t_i\\) is \\[\\begin{equation}\\label{eq:poissondist}\nY_i|(U_i=u_i \\text{ and } T_i=t_i) \\sim \\text{Poisson}(\\mu u_i t_i).\n\\end{equation}\\] Note that as \\(\\kappa\\) approaches zero the negative binomial distribution becomes increasingly similar to a Poisson distribution. The glm.nb in the MASS R package instead uses a parametrization in terms of \\(\\theta := 1/\\kappa\\).\nThus, a negative binomial regression model is a Poisson random effects model with a log-link function: \\[\\begin{equation}\\label{eq:Poisson_re_model}\n\\log E(Y_i|U_i=u_i \\text{ and } T_i=t_i) = \\log \\mu + \\log u_i + \\log t_i\n\\end{equation}\\] or more generally: \\[\\begin{equation*}\n\\log E(Y_i|U_i=u_i \\text{ and } T_i=t_i) = \\boldsymbol{x_i}\\boldsymbol{\\beta} + \\log u_i + \\log t_i.\n\\end{equation*}\\] \\(\\boldsymbol{x_i}\\) is the covariate vector for subject \\(i\\) and \\(\\log t_i\\) is an offset variable — i.e. a covariate with coefficient 1. In a clinical trial setting \\(\\boldsymbol{\\beta}\\) will usually consist of the coefficient for an intercept \\(\\beta_0\\), coefficients \\(\\beta_k\\) for each test treatment group \\(k=1, \\ldots, K\\) — these are the log-rate-ratios for each test group compared to the control group — and the coefficients for other covariates. Other covariates might be e.g.@ the logarithm of number of events in some preceding time period.\n\n\n11.3.2 Extension to multiple observed time periods per patient\nWe can extent this model to having multiple records per patient with potentially different follow-up and covariates in each time period, with all observations from the same patient being linked through/sharing the same random effect \\(u_i\\). E.g. for observation \\(j\\) of patient \\(i\\), we can use \\[\\begin{equation*}\n\\log E(Y_{ij}|U_i=u_i \\text{ and } T_{ij}=t_{ij}) = \\boldsymbol{x_{ij}}\\boldsymbol{\\beta} + \\log u_i + \\log t_{ij},\n\\end{equation*}\\]\nInstead of gamma-distributed random effects \\(\\log U_i\\), we will use normally distributed random effects \\(\\nu_i \\sim N(0, \\tau)\\) in our imputation model. This is computationally an easier model to fit for brms. Additionally, we can approximate the distribution of a log-transformed gamma random variable quite well with a normal distribution.\n\n\n11.3.3 An imputation framework for missing count data\nWe will look at different estimands for count data that have all been used in practice:\n\nHypothetical estimand (“as if all patients had stayed on treatment until the end of the trial”)\n\nApproach #1: Maximum likelihood estimation using a negative binomial regression model for the on-treatment data (discarding any observed off-treatment data)\nApproach #2: Explicit imputation of off-treatment or lost-to-follow-up time periods based on on-treatment data.\nThese two approaches should give almost identical results.\nIn the second approach we fit a Bayesian imputation model on the on-treatment data. Then we perform prediction with this model (=multiple imputation) setting the treatment covariate to the treatment assigned as randomization for the records that need imputation. We will impute hypothetical on-treatment data for both time periods that were unobserved, but also for any observed post-treatment time periods.\n\nTreatment policy estimand (reflecting that after treatment discontinuations patients are off treatment):\n\nApproach #1: Imputation from observed off-treatment data\nApproach #2: Imputation based on the control (“reference”) group event rate (“jump-to-reference” or “J2R”)\nIn the first approach, we fit a Bayesian imputation model on the on-treatment and observed off-treatment data. We add a “type of time period” (on- or off-treatment) covariate, as well as its interaction with the assigned treatment group. This allows us to predict off-treatment records for all treatment groups based on the observed off-treatment data by setting the type of time period to “off-treatment” for the periods that we did not observe data for.\nIn the second approach, we fit a Bayesian imputation model on the on-treatment data and impute the unobserved off-treatment data based on the reference group distribution (i.e. we set the treatment covariate to the reference group before predicting the unobserved data for these time periods). We do not impute the observed off-treatment data and include it in the data for each patient before the analysis of each imputed dataset.\nArguably the first approach is the more logical, because it directly uses the available evidence on what happens after discontinuing treatment. However, there are some difficulties. Firstly, there is often very little observed post-treatment data so that there will be a lot of uncertainty about the post-discontinuation event rate. Secondly, it is very challenging to model to what extent the event rate may change the more time has passed post-treatment-discontinuation (e.g. is there a residual treatment effect that needs to “wash-out” over time?) and in practice this is usually ignored.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#implementation",
    "href": "src/02e_multiple_imputation.html#implementation",
    "title": "11  Multiple imputation",
    "section": "11.4 Implementation",
    "text": "11.4 Implementation\n\n11.4.1 Imputation from observed off-treatment data\nFirst, let us conduct an imputation model based on the observed off-treatment data, in which we specify that event rates can be different on- and off-treatment for each treatment group. Using this model, we perform predictions for the missing post-treatment values (patients lost-to-follow-up).\nWe use family=poisson() here, because a Poisson model with a random patient effect on the intercept is approximately equivalent to a negative binomial regression model and this is a way of reflecting the correlation of multiple observation periods for a patient that is consistent with that. However, we could also make the imputation model more complex e.g. by allowing counts to be overdispersed within patient by using family=negbinomial(), or allowing for some interactions such as treatment:logBASE or period:logBASE.\n\n# Fit imputation model\nbrmfit1 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + period + treatment:period + logBASE,\n               data=count_data %&gt;% \n                   filter(period %in% c(\"On-treatment\", \"Post-treatment\") & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 11.2 seconds.\nChain 2 finished in 10.7 seconds.\nChain 3 finished in 13.6 seconds.\nChain 4 finished in 13.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 49.2 seconds.\n\n# Predictions for missing (lost-to-follow-up) post-treatment values\n# Output format is a matrix with each column being a patient record and each row being one imputation\nimputations1 &lt;- predict(brmfit1, \n                        newdata=count_data %&gt;% \n                            filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n                            mutate(period=\"Post-treatment\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\n# Combine predictions with existing data frame of lost-to-follow-up records\n# (results in one row per imputation per patient), then bind together with \n# non-missing values (on-treatment and observed off-treatment values), for\n# which we have one record per patient and `.imputation` will be missing.\nimputed1 &lt;- count_data %&gt;% \n    filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations1[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n    dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period %in% c(\"On-treatment\", \"Post-treatment\") & \n                     follow_up&gt;0))\n\n\n\n11.4.2 Imputation based on the control group event rate (“jump-to-reference”)\nNow, we do an imputation based on control group data (i.e. assuming that after treatment discontinuation patients are off treatment and we use either the observed off-treatment data or impute missing data based on the control group event rate).\nFor this the code looks very similar to what we did in the previous subsection. We just slightly change our imputation model, and now predict based on setting the period covariate to \"On-treatment\" and the treatment covariate to \"0\" (=control group).\n\nbrmfit2 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + logBASE,\n               data=count_data %&gt;% \n                   filter(period==\"On-treatment\" & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 10.1 seconds.\nChain 2 finished in 10.1 seconds.\nChain 3 finished in 12.9 seconds.\nChain 4 finished in 12.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.3 seconds.\nTotal execution time: 45.7 seconds.\n\nimputations2 &lt;- predict(brmfit2, \n                        newdata=count_data %&gt;% \n                            filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n                        mutate(period=\"On-treatment\",\n                               treatment=\"0\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\nimputed2 &lt;- count_data %&gt;% \n    filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations2[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n  dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period %in% c(\"On-treatment\", \"Post-treatment\") & \n                     follow_up&gt;0))\n\n\n\n11.4.3 Imputation under a hypothetical estimand\nFinally, we also do an imputation based on on-treatment data of the same treatment group. I.e. we do an analysis under a hypothetical estimand “as if all patients had finished treatment”. As before, we just change our imputation model and the covariates for the data to be imputed accordingly.\n\nbrmfit3 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + logBASE,\n               data=count_data %&gt;% \n                   filter(period==\"On-treatment\" & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 12.6 seconds.\nChain 2 finished in 9.7 seconds.\nChain 3 finished in 10.3 seconds.\nChain 4 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.4 seconds.\nTotal execution time: 45.7 seconds.\n\nimputations3 &lt;- predict(brmfit3, \n                        newdata=count_data %&gt;% \n                            filter(period %in% c(\"Lost-to-follow-up\", \"Post-treatment\") & follow_up&gt;0) %&gt;%\n                         mutate(period=\"On-treatment\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\nimputed3 &lt;- count_data %&gt;% \n    filter(period %in% c(\"Lost-to-follow-up\", \"Post-treatment\") & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations3[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n    dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period==\"On-treatment\" &  follow_up&gt;0))\n\n\n\n11.4.4 Combining the imputations and a quick look at them\nWe combine the imputations from the different approaches and have a look at the mean rates under the different imputation approaches. As you can see, assuming a continued treatment effect results in a lower imputed mean event rate than assuming a cessation of the treatment effect (or basing the imputation on observed off-treatment data).\n\n# How different are the numbers?\nimputed &lt;- bind_rows(\n  imputed1 %&gt;%\n    mutate(Method=\"Retrieved drop-out imputation\"),\n  imputed2 %&gt;%\n    mutate(Method=\"Jump-to-reference (J2R)\"),\n  imputed3 %&gt;%\n    mutate(Method=\"Hypothetical (MAR)\")\n) \n\nimputed %&gt;%\n  group_by(Method, treatment, period, patient) %&gt;%\n  summarize(event_rate=mean(events/follow_up), .groups=\"drop\") %&gt;%\n  group_by(treatment, period, Method) %&gt;%\n  summarize(event_rate=mean(event_rate), .groups=\"drop\")\n\n# A tibble: 18 × 4\n   treatment period            Method                        event_rate\n   &lt;fct&gt;     &lt;fct&gt;             &lt;chr&gt;                              &lt;dbl&gt;\n 1 0         On-treatment      Hypothetical (MAR)                 1.61 \n 2 0         On-treatment      Jump-to-reference (J2R)            1.61 \n 3 0         On-treatment      Retrieved drop-out imputation      1.61 \n 4 0         Post-treatment    Hypothetical (MAR)                 1.68 \n 5 0         Post-treatment    Jump-to-reference (J2R)            1.48 \n 6 0         Post-treatment    Retrieved drop-out imputation      1.48 \n 7 0         Lost-to-follow-up Hypothetical (MAR)                 1.56 \n 8 0         Lost-to-follow-up Jump-to-reference (J2R)            1.55 \n 9 0         Lost-to-follow-up Retrieved drop-out imputation      1.54 \n10 1         On-treatment      Hypothetical (MAR)                 0.830\n11 1         On-treatment      Jump-to-reference (J2R)            0.830\n12 1         On-treatment      Retrieved drop-out imputation      0.830\n13 1         Post-treatment    Hypothetical (MAR)                 0.859\n14 1         Post-treatment    Jump-to-reference (J2R)            1.49 \n15 1         Post-treatment    Retrieved drop-out imputation      1.49 \n16 1         Lost-to-follow-up Hypothetical (MAR)                 0.899\n17 1         Lost-to-follow-up Jump-to-reference (J2R)            1.75 \n18 1         Lost-to-follow-up Retrieved drop-out imputation      1.49 \n\n\n\n\n11.4.5 Negative binomial regression for each imputed dataset\nWe can now conduct an analysis of each imputed dataset and look at how much the results vary between them. To do that, we need to fit a negative binomial regression to each imputed dataset. Negative binomial regression is implemented in all popular statistical software, but there are some particular challenges. Skip the next sub-section, if you are not interested in these particular points regarding negtive binomial regression using R.\n\n11.4.5.1 Negative binomial regression in R (optional subsection)\nFor example, the SAS/STATsoftware provides the GENMOD procedure and for the Poisson random effects version the COUNTREG procedure. Similarly, R has the glm.nb function in the MASS package. \nThese implementations differ in their parameterization. The GENMOD procedure estimates \\(\\kappa\\) and reverts to Poisson regression on the boundary of the parameter space when \\(\\hat{\\kappa}=0\\). The glm.nb function parameterizes the model in terms of \\(\\theta:=1/\\kappa\\) and its algorithm will not converge to a finite estimate \\(\\hat{\\theta}\\) whenever the maximum likelihood estimator for \\(\\kappa\\) is \\(\\hat{\\kappa}=0\\). However, after a sufficient number of iterations \\(\\hat{\\theta}\\) will be large enough that the estimates of the regression coefficients will be those we would obtain by Poisson regression — alternatively, we can use the results from Poisson regression in this case. This is implemented in the code below.\nInstead of the preferable observed Fisher information used in the GENMOD procedure, the glm.nb function uses the expected Fisher information for the standard errors of the regression coefficients — while the standard error for \\(\\hat{\\theta}\\) is based on the observed rather than the expected Fisher information — as of version 7.3-47 of the MASS package. Especially for small samples sizes this may result in too small standard errors. Thus, they should — especially for confirmatory clinical trials — be corrected, which is easily done using code provded by Bartlett.  \n\nglm.nb.cov &lt;- function(mod) {\n  # Basis of code for this function: \n  # https://stats.stackexchange.com/questions/221648/negative-binomial-regression-in-r-allowing-for-correlation-between-dispersion\n  #given a model fitted by glm.nb in MASS, this function returns a variance covariance matrix for the\n  #regression coefficients and dispersion parameter, without assuming independence between these\n  #note that the model must have been fitted with x=TRUE argument so that design matrix is available\n  #formulae based on p23-p24 of http://pointer.esalq.usp.br/departamentos/lce/arquivos/aulas/2011/LCE5868/OverdispersionBook.pdf\n  #and http://www.math.mcgill.ca/~dstephens/523/Papers/Lawless-1987-CJS.pdf\n  \n  k &lt;- mod$theta\n  p &lt;- dim(vcov(mod))[1]\n  \n  #construct observed information matrix\n  obsInfo &lt;- array(0, dim=c(p+1, p+1))\n  \n  #first calculate top left part for regression coefficients\n  for (i in 1:p) {\n    for (j in 1:p) {\n      obsInfo[i,j] &lt;- sum( (1+mod$y/mod$theta)*mod$fitted.values*mod$x[,i]*mod$x[,j] / (1+mod$fitted.values/mod$theta)^2  )\n    }\n  }\n  \n  #information for dispersion parameter\n  obsInfo[(p+1),(p+1)] &lt;- -sum(trigamma(mod$theta+mod$y) - trigamma(mod$theta) -\n                                 1/(mod$fitted.values+mod$theta) + (mod$theta+mod$y)/(mod$theta+mod$fitted.values)^2 - \n                                 1/(mod$fitted.values+mod$theta) + 1/mod$theta)\n  \n  #covariance between regression coefficients and dispersion\n  for (i in 1:p) {\n    obsInfo[(p+1),i] &lt;- -sum(((mod$y-mod$fitted.values) * mod$fitted.values / ( (mod$theta+mod$fitted.values)^2 )) * mod$x[,i] )\n    obsInfo[i,(p+1)] &lt;- obsInfo[(p+1),i]\n  }\n  \n  #return variance covariance matrix\n  solve(obsInfo)\n}\n\nfitmodel &lt;- function(data) {\n  # Function to fit NB model - returns result of Poisson model,\n  # if estimate of 1/dispersion parameter appears to go towards infinity.\n  # Additionally, standard errors are calculated using the observed\n  # information matrix by calling the previously defined function for that.\n  poifit &lt;- glm(\n    data = data,\n    formula = events ~ treatment + logBASE + offset(logfollowup),\n    control = glm.control(maxit = 2500),\n    family = poisson()\n  )\n  # glm.nb may fail to converge with a message such as\n  # \"Warning: step size truncated due to divergence\"\n  # \"Error in glm.fitter(X[, \"(Intercept)\", drop = FALSE], Y, w, offset = offset,: NA/NaN/Inf in 'x'\"\n  # We need to make sure we catch that issue. In that case we set nbfit = list(theta=10001), which\n  # results in Poisson regression results being used (see check on nbfit$theta later).\n  nbfit &lt;- tryCatch(\n      glm.nb(\n        data = data,\n        formula = aval ~ events ~ treatment + logBASE + offset(logfollowup),\n        start = poifit$coefficients,\n        init.theta = 1.25,\n        control = glm.control(maxit = 30),\n        x = TRUE\n      ), warning = function(err)\n        list(theta = 99999),\n      error = function(err)\n        list(theta = 99999)\n    )\n  \n  emm_options(msg.nesting = FALSE,\n              msg.interaction = FALSE)\n  if (nbfit$theta &gt; 10000) {\n    results &lt;- data.frame(\n      treatment = sort(unique(data$treatment)),\n      logrr = c(0, summary(poifit)$coefficients[2, 1]),\n      logrrSE = c(NA, summary(poifit)$coefficients[2, 2]),\n      as.matrix(data.frame(summary(\n        emmeans(\n          poifit,\n          ~ factor(treatment),\n          weights = \"proportional\",\n          at = list(logfollowup = 0)\n        )\n      ))[, c(\"emmean\", \"SE\")])\n    )\n  } else {\n    results &lt;- data.frame(\n      trt = sort(unique(data$treatment)),\n      logrr = c(0, summary(nbfit)$coefficients[2:2, 1]),\n      logrrSE = c(NA, sqrt(diag(\n        glm.nb.cov(nbfit)\n      ))[2:2]),\n      as.matrix(data.frame(summary(\n        emmeans(\n          nbfit,\n          ~ factor(treatment),\n          weights = \"proportional\",\n          at = list(logfollowup = 0),\n          vcov = glm.nb.cov(nbfit)[1:length(nbfit$coefficients), 1:length(nbfit$coefficients)]\n        )\n      ))[, c(\"emmean\", \"SE\")])\n    )\n    \n  }\n  \n  return(results)\n}\n\n\n\n11.4.5.2 Fitting the negative binomial regression to the imputed data\nActually fitting negative binomial regression on each imputed datasets is now easy with the function above:\n\nresults &lt;- tibble(.imputation = 1:max(imputed$.imputation, na.rm = T)) %&gt;%\n  mutate(results = parallel::mclapply(.imputation , function(x)\n    bind_rows(\n      imputed %&gt;%\n        filter(\n          Method == \"Hypothetical (MAR)\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Hypothetical (MAR)\"),\n      imputed %&gt;%\n        filter(\n          Method == \"Retrieved drop-out imputation\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Retrieved drop-out imputation\"),\n      imputed %&gt;%\n        filter(\n          Method == \"Jump-to-reference (J2R)\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Jump-to-reference (J2R)\")\n    ))) %&gt;%\n  unnest(results)\n\nresults %&gt;%\n  ggplot(aes(x=logrr, xmin=logrr-qnorm(0.975)*logrrSE,\n             xmax=logrr+qnorm(0.975)*logrrSE, y=.imputation, )) +\n  theme_bw(base_size=16) +\n  geom_vline(xintercept=0, linetype=2) +\n  geom_point() +\n  geom_errorbarh() +\n  facet_wrap(~Method) +\n  scale_x_continuous(limits = c(-0.9, 0)) +\n  ylab(\"Imputation number\") +\n  xlab(\"log-rate ratio (drug versus placebo, &lt;0 favors drug)\")\n\n\n\n\n\n\n\n\n\n\n\n11.4.6 Combining the results of the analysis of each dataset using Rubin’s rule\nFinally, we use Rubin’s rule to combine results across the different imputed datasets for each imputation method.\n\nrubins_rule &lt;- function(estimates, SEs) {\n  nmi &lt;- length(estimates)\n  mi_estimate &lt;- mean(estimates)\n  mi_SE &lt;- sqrt( sum(SEs^2)/nmi + (1+1/nmi) * sum((mi_estimate-estimates)^2) / (nmi-1) )\n  mi_Z &lt;- mi_estimate/mi_SE\n  mi_p &lt;- 2*min(pnorm(mi_Z), 1-pnorm(mi_Z))\n  return( tibble(logrr=mi_estimate, logrrSE=mi_SE, p=mi_p) )\n}\n\nfinal_results &lt;- results %&gt;%\n  group_by(Method) %&gt;%\n  do(rubins_rule(.$logrr, .$logrrSE)) %&gt;%\n  ungroup() %&gt;%\n  bind_rows(\n    count_data %&gt;%\n      filter(period==\"On-treatment\" & follow_up&gt;0) %&gt;%\n      mutate(logfollowup=log(follow_up)) %&gt;%\n      fitmodel(data=.) %&gt;% \n      as_tibble() %&gt;%\n      filter(treatment==\"1\") %&gt;% \n      mutate(Method=\"Hypothetical\\n(NegBin for on-treatment data)\",\n             p=2*min(pnorm(logrr/logrrSE), 1-pnorm(logrr/logrrSE)) ) %&gt;%\n      dplyr::select(-treatment, -emmean, -SE)\n  ) %&gt;%\n  mutate(upper = logrr + qnorm(0.975)*logrrSE,\n         lower = logrr - qnorm(0.975)*logrrSE)\n\n\n\n11.4.7 What if we want to do a Bayesian analysis after MI?\nIf we have multiple imputated datasets and wish to perform Bayesian inference, we would recommend the approach of described in the Bayesian Data Analysis book by Gelman et al. (Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A. and Rubin, D.B., 2014. Bayesian data analysis (3rd ed.). CRC press. p.452) that uses the (equally weighted) mixture distribution of the posterior distributions as the combined posterior distribution. I.e. we use all the (or a random subset) of the MCMC samples from each analysis as samples from the combined posterior distribution. This has been reported to perform well with a sufficient number of imputations such as 100 (Zhou, X. and Reiter, J.P., 2010. A note on Bayesian inference after multiple imputation. The American Statistician, 64(2), pp.159-163.).\nWe could of course write some code to fit separate Bayesian models for each imputed dataset and then combine the posterior samples, but we do not have to, because brms can take care of it for us. E.g. the code below would run 1 MCMC chain for each of 20 imputed datasets (without specifying any prior distributions, which we would in practice of course also do).\n\nlibrary(future)\nplan(multisession)\nfit_multiple1 &lt;- brm_multiple(\n    events | rate(follow_up) ~ treatment + logBASE,\n    data = map(1L:20L, \n               function(x) imputed1 %&gt;% \n                 filter(is.na(.imputation) | .imputation==x) %&gt;%\n                 group_by(patient, logBASE, treatment) %&gt;%\n                 summarize(events=sum(events), follow_up=sum(follow_up))),\n    family = negbinomial(),\n    chains = 1, seed = 6234)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#results",
    "href": "src/02e_multiple_imputation.html#results",
    "title": "11  Multiple imputation",
    "section": "11.5 Results",
    "text": "11.5 Results\nAs we can see the results from imputing explicitly “as if all patients had finished treatment” matches the results from a negative binomial regression of on-treatment data rather closely. Similarly, the two approaches for treatment policy estimands (jump-to-reference or imputation based on retrieved drop-outs) give quite similar results, but with higher variability when basing imputations on the small number of subjects with post-treatment-discontinuation data.\n\nfinal_results %&gt;%\n  ggplot(aes(x=exp(logrr), y=Method, xmin=exp(lower), xmax=exp(upper))) +\n  geom_vline(xintercept=1, linetype=1) +\n  geom_point() +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10(limits=c(0.25, 1.0)) +\n  xlab(\"Rate ratio (drug vs. placebo)\\n(values &lt;1.0 favor drug over placebo)\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#conclusion",
    "href": "src/02e_multiple_imputation.html#conclusion",
    "title": "11  Multiple imputation",
    "section": "11.6 Conclusion",
    "text": "11.6 Conclusion\nImputation for overdispersed count data is not covered by popular alternatives for missing data imputation such as the PROC MI procedure in SAS or the Amelia package in R. We are able to quickyly implement something sensible in brms. For this purpose, brms enabled us to fit imputation models and do imputation based on MAR (hypothetical estimand), J2R (treatment policy estimand) or retrieved drop-out data (treatment policy estimand).\nWe can also fit multivariate imputation models (see the corresponding brms vignette) across different types of outcomes that could be missing and correlated. For that we would use syntax like bf1 &lt;- bf(severity | subset(is_obs_severity) ~ (1|p|gr(USUBJID, by=treatment))) + cumulative(), bf2 &lt;- bf(CHG1 | subset(is_obs_CHG1) ~ treatment + (1|p|gr(USUBJID, by=treatment))) + gaussian() and bf9 &lt;- bf(country | subset(is_obs_country) ~ (1|p|gr(USUBJID, by=treatment))) + categorical() in combination with formula = bf1 + bf2 + bf3 + set_rescor(FALSE) in our call to the brm function. However, such models can be more challenging to fit and we aim to add examples on this in the future.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#excercises",
    "href": "src/02e_multiple_imputation.html#excercises",
    "title": "11  Multiple imputation",
    "section": "11.7 Excercises",
    "text": "11.7 Excercises\n\n11.7.1 Excercise 1: Food allergy\nThis is example is about a simulated randomized controlled trial of a new treatment compared with placebo for peanut allergy. The primary endpoint is the result of a double-blind, placebo controlled food challenge (DBPCFC), during which patients take increasing amounts of peanut flour (1, 3, 10, 30, 100, 300, 600 and 1000 mg). Tolerating a higher amount of peaanut flour is a better outcome and the proportion of patients tolerating at least 600 mg at week 52 will be analyzed in the primary analysis. DBPCFC is performed at baseline, at week 12, 26 and 52 (1-year). The code below creates a tibble with simulated data from a hypothetical study.\n\nset.seed(1234)\n\npatients = 200L\namounts = c(1, 3, 10, 30, 100, 300, 600, 1000)\npatient_re = rnorm(patients, mean=1, sd=0.5)\n\nsimulated = expand_grid(patient=1L:patients, visit=c(0L,12L,26L,52L)) %&gt;%\n  mutate(Treatment = 1L*(patient&gt;patients/2)) %&gt;%\n  arrange(patient, visit) %&gt;%\n  group_by(patient) %&gt;%\n  mutate(\n    discontinued = ( cumsum( (visit&gt;=12L)*rbinom(n=n(), size = 1, prob=0.2) ) &gt;=1),\n    lost_to_followup = ( cumsum( discontinued * rbinom(n=n(), size=1, prob=0.3) ) &gt;= 1) ,\n    latent = exp(rnorm(n=n(), sd=1.0) + 4.0*(visit&gt;=12L) + ((visit&gt;=12L)*2.0+(visit&gt;=26)*2.0+(visit&gt;=52)*1.0)*Treatment*(discontinued==F) + patient_re[patient]),\n    aval = case_when(latent&lt;1 ~ 1L,\n                     latent&lt;3 ~ 2L,\n                     latent&lt;10 ~ 3L,\n                     latent&lt;30 ~ 4L,\n                     latent&lt;100 ~ 5L,\n                     latent&lt;300 ~ 6L,\n                     latent&lt;600 ~ 7L,\n                     latent&lt;1000 ~ 8L,\n                     TRUE ~ 9L))  %&gt;%  \n  mutate(aval = ifelse(lost_to_followup, NA_integer_, aval)) %&gt;% \n  ungroup() %&gt;%\n  dplyr::select(-latent)\n\nmodel_data = simulated %&gt;%\n  filter(visit&gt;0L) %&gt;%\n  left_join(simulated %&gt;% \n              filter(visit==0L) %&gt;% \n              mutate(base=aval) %&gt;% \n              dplyr::select(patient, base), \n            by=\"patient\") %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\")),\n         aval = ordered(aval, levels=1L:9L),\n         visit = factor(visit, levels=c(12L, 26L, 52L)))\n\nSummarizing the data by treatment group and visit as a bar plot, it appears like the new drug may have a large effect on the outcome of the DBPCFC.\n\nsimulated %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\")),\n         `Tolerated amount\\nof peanut flour` = ordered(aval, levels=c(NA_integer_, 1L:9L),\n                        labels=c( \"&lt; 1 mg\", paste0(c(1, 3, 10, 30, 100, 300, 600, 1000), \" mg\"))),\n         visit = ordered(visit, levels=c(0L,12L,26L,52L),\n                         labels = paste0(\"Visit \", c(0,12,26,52)))) %&gt;%\n  ggplot(aes(x=Treatment, fill=`Tolerated amount\\nof peanut flour`)) +\n  geom_bar(col=\"darkgrey\") +\n  scale_fill_brewer(palette=\"PuBu\", na.value=\"black\") +\n  facet_wrap(~visit, nrow = 1, ncol=4) +\n  theme(legend.position=\"bottom\") +\n  ylab(\"Patients\") +\n  theme(legend.text=element_text(size=rel(0.5)), \n        legend.title = element_text(size=rel(0.5)),\n        legend.key.size = unit(0.2, \"cm\"))\n\n\n\n\n\n\n\n\nOr, as an alternative display as a spaghetti plot (note the handy seed option in position_jitter() to keep the line and point geoms aligned).\n\nsimulated %&gt;%\n  filter(!is.na(aval)) %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\"))) %&gt;%\n  ggplot(aes(x=visit, y=aval, group=patient, color=Treatment)) +\n  geom_line(alpha=0.2, position = position_jitter(width = 0.2, height=0.1, seed = 123)) +\n  geom_point(alpha=0.2, position = position_jitter(width = 0.2, height=0.1, seed = 123)) +\n  scale_y_continuous(breaks=1:9, labels=c( \"&lt; 1 mg\", paste0(c(1, 3, 10, 30, 100, 300, 600, 1000), \" mg\"))) +\n  scale_x_continuous(breaks=c(0,12,26,52)) +\n  facet_wrap(~Treatment) +\n  xlab(\"Time since randomization (weeks)\") +\n  ylab(\"Tolerated amount of peanut flour\")\n\n\n\n\n\n\n\n\nExcercises in increasing order of complexity (* marks advanced questions):\n\nFit a simple logistic regression (i.e. glm(formula = response ~ Treatment, family=binomial(link=\"logit\"))) that uses a composite estimand treating discontinuation of treatment as equivalent to the worst response category (not tolerating 1 mg of peanut flour, i.e. model_data %&gt;% filter(visit == 52L) %&gt;% mutate(aval = ifelse(is.na(aval), 1L, aval), response = 1L*(aval &gt;= 8L))). Does this support the alternative hypothesis that the new drug is better than placebo? What if we change the null hypothesis to the difference in response rate being less than 10 percentage points higher than the placebo response rate (and the alternative hypothesis to it being 10 or more percentage points better than placebo)?\nCompare the results to imputing the ordinal outcome under a treatment policy estimand either based on the observed post-treatment data for patients that discontinued treatemnt. For this, you could use family=cumulative() and formula = aval ~ 1 + (1|patient) + visit*Treatment*discontinued*mo(base). Note that mo(base) implies a monotonic effect of the baseline DBPCFC outcome being higher without imposing a linear relationship. Allowing for all of the interactions we are specifying and the adjustment for baseline are examples of making an imputation model more complex than our analysis model. Perform, say, 100 imputations, perform the analysis for each imputation and combine estimates of the log-odds-ratio.\n\nIn this case, do we appear to gain much for the purposes of showing that the drug works from the more complex imputation approach?\nDo you think the answer to this would change, if half of the discontinuation occurred due to the COVID-19 pandemic and we imputed post-treatment data for these patients under a hypothetical estimand (assuming patients would continue to take treatment to the end of the study)?\n\nHow would you target the treatment policy estimand using the jump-to-reference approach? What would your imputation model look like and for which records would you perform the imputation?\n* Would a linear model for log-tolerated amount of peanut flour fit the data better than the ordinal regression model we used to fit the data? For this, treat not tolerating 1 mg as being right-censored below 1 mg (brms allows you to use the aval | cens(censored) ~ regression formula syntax, where the variable censored would contain the values 'none' for uncensored observations and 'right' for right-censored observations).\n* Perform similar analyses as above (1-3), but use an ordinal regression model e.g. via MASS::polr or rms::orm.\n\nFurther reading: Several blogposts by Frank Harrell cover proportional odds models (e.g. 1, 2, 3 and 4).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html",
    "href": "src/02g_longitudinal.html",
    "title": "12  Longitudinal data",
    "section": "",
    "text": "12.1 Background\nMany clinical trials assess efficacy and other endpoints at numerous timepoints post-baseline. For example, consider the case where a continuous response endpoint is assessed every 2 weeks post baseline (Week 2, Week 4, …, Week 12). Although clinical interest may focus on the response rate at a particular visit (e.g. at Week 12), efficacy data at other visits is of course still of interest, either in its own right, to build understanding of the full time vs. efficacy profile, or as a means to increase statistical power for estimation at Week 12.\nThere are multiple approaches for proceeding with estimation of the Week 12 treatment effect. Adopting a cross-sectional approach , one could ignoring all other post-baseline assessments except Week 12. If certain patients missed the Week 12 assessment, this missing data would need to be handled using an appropriate pre-defined strategy (e.g. for response rate endpoints, imputing missing outcomes with nonresponder status, carrying the last observation forward, multiple imputation approaches, or even dropping missing data altogether). Such approaches may lose signifcant power or incur bias if there is substantial missing data.\nAlternatively longitudinal models incorporate the full post-baseline set of assessments to facilitate modeling of efficacy at all visits. In some cases, these may substantially increase statistical power.\nThe brms package contains a deep suite of modelling tools for longitudinal data, including many options for modelling the mean trajectory across visits/time, and for modelling autocorrelated errors within patients across time. In this section we illustrate some potential uses of these tools in a clinical trial setting.\nlibrary(dplyr)\nlibrary(brms)\nlibrary(emmeans)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8904568)\ncontrol_args &lt;- list(adapt_delta = 0.95)\nadpasi &lt;- readr::read_csv(here::here(\"data\", \"longitudinal.csv\"), show_col_types = FALSE) %&gt;%\n  dplyr::filter(TRT01P %in% c(\"PBO\", \"TRT\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\npasi_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#data",
    "href": "src/02g_longitudinal.html#data",
    "title": "12  Longitudinal data",
    "section": "12.2 Data",
    "text": "12.2 Data\nThe example involves simulated results of a hypothetical Phase-II study of an experimental treatment for Psoriasis. Synthetic data are generated using the mmrm package. We consider a subset of the study involving 100 patients, 50 of whom were randomized to receive placebo, and 50 of whom received treatment.\nEfficacy was assessed using the Psoriasis Area and Severity Index (PASI), a numerical score which measures the severity and extent of psoriasis. This is assessed at baseline, and again at 7 post-baseline timepoints.\n\n\n\n\n\n\n\n\n\nTwo endpoints of interest based on the PASI score are (1) PASI change from baseline, and (2) the binary endpoint PASI 75, which defines a responder as any patient with at least a 75% change from baseline in PASI.\n\n\n\n\n\n\n\n\n\nThe data has been transformed to follow a typical CDISC Analysis Data Model (ADaM) format.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#models",
    "href": "src/02g_longitudinal.html#models",
    "title": "12  Longitudinal data",
    "section": "12.3 Models",
    "text": "12.3 Models\nThere are a few key ingredients to a longitudinal model for the PASI score outcomes.\n\nA mean model which describes the expected value of the response over time, across treatments, and across values of any other relevant covariates.\nA correlation model which describes the correlation structure of the error terms.\n\nbrms offers many modelling options for each component.\n\n12.3.1 Mean models\n\n12.3.1.1 Cell-means model\nThe most common approach for modeling the mean in clinical trial practice is to adopt a specification which allows the mean to vary freely across visits, without any parametric specification of the trajectory of the mean over time. Here we will call this the “cell-means” model, where levels of the treatment group and visit comprise the cells.\nIn this approach, we include all treatment-by-visit interactions. In brms, the formula specification would be:\n\n\nCHG ~ BASE + TRT01P * AVISIT\n\n\n\n\n12.3.1.2 Linear-in-time model\nA far stronger assumption would be to assume the mean response is linear in time (i.e. the number of weeks post baseline for visit). In this case, the formula specification would be\n\n\nCHG ~ BASE + TRT01P * AVISITN\n\n\nNote the key difference is the use of AVISITN (a numeric variable indicating the number of weeks post baseline) rather than AVISIT (a factor variable for the visit id).\nSuch a model should be considered only for exploratory modelling purposes, and not for confirmatory analyses, the main reason being that the linearity assumption cannot be assessed at the trial design stage before having collected the data.\nEven if the linearity assumption appeared reasonable over the time range explored in the trial, it should not be used to extrapolate outside the observed time period.\n\n\n12.3.1.3 Quadratic-in-time model\nIn the previous section, we saw there was a hint of curvature in the sample mean response over time:\n\n\n\n\n\n\n\n\n\nHence one might consider a mean model that assumes the mean response is quadratic in time rather than linear:\n\n\nCHG ~ BASE + TRT01P * AVISITN + TRT01P * AVISITN^2\n\n\nThis model should similarly not be used to extrapolate outside the week 1-12 window.\n\n\n12.3.1.4 Gaussian process prior\nAn interesting nonparametric alternative to the just-discussed specifications is based on a Gaussian process prior for the mean across visits within treatments. As in the case of the cell-means model, this model does not assume any parametric shape for the mean response over time. It assumes only that the mean response over time follows a continuous curve which is assigned a Gaussian process (GP) prior.\nWhile the GP itself is a continuous-time process, the joint distribution of a realization at any discrete set of timepoints is multivariate normal.\nIn brms, the formula specification would be as follows:\n\n\nCHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P)\n\n\nThe implication of this model is that there is a correlation between the mean response at any collection of visits, and the correlation between any pair of visits decays as the time between them increases, according to an exponential covariance function. See ?gp for additional details on the brms implementation.\n\n\n\n12.3.2 Correlation models\nThe repeated measurement of an endpoint over time on the same patient induces autocorrelation between the within-patient measurements. To illustrate, consider if we ignored the correlations and fit a model with uncorrelated errors. Below are scatterplots and correlation coefficients for the residuals of such a fitted model. The correlations are quite strong, especially between visits that are closer in time.\n\npasi_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\")\nlm_fit &lt;- lm(CHG ~ BASE + TRT01P * AVISIT, data = pasi_data)\nres &lt;- residuals(lm_fit)\npasi_data %&gt;%\n  mutate(res = res) %&gt;%\n  select(SUBJID, AVISIT, res) %&gt;%\n  pivot_wider(id_cols = SUBJID, names_from = AVISIT, values_from = res) %&gt;%\n  select(-SUBJID) %&gt;%\n  GGally::ggpairs()\n\n\n\n\n\n\n\n\nFailing to model these within-subject correlations will result in a loss of statistical power (wider confidence intervals, less powerful tests). brms offers several options for modelling within-subject correlations.\n\n12.3.2.1 Subject-level random effects\nOne way to achieve within-subject correlation is to use a model that includes a subject-level random effect. When the resulting mean response function is averaged over the distribution of the random effect terms (“integrating them out”), a uniform correlation is induced between all measurements on the same patient. The magnitude of the correlation is determined by the relative size of the random-effect variance and the error variance. (See exercise 1.)\nIn brms, a subject-level random intercept can be easily added in the formula specification. For example:\n\n\nCHG ~ BASE + TRT01P * AVISIT + (1 | SUBJID)\n\n\n\n\n12.3.2.2 Autoregressive correlation structures\nA very common model for serial correlation is based on an autoregressive process. Under such a process, the error term \\(\\varepsilon_t\\) at a timepoint \\(t\\) is explicitly dependent on some collection of preceeding error terms. Under a first-order autoregressive process, for example, it is dependent only on one of its predecessors: \\[ \\varepsilon_t = \\alpha\\varepsilon_{t-1} + Z_t,\\] for \\(t\\geq 2\\), where \\(Z_t\\) are iid \\(\\mathrm N(0, \\sigma^2)\\). The process is initialized with \\(\\varepsilon_1 \\sim \\mathrm N(0, \\sigma^2 / (1 - \\alpha^2))\\).\nThe resulting covariance matrix of a collection \\((\\varepsilon_1,\\ldots,\\varepsilon_T)\\) has a simple form; the reader is referred to the SAS paper “Guidelines for Selecting the Covariance Structure in Mixed Model Analysis” by Chuck Kincaid for detail on the AR structure (c.f. page 2), and other covariance structures.\nThe autocor argument of brm() and brmsformula() is used to set an autoregressive correlation strucutre. For order-1 autoregressive. Below is a choice of AR(1) autocorrelation that we might consider in the psoriasis example:\n\n\n~ar(time = AVISIT, gr = SUBJID, p = 1)\n\n\nThis choice implies that autocorrelation exists across visits within subjects.\n\n\n12.3.2.3 Compound symmetry\nAnother choice offered by brms is that of compound symmetry. The reader is again referred to the SAS paper linked above and ?cosy for more information.\n\n\n~cosy(time = AVISIT, gr = SUBJID)\n\n\n\n\n12.3.2.4 Other choices\nbrms offers several other choices for autocorrelation models. See ?'autocor-terms' for more a listing.\nWe note that currently, unstructured correlation models (a standard choice for MMRM specifications in clinical trial protocols) re not supported by brms.\nHowever, fixed correlation structures are, and one could consider plugging in for this an unstructured correlation estimate from a frequentist MMRM. For example,\n\n# fit a MMRM using gls\ngls_fit &lt;- nlme::gls(CHG ~ BASE + TRT01P * AVISIT,\n                     data = pasi_data,\n                     correlation = nlme::corSymm(form = ~ 1 | SUBJID))\n\n# estimated correlation matrices by subject\nSig_subj &lt;- nlme::corMatrix(gls_fit$modelStruct[[1]])\nSig_subj[1]\n\n$`1`\n            [,1]        [,2]      [,3]      [,4]        [,5]        [,6]\n[1,]  1.00000000 0.896714971 0.5883171 0.2669088 -0.03659905 -0.05666876\n[2,]  0.89671497 1.000000000 0.6313654 0.3841036  0.03162521  0.03550198\n[3,]  0.58831712 0.631365431 1.0000000 0.4509002  0.37604603  0.21075194\n[4,]  0.26690877 0.384103637 0.4509002 1.0000000  0.63152510  0.47401182\n[5,] -0.03659905 0.031625213 0.3760460 0.6315251  1.00000000  0.77205541\n[6,] -0.05666876 0.035501982 0.2107519 0.4740118  0.77205541  1.00000000\n[7,] -0.08775131 0.008196384 0.2465146 0.5921762  0.79671624  0.74110361\n             [,7]\n[1,] -0.087751306\n[2,]  0.008196384\n[3,]  0.246514616\n[4,]  0.592176181\n[5,]  0.796716238\n[6,]  0.741103610\n[7,]  1.000000000\n\n\nA block-diagonal matrix containing these estimated within-subject correlations could be plugged in as the M argument of the fcor() function in brms.\n\n\n\n12.3.3 Cross-sectional approaches\nFor modeling a continuous endpoint such as PASI, the cross-sectional analogue of the longitudinal models we’ve discussed is the ANCOVA model, in which a linear regression model (conditioning as before on the baseline PASI covariate) is fit to the Week-12 cross section of data.\n\n# analysis data includes only the Week 12 cross section\nancova_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL, AVISITN == 12)\n\n# formula specification for ANCOVA\nancova_formula &lt;- bf(\n  CHG ~ BASE + TRT01P,\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nancova_prior &lt;- get_prior(\n  ancova_formula,\n  data = ancova_data\n)\n\nancova_fit &lt;- brm(\n  ancova_formula,\n  prior = ancova_prior,\n    data = ancova_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nIn the next section, this model is also explored for the purposes of comparing to the longitudinal approaches",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#results",
    "href": "src/02g_longitudinal.html#results",
    "title": "12  Longitudinal data",
    "section": "12.4 Results",
    "text": "12.4 Results\nA typical estimand for longitudinal or ANCOVA involve Least Squares means (LS means) or estimated marginal means (EMM). Inference for EMMs is extremely convenient with brms due to its integration with the emmeans R package. The EMM can be roughly understood as the average, stratified by treatment group and visit, of the mean prediction across subjects in the trial.\nWe begin by briefly illustrating how emmeans can be used with a brmsfit object to estimate LS means and their contrasts.\n\nanalysis_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL)\n\nbrms_formula &lt;- bf(\n  CHG ~ BASE + TRT01P * AVISIT,\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nprior &lt;- get_prior(\n  brms_formula,\n  data = analysis_data\n)\n\nfit &lt;- brm(\n  brms_formula,\n  prior = prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 12.1 seconds.\nChain 2 finished in 11.9 seconds.\nChain 3 finished in 12.4 seconds.\nChain 4 finished in 11.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.9 seconds.\nTotal execution time: 48.1 seconds.\n\nfit\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CHG ~ BASE + TRT01P * AVISIT \n         autocor ~ cosy(time = AVISIT, gr = SUBJID)\n   Data: analysis_data (Number of observations: 684) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncosy     0.41      0.04     0.33     0.51 1.00     3163     2356\n\nRegression Coefficients:\n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                 12.95      1.87     9.29    16.56 1.00     2673\nBASE                      -0.70      0.08    -0.85    -0.54 1.00     3934\nTRT01PTRT                 -3.56      1.59    -6.62    -0.45 1.00     1884\nAVISITWeek2               -0.16      1.16    -2.47     2.11 1.00     2034\nAVISITWeek4               -2.49      1.23    -4.87     0.02 1.00     2158\nAVISITWeek6               -0.87      1.31    -3.46     1.70 1.00     2142\nAVISITWeek8                1.10      1.34    -1.55     3.77 1.00     2250\nAVISITWeek10              -1.22      1.32    -3.78     1.43 1.00     2184\nAVISITWeek12              -3.66      1.17    -6.01    -1.41 1.00     2064\nTRT01PTRT:AVISITWeek2     -4.13      1.75    -7.59    -0.71 1.00     1942\nTRT01PTRT:AVISITWeek4     -6.15      1.79    -9.73    -2.59 1.00     2173\nTRT01PTRT:AVISITWeek6     -9.30      1.83   -12.89    -5.67 1.00     1916\nTRT01PTRT:AVISITWeek8    -12.69      1.85   -16.38    -9.16 1.00     1981\nTRT01PTRT:AVISITWeek10   -10.77      1.89   -14.41    -7.06 1.00     2080\nTRT01PTRT:AVISITWeek12    -7.38      1.73   -10.82    -4.09 1.00     1890\n                       Tail_ESS\nIntercept                  2488\nBASE                       3210\nTRT01PTRT                  2654\nAVISITWeek2                2992\nAVISITWeek4                2789\nAVISITWeek6                2597\nAVISITWeek8                2734\nAVISITWeek10               2506\nAVISITWeek12               2704\nTRT01PTRT:AVISITWeek2      2370\nTRT01PTRT:AVISITWeek4      2821\nTRT01PTRT:AVISITWeek6      2290\nTRT01PTRT:AVISITWeek8      2465\nTRT01PTRT:AVISITWeek10     2343\nTRT01PTRT:AVISITWeek12     2542\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.21      0.32     7.65     8.89 1.00     3278     2296\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# estimate the EMMs\nemm &lt;- emmeans(fit, c(\"TRT01P\", \"AVISIT\"), nesting = list())\nemm\n\n TRT01P AVISIT   emmean lower.HPD upper.HPD\n PBO    Week 1   -1.214     -3.38    0.8924\n TRT    Week 1   -4.782     -6.98   -2.3953\n PBO    Week 2   -1.378     -3.45    0.7517\n TRT    Week 2   -9.065    -11.30   -6.7377\n PBO    Week 4   -3.718     -6.02   -1.4343\n TRT    Week 4  -13.426    -15.69  -11.3379\n PBO    Week 6   -2.099     -4.40    0.3942\n TRT    Week 6  -14.976    -17.23  -12.6760\n PBO    Week 8   -0.144     -2.63    2.2711\n TRT    Week 8  -16.363    -18.77  -14.1551\n PBO    Week 10  -2.449     -4.94    0.0976\n TRT    Week 10 -16.780    -19.11  -14.3734\n PBO    Week 12  -4.874     -7.17   -2.7912\n TRT    Week 12 -15.795    -18.10  -13.5977\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n# estimate EMM contrasts\nemm_contrasts &lt;- contrast(emm, method = \"revpairwise\", by = \"AVISIT\")\nemm_contrasts\n\nAVISIT = Week 1:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -3.58     -6.53    -0.391\n\nAVISIT = Week 2:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -7.67    -10.81    -4.698\n\nAVISIT = Week 4:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -9.70    -12.87    -6.692\n\nAVISIT = Week 6:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -12.88    -16.12    -9.505\n\nAVISIT = Week 8:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -16.24    -19.47   -12.831\n\nAVISIT = Week 10:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -14.33    -17.57   -10.757\n\nAVISIT = Week 12:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -10.93    -14.22    -8.029\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n12.4.1 Fitting several models\nIn order to understand the impact of possible choices for the mean model and correlation model, we fit a series of models in loop.\nCode to prepare a list of models to be fit:\n\n\nCode\nsetup_analyses &lt;- function(adpasi){\n  \n  formulas &lt;- tribble(\n    ~endpoint,   ~formula_name,   ~paramcd,     ~family,                                             ~formula, ~correl, ~longitudinal,\n       \"PASI\",    \"cell-means\", \"PASITSCO\",  gaussian(),                         CHG ~ BASE + TRT01P * AVISIT,    TRUE,          TRUE,\n       \"PASI\",        \"linear\", \"PASITSCO\",  gaussian(),                        CHG ~ BASE + TRT01P * AVISITN,    TRUE,          TRUE,\n       \"PASI\",     \"quadratic\", \"PASITSCO\",  gaussian(), CHG ~ BASE + TRT01P * AVISITN + TRT01P * AVISITN ^ 2,    TRUE,          TRUE,\n       \"PASI\",            \"gp\", \"PASITSCO\",  gaussian(),       CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),    TRUE,          TRUE,\n       \"PASI\",        \"raneff\", \"PASITSCO\",  gaussian(),          CHG ~ BASE + TRT01P * AVISIT + (1 | SUBJID),   FALSE,          TRUE,\n       \"PASI\", \"cross-section\", \"PASITSCO\",  gaussian(),                                  CHG ~ BASE + TRT01P,   FALSE,         FALSE\n  )\n  \n  datasets &lt;- tribble(\n      ~paramcd, ~longitudinal, ~missing_approach,                                               ~analysis_data,\n    \"PASITSCO\",          TRUE,            \"drop\",                filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL),\n    \"PASITSCO\",         FALSE,            \"drop\", filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL, AVISITN == 12)\n  )\n  \n  autocor &lt;- tribble(\n    ~correl, ~autocor_name, ~autocor,\n       TRUE,         \"AR1\", ~ ar(time = AVISIT, gr = SUBJID, p = 1),\n       TRUE,        \"COSY\", ~ cosy(time = AVISIT, gr = SUBJID)\n  )\n  \n  analyses &lt;- formulas %&gt;%\n    full_join(datasets, c(\"paramcd\", \"longitudinal\"), multiple = \"all\") %&gt;%\n    full_join(autocor, c(\"correl\"), multiple = \"all\") %&gt;%\n    replace_na(list(autocor_name = \"none\"))\n  \n  analyses\n  \n}\n\n\nCode to fit the models in a loop using clustermq:\n\n\nCode\nhere::i_am(\"src/longitudinal/fit_models.R\")\nlibrary(dplyr)\nlibrary(brms)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(here)\nlibrary(emmeans)\nlibrary(clustermq)\nlibrary(purrr)\n\nsource(here(\"src\", \"longitudinal\", \"setup_analyses.R\"))\n\n# iter &lt;- Sys.getenv(\"BRMS_ITER\")\n# chains &lt;- Sys.getenv(\"BRMS_CHAINS\")\n\nadpasi &lt;- readr::read_csv(here(\"data\", \"longitudinal.csv\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\nanalyses &lt;- setup_analyses(adpasi)\nanalyses &lt;- filter(analyses, endpoint == \"PASI\") %&gt;%\n  mutate(id = 1:n())\n\nfit_model &lt;- function(id, analysis_data, formula, formula_name,\n                      family, correl, autocor, autocor_name,\n                      save_individuals = FALSE){\n  \n  \n  # set up the formula ---------------------------------------------------------\n  \n  if(correl){\n    autocor &lt;- autocor\n  } else{\n    autocor &lt;- NULL\n  }\n    \n  brms_formula &lt;- bf(\n    formula,\n    autocor = autocor,\n    family = family,\n    center = FALSE,\n    nl = FALSE\n  )\n  \n  # fit the model --------------------------------------------------------------\n  \n  prior &lt;- get_prior(\n    brms_formula,\n    data = analysis_data\n  )\n  \n  cat(\"\\n*** Fitting model\", id, \": \", formula_name, \", \", autocor_name, \"***\\n\")\n  \n  fit &lt;- brm(\n    brms_formula,\n    prior = prior,\n    cores = 4,\n    backend = \"rstan\",\n    data = analysis_data\n  )\n  \n  if(save_individuals) saveRDS(fit, here::here(\"reports\", paste0(\"longitudinal_fit_\", id, \".rds\")))\n  \n  # estimate marginal means ----------------------------------------------------\n  \n  cat(\"\\n*** Estimating marginal means\", id, \": \", formula_name, \", \", autocor_name, \"***\\n\")\n  \n  base &lt;- analysis_data %&gt;%\n    select(SUBJID, BASE) %&gt;%\n    distinct()\n  \n  visits &lt;- analysis_data %&gt;%\n    select(AVISIT, AVISITN) %&gt;%\n    distinct()\n  \n  treatments &lt;- analysis_data %&gt;%\n    select(TRT01P) %&gt;%\n    distinct()\n  \n  emm &lt;- bind_rows(lapply(\n    split(visits, 1:nrow(visits)),\n    function(visit){\n      \n      nd &lt;- visit %&gt;%\n        crossing(treatments) %&gt;%\n        crossing(base) %&gt;%\n        split(.$TRT01P)\n      \n      lp &lt;- map(nd, ~ posterior_linpred(fit, transform = TRUE, newdata = .))\n      \n      marginal_mean &lt;- lapply(lp, rowMeans) %&gt;%\n        as_tibble() %&gt;%\n        mutate(diff = .[[2]] - .[[1]]) %&gt;%\n        setNames(c(names(nd),\n                   paste(rev(names(nd)), collapse = \" - \")))\n      \n      bind_cols(\n        visit,\n        summarise_draws(as_draws_df(marginal_mean))\n      ) %&gt;%\n        mutate(\n          TRT01P = factor(variable, levels(analysis_data$TRT01P)),\n          contrast = case_when(\n            is.na(TRT01P) ~ variable,\n            TRUE ~ NA_character_\n          )\n        )\n      \n    }\n  ))\n  \n  if(save_individuals) saveRDS(emm, here::here(\"reports\", paste0(\"longitudinal_emm\", id, \".rds\")))\n  \n  return(emm)\n  \n}\n\nemm &lt;- clustermq::Q_rows(\n  select(analyses, id, analysis_data, formula, formula_name, family, correl, autocor, autocor_name),\n  fit_model,\n  const = list(save_individuals = FALSE),\n  n_jobs = nrow(analyses),\n  pkgs = c(\"brms\", \"emmeans\", \"tidyr\", \"dplyr\", \"purrr\", \"posterior\"),\n  template = list(\n    walltime = 120,\n    job_name = \"longitudinal_child\",\n    log_file = here(\"reports\", \"longitudinal_child_%I.log\"),\n    memory = 3000,\n    cores = 4\n  ),\n  job_size = 1\n)\n\nsaveRDS(emm, file = here(\"reports\", \"longitudinal_fits.rds\"))\nsaveRDS(analyses, file = here(\"reports\", \"longitudinal_analyses.rds\"))\n\nanalyses$emm &lt;- emm\n\n\n\n\n12.4.2 PASI change from baseline: EMMs by visit\n\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\n\n12.4.3 PASI change from baseline: EMM contrasts with placebo by visit\n\ncontrast_results &lt;- analyses %&gt;%\n  select(formula_name, autocor_name, emm) %&gt;%\n  unnest(emm) %&gt;%\n  dplyr::filter(!is.na(contrast)) %&gt;%\n  mutate(formula_name = factor(formula_name, unique(formula_name)),\n         autocor_name = factor(autocor_name, unique(autocor_name)),\n         contrast = factor(contrast, unique(contrast))) %&gt;%\n  rename(\n    mean_model = formula_name,\n    corr_model = autocor_name\n  )\n\nblank &lt;- mutate(slice(group_by(contrast_results, mean_model, corr_model), 1), estimate = 0)\n\nggplot(\n  data = contrast_results,\n  mapping = aes(x = AVISIT, group = mean_model, color = mean_model,\n                y = median, ymin = q5, ymax = q95)\n) +\n  geom_pointrange(position = position_dodge(0.5)) +\n  geom_path(position = position_dodge(0.5)) +\n  labs(x = \"Visit\", y = \"PASI change from baseline\\nTreatment - Control\\nDifference in Estimated Marginal Means\") +\n  geom_blank(data = blank) +\n  facet_grid(. ~ corr_model, labeller = label_both) +\n  scale_x_discrete(labels = levels(pasi_data$AVISIT)) +\n  scale_color_discrete(\"Mean model\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n12.4.4 Estimates of Week 12 efficacy\n\ncontrast_results %&gt;%\n  dplyr::filter(AVISITN == 12,\n         !mean_model %in% c(\"linear\", \"quadratic\"),\n         corr_model %in% c(\"COSY\", \"none\")) %&gt;%\n  mutate(method = paste(\"mean model:\", mean_model, \"\\ncorrelation model:\", corr_model)) %&gt;%\n  ggplot(aes(x = method, y = median, ymin = q5, ymax = q95)) +\n  geom_pointrange(position = position_dodge(0.6)) +\n  labs(x = \"Modeling approach\",\n       y = \"PASI change from baseline\\nTreatment - Control\\nDifference in Estimated Marginal Means\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n12.4.5 Assessing model fit\nIn this section we illustrate the use of side-by-side posterior predictive checks to compare the fit of two models: one with a GP prior for the mean structure and one with a linear-in-time model for the mean, each with compound-symmetric autocorration models.\nThe checks are done across timepoint (# of weeks since baseline), by treatment group and quartile of the distribution of baseline PASI.\n\n# Gaussian process model -------------------------------------\ngp_formula &lt;- bf(\n  CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\ngp_prior &lt;- get_prior(\n  gp_formula,\n  data = analysis_data\n)\n\ngp_fit &lt;- brm(\n  gp_formula,\n  prior = gp_prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 34.5 seconds.\nChain 2 finished in 38.1 seconds.\nChain 3 finished in 35.5 seconds.\nChain 4 finished in 38.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 36.6 seconds.\nTotal execution time: 146.9 seconds.\n\n\nWarning: 10 of 4000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n# Linear mean model -------------------------------------\nlm_formula &lt;- bf(\n  CHG ~ BASE + TRT01P * AVISITN,\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nlm_prior &lt;- get_prior(\n  lm_formula,\n  data = analysis_data\n)\n\nlm_fit &lt;- brm(\n  lm_formula,\n  prior = lm_prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 7.0 seconds.\nChain 2 finished in 7.6 seconds.\nChain 3 finished in 6.9 seconds.\nChain 4 finished in 7.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 7.2 seconds.\nTotal execution time: 29.1 seconds.\n\nbase_quartiles &lt;- pasi_data %&gt;%\n  select(SUBJID, BASE) %&gt;%\n  distinct() %&gt;%\n  pull(BASE) %&gt;%\n  quantile(c(0, 0.25, 0.5, 0.75, 1))\n\nquartile_center &lt;- (base_quartiles[1:4] + base_quartiles[2:5]) / 2\n\nfull_newdata &lt;- pasi_data %&gt;%\n  select(SUBJID, BASE) %&gt;%\n  distinct() %&gt;%\n  mutate(base_catn = as.numeric(cut(BASE, base_quartiles)),\n         base_cat = paste(\"Baseline PASI quartile\", base_catn),\n         BASE = quartile_center[base_catn])\n\npp_checks &lt;- lapply(\n  split(full_newdata, full_newdata$base_catn),\n  function(nd){\n    \n    p1 &lt;- brms::pp_check(gp_fit, type = \"ribbon_grouped\", group = \"TRT01P\", x = \"AVISITN\",\n                         newdata = inner_join(nd, select(pasi_data, -BASE), \"SUBJID\", multiple = \"all\"),\n                         y_draw = \"points\") +\n      labs(x = \"Weeks post baseline\",\n           y = \"Week 12 PASI change from baseline\",\n           title = paste(\"Quartile\", unique(nd$base_catn), \"of Baseline PASI\")) +\n      theme(legend.position = \"bottom\") +\n      ylim(-50, 30)\n    \n    p2 &lt;- brms::pp_check(lm_fit, type = \"ribbon_grouped\", group = \"TRT01P\", x = \"AVISITN\",\n                         newdata = inner_join(nd, select(pasi_data, -BASE), \"SUBJID\", multiple = \"all\"),\n                         y_draw = \"points\") +\n      labs(x = \"Weeks post baseline\",\n           y = \"Week 12 PASI change from baseline\",\n           title = paste(\"Quartile\", unique(nd$base_catn), \"of Baseline PASI\")) +\n      theme(legend.position = \"bottom\") +\n      ylim(-50, 30)\n    \n    list(\n      gp = p1,\n      lm = p2\n    )\n    \n  }\n)\n\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\n\n\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\n\n\nNow we visualize the posterior predictive distribution against the observed data, first for the Gaussian process model:\n\n\nCode\npp_checks[[1]]$gp / pp_checks[[2]]$gp / pp_checks[[3]]$gp / pp_checks[[4]]$gp\n\n\n\n\n\n\n\n\n\nand next for the linear-in-time model:\n\n\nCode\npp_checks[[1]]$lm / pp_checks[[2]]$lm / pp_checks[[3]]$lm / pp_checks[[4]]$lm\n\n\n\n\n\n\n\n\n\nAnother useful visualization for assessing model involves approximate leave-one-out (LOO) cross-validation techniques.\nOne can numerically compare the approximate expected log predictive density (ELPD) for holdout observations using loo_compare.\n\nloo_gp &lt;- loo(gp_fit)\nloo_lm &lt;- loo(lm_fit)\nloo_compare(loo_gp, loo_lm)\n\n       elpd_diff se_diff\ngp_fit   0.0       0.0  \nlm_fit -20.8       5.2  \n\n\nThis suggests the Gaussian-process based model has a superior model fit.\nTo visualize the predictive densities versus observed values, pp_check() with type \"loo_pit\". In these visualizations, the observed outcomes are compared with their respective leave-one-out predictive distributions using the probability integral transformation (PIT). In an ideal model fit, the resulting PIT-transformed values would be uniformly distributed: i.e. the blue points and dashed lines would align with the distribution function of a Uniform(0,1) variable (solid line).\n\ngp_loo &lt;- pp_check(gp_fit, type = \"loo_pit\") + geom_abline(intercept = 0, slope = 1) + ggtitle(\"LOO PIT: GP model\")\nlm_loo &lt;- pp_check(lm_fit, type = \"loo_pit\") + geom_abline(intercept = 0, slope = 1) + ggtitle(\"LOO PIT: linear-in-time model\")\ngp_loo + lm_loo",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#exercises",
    "href": "src/02g_longitudinal.html#exercises",
    "title": "12  Longitudinal data",
    "section": "12.5 Exercises",
    "text": "12.5 Exercises\n\nUsing brms, fit a cross-sectional (non longitudinal) logistic regression model to the binary PASI 75 endpoint at Week 12. The only covariate term should be the treatment effect. Use the following analysis data (which uses only the Week 12 outcomes, and uses nonresponder imputation for any missing outcomes at Week 12). Use the emmeans package and function to estimate the marginal mean Week 12 response rates by treatment. Use emmeans::contrast to estimate the treatment-vs-control contrasts in response rates.\n\n\nadpasi &lt;- readr::read_csv(here(\"data\", \"longitudinal.csv\"), show_col_types = FALSE) %&gt;%\n  dplyr::filter(TRT01P %in% c(\"PBO\", \"TRT\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\nanalysis_data1 &lt;- filter(adpasi, PARAMCD == \"PSRS75\", NRFL, AVISIT == \"Week 12\")\n\n\n\nCode\n# solution\nfit1 &lt;- brm(AVAL ~ TRT01P, family = bernoulli(), data = analysis_data1,\n            silent = 2, refresh = 0)\nemm1 &lt;- emmeans(fit1, c(\"TRT01P\"), transform = \"response\")\nemm1\ncontrast(emm1, method = \"revpairwise\")\n\n\n\nFit a longitudinal model to the binary PASI 75 endpoint. Use a Gaussian process prior (stratified by treatment arm) for the mean across weeks and an AR(1) process to model autocorrelation in the residuals. Use the following analysis data (in which any missing assessments are not imputed). Use the emmeans package and function to estimate the marginal mean response rates by treatment and visit. Use emmeans::contrast to estimate the treatment-vs-control contrasts in response rates by visit. How does the inference for Week 12 response rate difference compare to the cross-sectional model fit from 1?\n\n\nanalysis_data2 &lt;- filter(adpasi, PARAMCD == \"PSRS75\", !MISSFL)\n\n\n\nCode\n# solution\nfit2 &lt;- brm(\n  bf(\n    AVAL ~ TRT01P + gp(AVISITN, by = TRT01P),\n    autocor = ~ cosy(time = AVISIT, gr = SUBJID)\n  ),\n  data = analysis_data2,\n  silent = 2,\n  refresh = 0\n)\nemm2 &lt;- emmeans(fit2, c(\"TRT01P\", \"AVISITN\"), cov.keep = c(\"TRT01P\", \"AVISITN\"), nesting = list(\"AVISTN\" = \"AVISIT\"), transform = \"response\")\nemm2\ncontrast(emm2, method = \"revpairwise\", by = \"AVISITN\")\n\n\n\nUse Leave-One-Out cross validation to compare the model fits to the observed Week 12 data. (Hint: use loo(fit, newdata = newdata) with the below choice of newdata, then use loo_compare). Which model has better predictive power?\n\n\nnewdata &lt;- filter(adpasi, PARAMCD == \"PSRS75\", !MISSFL, AVISIT == \"Week 12\")\n\n\n\nCode\n# solution\nloo1 &lt;- loo(fit1, newdata = newdata)\nloo2 &lt;- loo(fit2, newdata = newdata)\nloo_compare(loo1, loo2)\n\n\n\n(Advanced) In this exercise, we will use a model fit to the continuous PASI change from baseline to do inference on the binary 75 response rate. First, fit a GP-based model to the PASI endpoint using the following code:\n\n\npasi_fit &lt;- brm(\n  bf(\n    CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),\n    autocor = ~ cosy(time = AVISIT, gr = SUBJID)\n  ),\n  data = pasi_data,\n  family = gaussian()\n)\n\nNext, create two sets of covariate values that includes “counterfactuals” for each patient as if they received both treatment and control. The following code is convenient:\n\nx_treatment &lt;- mutate(filter(pasi_data, AVISIT == \"Week 12\"),\n                      TRT01P = factor(\"TRT\", levels(pasi_data$TRT01P)))\nx_control &lt;- mutate(filter(pasi_data, AVISIT == \"Week 12\"),\n                    TRT01P = factor(\"PBO\", levels(pasi_data$TRT01P)))\n\nNext, for each of x_treatment and x_control:\n\nuse posterior_predict using the newdata argument to sample from the posterior predictive distribution for PASI change from baseline at each level x_treatment and x_control, respectively. The result of posterior_predict whose columns are posterior predictions of PASI change from baseline for individual patients.\nDivide these predictions by the baseline PASI for the respective patients to obtain predictive draws for PASI % change from baseline.\nConvert the result to binary indicators of PASI % change from baseline being below \\(-75\\%\\).\nFor each MCMC iteration, compute the percentage of responders to obtain a vector of posterior draws for the marginal mean response rates for x_treatment and x_control, respectively.\nHow do the median and credible intervals for the marginal mean response rates compare to the results of emmeans from exercises 1 and 2?\nGraph the posterior density for the difference in marginal mean response rates for treatment minus control.\n\n\n\nCode\n# solution\n\n# predicted change from baseline\npasi_chg_treatment &lt;- posterior_predict(pasi_fit, newdata = x_treatment)\npasi_chg_control &lt;- posterior_predict(pasi_fit, newdata = x_control)\n\n# predicted % change from baseline\npasi_pchg_treatment &lt;- sweep(pasi_chg_treatment, 2, x_treatment$BASE, '/')\npasi_pchg_control &lt;- sweep(pasi_chg_control, 2, x_control$BASE, '/')\n\n# predicted PASI 75 response\npasi_rr_treatment &lt;- pasi_pchg_treatment &lt; -0.75\npasi_rr_control &lt;- pasi_pchg_control &lt; -0.75\n\n# marginal PASI 75 response rates\nmarginal_rr_treatment &lt;- rowMeans(pasi_rr_treatment)\nmarginal_rr_control &lt;- rowMeans(pasi_rr_control)\n\n# How do the median and credible intervals compare to emm1 and emm2?\napply(cbind(trt = marginal_rr_treatment,\n            ctrl = marginal_rr_control), 2, median)\ncoda::HPDinterval(coda::as.mcmc(cbind(trt = marginal_rr_treatment,\n                                      ctrl = marginal_rr_control)))\nemm1\nemm2\n\n# visualize posterior distribution for marginal mean response rates by treatment\nqplot(\n  x = cbind(marginal_rr_control, marginal_rr_treatment),\n  group = cbind(rep(\"control\", length(marginal_rr_treatment)),\n                rep(\"treatment\", length(marginal_rr_treatment))),\n  color = cbind(rep(\"control\", length(marginal_rr_treatment)),\n                rep(\"treatment\", length(marginal_rr_treatment))),\n  geom = \"density\"\n) + scale_color_discrete(NULL) + theme(legend.position = \"bottom\") +\n  labs(x = \"Marginal mean response rate\",\n       y = \"Posterior density\")\n\n# visualize posterior distribution for difference in response rates\nqplot(\n  x = marginal_rr_treatment - marginal_rr_control,\n  geom = \"density\"\n) + \n  labs(x = \"Difference in marginal mean response rate\",\n       y = \"Posterior density\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html",
    "href": "src/02h_mmrm.html",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "",
    "text": "13.1 Background\nIn randomized controlled clinical trials efficacy variables are often measured at multiple points of time. This may be at visits to the trial site for assessments that require a patient to be in the investigator’s office, but could also be at a patient’s home (e.g. for a daily quality of life questionnaire). Multiple assessments over time are useful for a wide variety of reasons. Firstly, we get information about how the difference between treatment groups develops over time. I.e. about both the onset of action, as well as whether treatment effects disappear or decline at some point (e.g. after the discontinuation of treatment either by some patients during the treatment period or by all patients during a planned post-treatment follow-up period). Secondly, seeing consistent data over time provides additional evidence for the presence of an effect of an intervention. Thirdly, pre-treatment (baseline) assessment(s) of a variable are often used as a covariate in analyses, because this tends to reduce unexplained variability leading to smaller standard errors. Finally, data from post-baseline visits can help us deal with missing data or data that are not relevant for our estimand of interest.\nSometimes, we would be primarily interested in the treatment difference at one particular visit (e.g. the final visit at the end of the planned treatment period) or we might want to combine (e.g. average) treatment effects across multiple visits.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#data",
    "href": "src/02h_mmrm.html#data",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.2 Data",
    "text": "13.2 Data\nWe will simulate data from a hypothetical parallel group RCT that tests three doses of a drug (10, 20 or 40 mg once daily) compared with a placebo (0 mg once daily). The endpoint of interest is continuous and assessed at a baseline visit, as well as at 4 post-baseline visits (week 2, 4, 8 and 12). 200 patients are randomly assigned to each treatment group (approximately 50 patients per arm).\nWe simulate data with the following covariance matrix:\n\n# Correlation matrix between visits (baseline + 4 post-baseline visits)\ncorr_matrix &lt;- diag(5)\nrho &lt;- c(0.6, 0.48, 0.4, 0.375)\ncorr_matrix[1,2:5] &lt;- rho[1:4]\ncorr_matrix[2,3:5] &lt;- rho[1:3]\ncorr_matrix[3,4:5] &lt;- rho[1:2]\ncorr_matrix[4,5:5] &lt;- rho[1:1]\ncorr_matrix[lower.tri(corr_matrix)] &lt;- t(corr_matrix)[lower.tri(corr_matrix)]\n \n# Standard deviations by visit (baseline + 4 post-baseline visits)\nsds &lt;- sqrt(c(0.75, 0.8, 0.85, 0.95, 1.1))\n\ncov_matrix &lt;- diag(sds) %*% corr_matrix %*% diag(sds)\nprint(cov_matrix, digits=3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.750 0.465 0.383 0.338 0.341\n[2,] 0.465 0.800 0.495 0.418 0.375\n[3,] 0.383 0.495 0.850 0.539 0.464\n[4,] 0.338 0.418 0.539 0.950 0.613\n[5,] 0.341 0.375 0.464 0.613 1.100\n\n\nIn our simulation some patients stop treatment before the end of the trial and actually drop out of the study. We no longer follow them, because we are interested in a hypothetical estimand as if they had stayed on drug, which means we are no longer interested in values after treatment discontinuation.\n\n\nShow the code\n# Simulate from multivariate normal for control group \n# (before adding treatment effect later)\n# We simulate 1000 patients and then apply inclusion criteria and keep the\n# first 200 that meet them.\nN &lt;- 1000\nNf &lt;- 200\nif(use_small_N) {\n    Nf &lt;- 50\n}\neffect_course &lt;- function(dose, time, ed50=8, et50=3) {\n    0.9 * dose /(dose + ed50) * time^3 /(time^3 + et50^3)\n}\nzero &lt;- setNames(rep(0, 5), c(\"BASE\", paste0(\"visit\", 1:4)))\nsimulated_data &lt;- rmvnorm(n = N,\n                          mean = zero,\n                          sigma = cov_matrix) %&gt;%\n    # turn into tibble\n    as_tibble() %&gt;%\n    # Apply inclusion criteria and keep first Nf patients\n    filter(BASE&gt;0) %&gt;%\n    filter(row_number()&lt;=Nf) %&gt;%\n    # Assign subject ID, treatment group and create missing data\n    mutate(USUBJID = row_number(),\n           TRT01P = dqsample(x=c(0L, 10L, 20L, 40L), size=Nf, replace=T),\n           # Simulate dropouts\n           dropp2 = plogis(visit1-2),\n           dropp3 = plogis(visit2-2),\n           dropp4 = plogis(visit3-2),\n           dropv = case_when(runif(n=n())&lt;dropp2 ~ 2L,\n                             runif(n=n())&lt;dropp3 ~ 3L,\n                             runif(n=n())&lt;dropp4 ~ 4L,\n                             TRUE ~ 5L),\n           visit2 = ifelse(dropv&lt;=2L, NA_real_, visit2),\n           visit3 = ifelse(dropv&lt;=3L, NA_real_, visit3),\n           visit4 = ifelse(dropv&lt;=3L, NA_real_, visit4)) %&gt;%\n    dplyr::select(-dropp2, -dropp3, -dropp4, -dropv) %&gt;%\n    # Turn data into long-format\n    pivot_longer(cols=starts_with(\"visit\"), names_to = \"AVISIT\", values_to=\"AVAL\") %&gt;%\n    mutate(\n        # Assign visit days\n        ADY = case_when(AVISIT==\"visit1\" ~ 2L*7L,\n                        AVISIT==\"visit2\" ~ 4L*7L,\n                        AVISIT==\"visit3\" ~ 8L*7L,\n                        AVISIT==\"visit4\" ~ 12L*7L),\n        # Turn to factor with defined order of visits\n        AVISIT = factor(AVISIT, paste0(\"visit\", 1:4)),\n        # Assume rising treatment effect over time (half there by week 3) with an \n        # Emax dose response (ED50 = 5 mg)\n        AVAL = AVAL + effect_course(ADY/7, TRT01P),\n        # Change from baseline = value - baseline\n        CHG = AVAL - BASE,\n        TRT01P=factor(TRT01P)) %&gt;%\n    relocate(USUBJID, TRT01P, AVISIT, ADY, AVAL, CHG, BASE) %&gt;%\n    # Discard missing data\n    filter(!is.na(AVAL))\n\n\nThe first 10 rows of the simulated data set are:\n\n\n\n\n\n\n\n\n\nUSUBJID\nTRT01P\nAVISIT\nADY\nAVAL\nCHG\nBASE\n\n\n\n\n1\n1\n0\nvisit1\n14\n1.592\n0.710\n0.882\n\n\n2\n2\n10\nvisit1\n14\n0.739\n0.207\n0.532\n\n\n3\n2\n10\nvisit2\n28\n0.595\n0.062\n0.532\n\n\n4\n3\n40\nvisit1\n14\n1.358\n0.582\n0.776\n\n\n5\n3\n40\nvisit2\n28\n1.175\n0.399\n0.776\n\n\n6\n3\n40\nvisit3\n56\n−0.364\n−1.140\n0.776\n\n\n7\n3\n40\nvisit4\n84\n1.353\n0.577\n0.776\n\n\n8\n4\n20\nvisit1\n14\n0.840\n0.079\n0.761\n\n\n9\n4\n20\nvisit2\n28\n1.920\n1.159\n0.761\n\n\n10\n5\n10\nvisit1\n14\n2.753\n0.946\n1.806\n\n\n11..614\n\n\n\n\n\n\n\n\n\n615\n200\n40\nvisit4\n84\n1.462\n0.808\n0.654\n\n\n\n\n\n\n\nThe assumed true change from baseline relationship in this simulation is:\n\n\n\n\n\n\n\n\n\nThe entire simulated data set with simulated residual noise in a graphical representation looks like:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#model-description",
    "href": "src/02h_mmrm.html#model-description",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.3 Model description",
    "text": "13.3 Model description\n\n13.3.1 The Mixed Model for Repeated Measures (MMRM)\nThe Mixed Model for Repeated Measures (MMRM) is a very popular model for continuous endpoints assessed at multiple visits (or their change from a pre-treatment baseline value). Part of its popularity stems from the fact that it is a flexible model for an outcome measured at different visits that accounts for within patient correlation and can handle missing data without imputation (if you are interested in the right estimand). In particular, it was a major milestone for treating missing data and drop-outs more appropriately than via last-observation-carried-forward (LOCF), which lead to it being recommended by some as a default analysis approach for a hypothetical estimand. Another contributing factor to MMRM’s success in the pharmaceutical industry is that it can be fit using restricted maximum likelihood (REML) with standard software.\nA widely used default analysis is to have the following (fixed effects) model terms\n\nvisit as a factor,\ntreatment as a factor,\ntreatment by visit interaction,\nbaseline (pre-treatment) value of the continuous endpoint as a continuous covariate, and\nvisit by baseline value interaction.\n\nThese terms allow for a flexible time course in the control group, as well as different treatment effects at different visits on top of that. Additionally, adjustment for baseline values usually reduces standard errors in clinical trials. By allowing for a visit by baseline interaction we allow the relationship between the values at each visit and the baseline to differ. This reflects that the longer ago a baseline value was measured, the less it will typically be correlated with future measurements.\nWhere a MMRM differs from a linear model is that the residuals from different visits of the same patient are assumed to be correlated. We do not usually wish to restrict this correlation structure to be of a particular form (such as first-order autoregressive AR(1)), but to allow it to be of an “unstructured” form. This is preferable over simpler structures like first-order autoregressive AR(1), for example. For example, AR(1) assumes a constant correlation between any adjacent visits while raising this correlation to a power for more distant observations. This simple correlation structure may inappropriately model these distant observations for a single patient as almost fully independent resulting in inappropriately small standard errors.\n\n\n13.3.2 Formal specification of MMRM\nFormally, let us assume that there are \\(V\\) visits. We usually assume that the \\(V\\)-dimensional response \\(\\boldsymbol{Y}_i\\) for patient \\(i\\) satisfies \\[\\boldsymbol{Y}_i = \\boldsymbol{X}_i \\boldsymbol{\\beta} + \\boldsymbol{Z}_i \\boldsymbol{b}_i + \\boldsymbol{\\epsilon}_i,\\] where \\(\\boldsymbol{\\beta}\\) is a vector of regression coefficients for fixed effects, the \\(\\boldsymbol{b}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{D})\\) are random effects with the vector \\(\\boldsymbol{Z}_i\\) indicating which one applies for which component (each corresponding to a visit) of the response, and the \\(\\boldsymbol{\\epsilon}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\) are residual errors. Then, \\[\\boldsymbol{Y}_i \\sim \\text{MVN}(\\boldsymbol{X}_i \\boldsymbol{\\beta}, \\boldsymbol{V}_i),\\] where \\[\\boldsymbol{V}_i = \\boldsymbol{Z}_i \\boldsymbol{D} \\boldsymbol{Z}_i^T + \\boldsymbol{\\Sigma}.\\]\nWith the goal to avoid unnnecessary assumptions, the marginal covariance matrix \\(\\boldsymbol{V}_i\\) is usually kept “unstructured”. I.e. we usually want to avoid imposing any particular structure on the covariance matrix. However, an unstructured covariance matrix requires to inform potentially many parameters which grow exponentially fast in the number of visits \\(V\\). Modeling the covariance matrix as a structured correlation matrix and by visit residual error standard deviations may facilitate stabilizing fitting procedures.\n\n\n\n\n\n13.3.3 What estimand does MMRM address?\nMMRM can address a hypothetical estimand about a continuous variable at one or more visits “as if all patients had remained on treatment to the end of the planned treatment period”. This is done by applying MMRM to on-treatment data only and making the assumption that stopping treatment or leaving the study occurs at random conditional on the preceding observations and the covariates being included in the model (“missing at random” or MAR).\nOne key attractive feature is that MMRM estimates this estimand without us needing to directly impute any data. I.e. there is no need for multiple imputation and fitting the model to each imputation, instead we only need to fit MMRM to the on-treatment data under analysis once. However, note that if there are no post-baseline observations for a patient the model will exclude the patient from the analysis, which is a valid thing to do under the MAR assumption.\nNevertheless, it produces identical results as if we had created a large number of multiple imputations, ran the model and then combined the results.\nIf we wish to target different estimands or make different assumptions than implied by the MMRM, we would need to impute prior to fitting a MMRM. If we perform multiple imputation, fitting a MMRM becomes unnecessary, because when all patients have (non-missing) data at all visits that should enter the analysis (i.e. is directly assessing our estimand of interest), then fitting a MMRM (with both treatment group and all baseline covariates have an interaction with visit) is equivalent to separately fitting a linear regression model for each visit.\n\n\n13.3.4 Is MMRM only good for one variable across visits?\nThere is nothing that requires the separate observations being modeled by a MMRM to be measurements of the same variable across time. You can just as appropriately apply MMRM to multiple outcomes measured at the same time, or across multiple occasions. This can be helpful when missingness or data becoming irrelevant to the estimand of interest depends on multiple variables.\nAn example would be a diabetes study, where both fasting plasma glucose (FPG) and glycated hemoglobin (HbA1c) are measured at each visit, and rescue medication is initiated when either of the two is above some threshold. In order to estimate the hypothetical estimand for the HbA1c difference “as if no rescue medication had been taken”, you can jointly model FPG and HbA1c.\n\n\n13.3.5 Should we use the baseline measurement as a covariate or an extra observation?\nFor some patients all post-baseline assessments will be missing, additionally some patients may not have a baseline assessment. In this situation (unless we somehow impute these data), the MMRM model we described cannot include such a patient in the analysis.\nOne potential idea for dealing with this situation is to use the baseline assessment not as a covariate, but as an extra observation. I.e. there is an additional baseline record for each patient, but the baseline term, as well as the baseline by visit interaction terms are removed from the model.\nThis is a reasonable approach, if the residuals across visits are jointly multivariate normal, which we are already assuming for the post-baseline visits. However, this can be a problematic assumption to make when inclusion criteria are applied at baseline that lead to the residuals for the baseline assessment not following a normal distribution. Such an inclusion criterion could be on the variable under analysis itself, or it could be on a variable that is sufficiently strongly correlated with the analysis variable to deform its distribution.\n\n\n\n\n\n\n\n\n\n\n13.3.6 How is MMRM different from a mixed effects model with a random subject effect on the intercept?\nA mixed (random) effects model with a random subject effect on the intercept effectively assumes that the residuals for all visits are equally strongly correlated (“compound symmetric covariance structure”, see next subsection) and also that the standard deviation is the same across visits (although brms would let us avoid this by using distributional regression as discussed later).\n\n\n13.3.7 What covariance structure to use?\nThere are several options for the covariance structures we can use\n\nThe standard deviation will usually go up over time in RCTs. This occurs in part because the trial population was originally forced to be somewhat homogenous at baseline by the trial inclusion criteria. The further we move away from that point in time the more heterogenous outcomes will become across patients. Thus, we at least want to allow the variability to differ between visits.\nData from visits that are closer in time to each other tend to be more highly correlated. Thus, a compound symmetric covariance structure that makes all visits equally correlated will not be realistic, but may sometimes be a reasonable approximation.\nThe correlation between visits of a patient usually never drops to zero even if visits are months or years apart. Generally, assuming a correlation structure that substantially underestimates the correlation between some visits potentially does the most harm. In particular, an AR(1) correlation structure should in practice be avoided as it leads to exponentially fast diminishing correlations between visits.\nIt is most common to use MMRM with a completely unstructured covariance structure across visits and we often even allow it to differ by treatment group. This avoids making (potentially unnecessary) assumptions. It also ensures the equivalence between MMRM and a by-visit-analysis in the absence of missing data.\n\nAs an illustration of what covariance matrices look like in practice, here is the estimated covariance structure reported by Holzhauer et al. 2015 for the fasting plasma glucose (FPG) values from one treatment group of a diabetes trial.\n\n\n\n\n\n\n\n\nExample of a covariance matrix for fasting plasma glucose in diabetic patients\n\n\nVisit\nWeek 0\nWeek 4\nWeek 12\nWeek 16\nWeek 24\nWeek 32\nWeek 40\nWeek 52\n\n\n\n\nWeek 0\n7.68\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 4\n4.00\n6.60\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 12\n3.55\n4.51\n6.52\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 16\n3.03\n3.83\n5.02\n6.90\nNA\nNA\nNA\nNA\n\n\nWeek 24\n3.32\n4.00\n4.29\n4.42\n6.20\nNA\nNA\nNA\n\n\nWeek 32\n3.06\n3.22\n3.72\n4.23\n4.45\n5.73\nNA\nNA\n\n\nWeek 40\n3.60\n3.74\n4.66\n5.10\n4.80\n5.30\n7.80\nNA\n\n\nWeek 52\n3.60\n3.44\n4.48\n4.53\n4.67\n5.16\n6.26\n8.51\n\n\n\n\n\n\n\nIn terms of longer term data, Holzhauer 2014 reported that in a RCT in pre-diabetic patients the correlation matrix for FPG could be well described by the numbers in the table below and that “during the first 3 years of the study, the variance appeared to be relatively constant around 0.68, but there was some indication of an increasing variability in years 3–6.” Unlike the numbers in diabetic patients, these numbers come from a model adjusting for the baseline FPG assessment.\nTo look at another endpoint, let us look at the covariance matrix for glycated hemoglobin A1c (HbA1c) trial in diabetes patients used in Holzhauer et al. 2015. Many patterns seem similar to FPG, but the correlation of adjacent visits is higher, which makes sense, because HbA1c is less subject to day-to-day variations than FPG.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#implementation",
    "href": "src/02h_mmrm.html#implementation",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.4 Implementation",
    "text": "13.4 Implementation\n\n13.4.1 Reference implementations using SAS and lme4\nWithin a frequentist framework, this model is usually estimated using Restricted Maximum Likelihood Estimation (REML). One popular way of fitting such a model is using SAS code similar to the following\n\nPROC MIXED DATA=simulated_data;\n  CLASS TRT01P AVISIT USUBJID;\n  MODEL CHG ~ TRT01P AVISIT BASE TRT01P*AVISIT AVISIT*BASE \n    / SOLUTION DDFM=KR ALPHA = 0.05;\n  REPEATED AVISIT / TYPE=UN SUBJECT = USUBJID R Rcorr GROUP=TRT01P;\n  LSMEANS TRT01P*AVISIT / DIFFS PDIFF CL OM E;\nRUN;\n\nNote that GROUP=TRT01P in the REPEATED statement specifies different covariance structures for each treatment groups. Importantly, DDFM=KR refers to using the method of Kenward and Roger for figuring out what degrees of freedom to use for inference with Student-t-distributions (or in other words to figure out how many independent observations the correlated observations across all our subjects are worth).\nAdditionally the OM option to the LSMEANS statement requests the by-treatment-group least squares means for each visit to be calculated for predicted population margins for the observed covariate distribution in the analysis dataset. On this occasion, it does not make a difference, because there are no categorical covariates in the model.\nIn R we can obtain a very similar frequentist fit for this model using the lme4 package using the following R code as explained in a R/PHARMA presentation on implementing MMRM in R by Daniel Sabanés Bové.\n\nlmer(data=simulated_data,\n     CHG ~ TRT01P + AVISIT + BASE + AVISIT*TRT01P + AVISIT*BASE + (0 + AVISIT | USUBJID),\n     control = lmerControl(check.nobs.vs.nRE = \"ignore\", optimizer=\"nlminbwrap\")) %&gt;%\n  summary()\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: CHG ~ TRT01P + AVISIT + BASE + AVISIT * TRT01P + AVISIT * BASE +      (0 + AVISIT | USUBJID)\n   Data: simulated_data\nControl: lmerControl(check.nobs.vs.nRE = \"ignore\", optimizer = \"nlminbwrap\")\n\nREML criterion at convergence: 1363.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.02960 -0.45454  0.00693  0.47992  2.25254 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr          \n USUBJID  AVISITvisit1 0.2711   0.5207                 \n          AVISITvisit2 0.3832   0.6190   0.67          \n          AVISITvisit3 0.4855   0.6968   0.36 0.58     \n          AVISITvisit4 0.3638   0.6031   0.30 0.54 0.77\n Residual              0.2323   0.4819                 \nNumber of obs: 615, groups:  USUBJID, 200\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)            0.140319   0.122380   1.147\nTRT01P10               0.048359   0.137772   0.351\nTRT01P20               0.012654   0.139987   0.090\nTRT01P40               0.175168   0.152565   1.148\nAVISITvisit2          -0.133165   0.156504  -0.851\nAVISITvisit3          -0.313168   0.204968  -1.528\nAVISITvisit4          -0.518651   0.196656  -2.637\nBASE                  -0.374132   0.100089  -3.738\nTRT01P10:AVISITvisit2  0.164612   0.175655   0.937\nTRT01P20:AVISITvisit2  0.288668   0.174938   1.650\nTRT01P40:AVISITvisit2  0.129673   0.193675   0.670\nTRT01P10:AVISITvisit3  0.471316   0.227321   2.073\nTRT01P20:AVISITvisit3  0.663268   0.231060   2.871\nTRT01P40:AVISITvisit3  0.257118   0.243162   1.057\nTRT01P10:AVISITvisit4  0.769739   0.218312   3.526\nTRT01P20:AVISITvisit4  0.854437   0.221969   3.849\nTRT01P40:AVISITvisit4  0.590619   0.234059   2.523\nAVISITvisit2:BASE      0.018250   0.129286   0.141\nAVISITvisit3:BASE      0.009984   0.175907   0.057\nAVISITvisit4:BASE      0.092543   0.168284   0.550\n\n\n\nCorrelation matrix not shown by default, as p = 20 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\noptimizer (nlminbwrap) convergence code: 0 (OK)\nModel is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\nNote that the choice of the particular optimizer via control = lmerControl(optimizer=\"nlminbwrap\") was chosen based on trying different ones to resolve convergence warnings. This process, as well as other tricks (like scaling and centering covariates) are conveniently wrapped in the dedicated mmrm R package. For this reason the mmrm package is preferable and its usage is demonstrated below:\n\nmmrm_fit &lt;- mmrm(\n  formula = CHG ~ TRT01P + AVISIT + BASE + AVISIT:TRT01P + AVISIT:BASE + us(AVISIT | USUBJID),\n  data = simulated_data %&gt;%\n    mutate(USUBJID=factor(USUBJID)))\n# reml=TRUE default option used (not specifically requested)\n# e.g. cs(AVISIT | USUBJID) would request compound-symmetric covariance\n#   structure instead of unstructed (\"us\")\n# us(AVISIT | TRT01P / USUBJID) would request separate covariance matrices\n#   for each treatment gorup\n\nWhile we can of course summarize the model fit and regression coefficients via summary(mmrm_fit), this is often not the main output we are interested in. Instead, we often want for each visit the treatment differences or least-squares means by treatment group. For a MMRM this involves linear combinations of regression coefficients, potentially weighted according to the distribution of (categorical baseline covariates). One very convenient way of obtaining these inferences is with the emmeans R package, which we can use with many different types of models including brms models. To obtain LS-means by visit for each treatment group, we use the emmeans function:\n\nemm1 &lt;- emmeans(mmrm_fit, ~ TRT01P | AVISIT, \n                lmer.df=\"kenward-roger\", weights = \"proportional\")\nprint(emm1)\n\nAVISIT = visit1:\n TRT01P  emmean     SE  df lower.CL upper.CL\n 0      -0.1131 0.1014 195 -0.31299  0.08681\n 10     -0.0647 0.0934 195 -0.24887  0.11941\n 20     -0.1004 0.0966 195 -0.29089  0.09002\n 40      0.0621 0.1141 195 -0.16295  0.28711\n\nAVISIT = visit2:\n TRT01P  emmean     SE  df lower.CL upper.CL\n 0      -0.2339 0.1223 162 -0.47546  0.00767\n 10     -0.0209 0.1150 164 -0.24798  0.20613\n 20      0.0674 0.1134 160 -0.15650  0.29135\n 40      0.0709 0.1389 163 -0.20336  0.34525\n\nAVISIT = visit3:\n TRT01P  emmean     SE  df lower.CL upper.CL\n 0      -0.4195 0.1512 125 -0.71882 -0.12017\n 10      0.1002 0.1405 124 -0.17797  0.37832\n 20      0.2564 0.1455 126 -0.03155  0.54440\n 40      0.0128 0.1586 121 -0.30114  0.32672\n\nAVISIT = visit4:\n TRT01P  emmean     SE  df lower.CL upper.CL\n 0      -0.5691 0.1386 124 -0.84347 -0.29466\n 10      0.2490 0.1288 124 -0.00591  0.50398\n 20      0.2980 0.1335 125  0.03382  0.56224\n 40      0.1967 0.1450 121 -0.09033  0.48378\n\nConfidence level used: 0.95 \n\n\nTo get the implied contrasts for each dose compared with the control we use the contrast function`:\n\ncontrasts1 &lt;- contrast(emm1, adjust=\"none\", method=\"trt.vs.ctrl\", ref=1)\nprint(contrasts1)\n\nAVISIT = visit1:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0   0.0484 0.138 195   0.351  0.7260\n TRT01P20 - TRT01P0   0.0127 0.140 195   0.090  0.9281\n TRT01P40 - TRT01P0   0.1752 0.153 195   1.148  0.2523\n\nAVISIT = visit2:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0   0.2130 0.168 163   1.269  0.2064\n TRT01P20 - TRT01P0   0.3013 0.167 161   1.806  0.0728\n TRT01P40 - TRT01P0   0.3048 0.185 163   1.648  0.1013\n\nAVISIT = visit3:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0   0.5197 0.206 125   2.517  0.0131\n TRT01P20 - TRT01P0   0.6759 0.210 125   3.221  0.0016\n TRT01P40 - TRT01P0   0.4323 0.219 123   1.973  0.0508\n\nAVISIT = visit4:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0   0.8181 0.189 124   4.324  &lt;.0001\n TRT01P20 - TRT01P0   0.8671 0.192 124   4.506  &lt;.0001\n TRT01P40 - TRT01P0   0.7658 0.201 122   3.817  0.0002\n\n\nThe pairwise comparisons showing nominal p-values only (due to adjust=\"none\"), but we could request e.g. adjust=\"bonferroni\" instead. When we specified method=\"trt.vs.ctrl\", we told the contrast function to use the first group as the control group by specifying its index via ref=1. Alternatively, we could have asked for all possible pairwise contrasts via contrast(..., method=\"pairwise\") (or pairs(...)).\nSometimes, we may want to average the treatment effects across weeks 8 and 12. E.g. if we are very sure that (almost) the full treatment effect will have set in by week 8. When doing so, it is useful to fit a model that estimates separate treatment effects for both visits and then to average them. In contrast to averaging outcomes across visits for individual patients first, this approach more coherently deals with missing data, intercurrent events and differing effects of covariates. emmeans also allows us to do that, we just need to specify our contrasts slightly more manually. Note that in this case we need to know the ordering of estimates in the output of the emmeans() function in order to specify the contrasts:\n\nemm1a &lt;- emmeans(mmrm_fit, ~ TRT01P:AVISIT, weights=\"proportional\")\ncontrast(emm1a, \n         method=list(\"40 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0,0.5, -0.5,0,0,0.5),\n                     \"20 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0.5,0, -0.5,0,0.5,0),\n                     \"10 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0.5,0,0, -0.5,0.5,0,0)),\n         adjust=\"none\")\n\n contrast      estimate    SE  df t.ratio p.value\n 40 vs Placebo    0.599 0.181 123   3.314  0.0012\n 20 vs Placebo    0.772 0.173 126   4.464  &lt;.0001\n 10 vs Placebo    0.669 0.170 125   3.932  0.0001\n\n\nWith the release of brms 2.19.0 in March 2023 it became possible to fit equivalent models easily based on a Bayesian implementation of a MMRM with an unstructured covariance matrix as shown below.\n\n\n13.4.2 brms implementation\nThe code below shows how we specify a MMRM model in a very similar way to SAS and the lme4 approach.\n\n# Setup forward difference contrasts for changes between visits\ncontrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_model1 &lt;- bf(CHG ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT,\n                  autocor = ~unstr(time=AVISIT, gr=USUBJID),\n                  sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P)\n\n# If we manually specify priors by providing out own Stan code via the stanvars \n# option, we may want to use `center=FALSE` in `bf()`, otherwise not.\n\n# Explicitly specifying quite weak priors (given the variability in the data).\n# These priors are set considering that unity is assumed to be the\n# sampling standard deviation and that the outcome data is scaled such that\n# unity is a considerable change.\nmmrm_prior1 &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=b) +\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) + # 90% CrI spans 1/10 - 10\n    prior(normal(0,  log(2.0)/1.64), class=b, dpar=sigma) +         # 90% CrI spans  1/2 - 2x\n    prior(lkj_corr_cholesky(1), class=\"Lcortime\")\n\nbrmfit1 &lt;- brm(\n  formula=mmrm_model1,\n  prior= mmrm_prior1,\n  data = simulated_data,\n  seed=234235,\n  control = control_args,\n  refresh = 0\n)\n\nbrmfit1\n\nNote that instead of writing (0 + AVISIT | USUBJID) like in the lme4 code, the brms syntax resembles the mmrm package syntax. We specify an unstructured correlation structure over time for groups of observations (here with the same unique patient identifier USUBJID) using autocor=~unstr(time=AVISIT, gr=USUBJID).\n\nNote that using (0 + AVISIT | USUBJID) as in the lme4 turns out to not work well with brms. While conceptually this implies the correct correlation structure via an unstructured random effect, the model becomes overparametrized due to the residual error and random effect standard deviations becoming non-identifiable. Trying to avoid this e.g. by setting the uncorrelated residual standard deviation to a fixed value that is small relative to the total variability (e.g. prior(constant(0.1), sigma)) still results in very poor sampling of the posterior distribution (high Rhat values and very low effective sample size for the MCMC samples). The approach implemented by the unstr syntax uses instead a marginal approach avoiding the non-identifability of the conditional approach.\n\nThe model formula above specifies that the correlation structure is the same across all treatment groups. This is not an assumption we have to make and SAS provides the GROUP=TRT01P option in the REPEATED statement to avoid it. At the time of writing this chapter, brms, lme4 and the mmrm package do not have an option for allowing separate unstructed covariance matrices for the different treatment groups. However, brms offers more flexibility than the other two packages regarding the variation at different visits and treatment groups through “distributional regression” (i.e. we can specify a regression model for parameters of the distribution other than the location, in this case sigma).\n\nIf we just wanted a compound symmetric correlation matrix, we would just use (1 | USUBJID), because a simple random effect on the intercept induces an equal correlation between all visits.\n\nBy writing sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P we specify that we want heterogeneous standard deviations over time that may differ by treatment group (note that brms models the standard deviation rather than the variance), which we would typically assume. If we instead wanted to assume the same variance at each visit, we would simply omit this option. Note that while we did not specify it explicitly, we are using a \\(\\log\\)-link function for sigma so that a regression coefficient of \\(0.69\\) approximately corresponds to a doubling of the standard deviation. The user can explicitly define the link using the family argument of bf and set family=gaussian(link_sigma=\"log\") (or use other links if desired). Thus, if we wanted to set prior distributions for the treatment and the visit-by-treatment interaction terms, even a normal(0, 1) prior already allows for considerable a-priori variation in sigma between treatment groups and visits. A useful way for specifying a prior on a log scale is by defining the standard deviation of a normal distribution to be equal to \\(\\log(s)/\\Phi^{-1}(1/2+c/2)\\), which implies that the central credible interval \\(c\\) spans the range \\(1/s - s\\), i.e. \\(\\log(2)/1.64\\) corresponds to the central 90% credible interval ranging from 1/2x to 2x around the mean.    \nAs for the mmrm package, we can also easily obtain the equivalent of least-squares means with highest-posterior density (HPD) credible intervals for each treatment group by visit using emmeans.\n\nemm2 &lt;- emmeans(brmfit1, ~ TRT01P | AVISIT, weights=\"proportional\")\nemm2\n\nAVISIT = visit1:\n TRT01P   emmean lower.HPD upper.HPD\n 0      -0.11526   -0.3211    0.0732\n 10     -0.06703   -0.2601    0.1164\n 20     -0.09825   -0.3092    0.0768\n 40      0.06265   -0.1875    0.2953\n\nAVISIT = visit2:\n TRT01P   emmean lower.HPD upper.HPD\n 0      -0.23131   -0.4608    0.0079\n 10     -0.02745   -0.2524    0.2125\n 20      0.06051   -0.1411    0.2672\n 40      0.06765   -0.2645    0.3558\n\nAVISIT = visit3:\n TRT01P   emmean lower.HPD upper.HPD\n 0      -0.39925   -0.6974   -0.1041\n 10      0.08770   -0.1813    0.3625\n 20      0.24422   -0.0110    0.5093\n 40      0.00771   -0.3653    0.3757\n\nAVISIT = visit4:\n TRT01P   emmean lower.HPD upper.HPD\n 0      -0.54006   -0.8580   -0.2486\n 10      0.23853   -0.0276    0.5150\n 20      0.28183    0.0303    0.5250\n 40      0.18429   -0.1164    0.4727\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\nWe can also get the pairwise treatment comparisons with HPD credible intervals in this manner.\n\ncontrast(emm2, adjust=\"none\", method=\"trt.vs.ctrl\")\n\nAVISIT = visit1:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0   0.0455  -0.24032     0.295\n TRT01P20 - TRT01P0   0.0157  -0.24632     0.299\n TRT01P40 - TRT01P0   0.1772  -0.12894     0.477\n\nAVISIT = visit2:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0   0.2032  -0.13069     0.523\n TRT01P20 - TRT01P0   0.2934  -0.00153     0.612\n TRT01P40 - TRT01P0   0.2950  -0.10206     0.667\n\nAVISIT = visit3:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0   0.4938   0.07790     0.875\n TRT01P20 - TRT01P0   0.6467   0.26292     1.043\n TRT01P40 - TRT01P0   0.4133  -0.03940     0.881\n\nAVISIT = visit4:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0   0.7718   0.38126     1.214\n TRT01P20 - TRT01P0   0.8252   0.46563     1.230\n TRT01P40 - TRT01P0   0.7191   0.30053     1.123\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n\n\nHPD and quantile based credible intervals may differ whenever the posterior is heavily skewed or not unimodal. The quantile based credible intervals are numerically simpler in their definition and can be estimated more robustly estimated via MCMC. Furthermore, the quantiles of a distribution are transformation invariant, whereas HPD intervals are not transformation invariant (this of interest for the case of generalized models with non-linear link functions). emmeans can report summaries (using the frequentist=TRUE option) based on a normal approximation resembling the frequentist estimate plus standard error approach.\nWhile the default HPD intervals reported from emmeans are a useful summary of the positerior, the respective quantile summaries can also be extracted by first converting with the as.mcmc conversion function the emmeans results into the respective posterior MCMC sample. This sample one can then conveniently summarized by using utilities from the posterior package:\n\n\nmc.emm2 &lt;- as.mcmc(emm2)\nsummarise_draws(mc.emm2, default_summary_measures()) \n\n# A tibble: 16 × 7\n   variable                    mean   median     sd    mad       q5     q95\n   &lt;chr&gt;                      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 TRT01P 0 AVISIT visit1  -0.114   -0.115   0.0996 0.0974 -0.277    0.0504\n 2 TRT01P 10 AVISIT visit1 -0.0665  -0.0670  0.0960 0.0932 -0.227    0.0908\n 3 TRT01P 20 AVISIT visit1 -0.100   -0.0983  0.0983 0.0965 -0.265    0.0585\n 4 TRT01P 40 AVISIT visit1  0.0614   0.0626  0.123  0.122  -0.148    0.263 \n 5 TRT01P 0 AVISIT visit2  -0.229   -0.231   0.119  0.117  -0.425   -0.0288\n 6 TRT01P 10 AVISIT visit2 -0.0251  -0.0275  0.120  0.119  -0.219    0.175 \n 7 TRT01P 20 AVISIT visit2  0.0622   0.0605  0.105  0.107  -0.106    0.237 \n 8 TRT01P 40 AVISIT visit2  0.0651   0.0676  0.160  0.159  -0.197    0.326 \n 9 TRT01P 0 AVISIT visit3  -0.400   -0.399   0.153  0.155  -0.653   -0.145 \n10 TRT01P 10 AVISIT visit3  0.0882   0.0877  0.140  0.136  -0.138    0.320 \n11 TRT01P 20 AVISIT visit3  0.247    0.244   0.135  0.133   0.0312   0.472 \n12 TRT01P 40 AVISIT visit3  0.00740  0.00771 0.183  0.175  -0.301    0.307 \n13 TRT01P 0 AVISIT visit4  -0.537   -0.540   0.155  0.147  -0.792   -0.277 \n14 TRT01P 10 AVISIT visit4  0.237    0.239   0.137  0.131   0.00818  0.462 \n15 TRT01P 20 AVISIT visit4  0.285    0.282   0.126  0.126   0.0809   0.495 \n16 TRT01P 40 AVISIT visit4  0.185    0.184   0.150  0.148  -0.0627   0.433 \n\nmc.contrast.emm2 &lt;- as.mcmc(contrast(emm2, adjust=\"none\", method=\"trt.vs.ctrl\"))\nsummarise_draws(mc.contrast.emm2, default_summary_measures())\n\n# A tibble: 12 × 7\n   variable                                    mean median    sd   mad      q5   q95\n   &lt;chr&gt;                                      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 contrast TRT01P10 - TRT01P0 AVISIT visit1 0.0474 0.0455 0.137 0.139 -0.175  0.268\n 2 contrast TRT01P20 - TRT01P0 AVISIT visit1 0.0135 0.0157 0.139 0.139 -0.214  0.242\n 3 contrast TRT01P40 - TRT01P0 AVISIT visit1 0.175  0.177  0.157 0.158 -0.0812 0.432\n 4 contrast TRT01P10 - TRT01P0 AVISIT visit2 0.204  0.203  0.169 0.167 -0.0759 0.474\n 5 contrast TRT01P20 - TRT01P0 AVISIT visit2 0.292  0.293  0.158 0.154  0.0305 0.559\n 6 contrast TRT01P40 - TRT01P0 AVISIT visit2 0.294  0.295  0.194 0.190 -0.0289 0.615\n 7 contrast TRT01P10 - TRT01P0 AVISIT visit3 0.489  0.494  0.203 0.199  0.151  0.819\n 8 contrast TRT01P20 - TRT01P0 AVISIT visit3 0.648  0.647  0.200 0.197  0.319  0.977\n 9 contrast TRT01P40 - TRT01P0 AVISIT visit3 0.408  0.413  0.231 0.225  0.0306 0.788\n10 contrast TRT01P10 - TRT01P0 AVISIT visit4 0.774  0.772  0.206 0.196  0.435  1.12 \n11 contrast TRT01P20 - TRT01P0 AVISIT visit4 0.822  0.825  0.194 0.191  0.500  1.14 \n12 contrast TRT01P40 - TRT01P0 AVISIT visit4 0.722  0.719  0.212 0.216  0.381  1.06 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs with a model fit using the mmrm package, we can also obtain inference about the average treatment effect across weeks 8 and 12 using the emmeans package:\n\nemm2a &lt;- emmeans(brmfit1, ~ TRT01P:AVISIT, weights=\"proportional\")\ncontrast(emm2a, \n         method=list(\"40 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0,0.5, -0.5,0,0,0.5),\n                     \"20 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0.5,0, -0.5,0,0.5,0),\n                     \"10 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0.5,0,0, -0.5,0.5,0,0)))\n\n contrast      estimate lower.HPD upper.HPD\n 40 vs Placebo    0.565     0.196     0.947\n 20 vs Placebo    0.734     0.404     1.065\n 10 vs Placebo    0.630     0.266     0.967\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n\n\n\n\n\n\n\n\n\n13.4.3 Model parameterization and the importance of (at least weak) priors\nHere, we explicitly specified a proper prior distribution (but very weak) for each regression coefficient. This is important, because by default brms would only have put a prior distribution on intercept terms. If we instead assume improper flat prior distributions, we often run into issues with sampling from the model. Weakly informative priors ensure stable sampling while not influencing the posterior distribution in relevant ways. One common approach to define a weakly informative prior is by way of identifying the scale of the parameter in question, e.g. parameters on a logarithmic scale commonly do not vary over many magnitudes or a parameter on a logistic scale varies within \\(-3\\) to \\(3\\) provided that the effects are not extreme.\nIn which way we are comfortable to specify prior distributions may in part determine how we parameterize the model:\n\nWe chose to setup forward difference contrasts for changes between visits when specifying contrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif in combination with CHG ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT. I.e. the intercept term will refer to the expected change from baseline averaged across all visits, while there will be regression coefficients for the expected difference from visit 1 to 2, 2 to 3 and 3 to 4. We can check this using solve(cbind(1, contrasts(simulated_data$AVISIT))).\nNot only does it seem reasonable to specify prior information about such parameters, but this also induces a reasonable correlation between our prior beliefs for each visit. I.e. if values at one visit are higher, we would expect values at the surrounding visits to also be higher. This is illustrated by the very typical pattern seen in the weight loss over time in the EQUIP study (Allison et al. 2012) shown in the figure below. As is typical for clinical trial data, there are only small visit-to-visit changes in the mean outcome, which can be nicely represented by forward difference contrasts. Furthermore, also note that the forward differences parametrization does imply a correlation structure which resembles the fact that we observe patient longitudinally accross the subsequent visits.\nIf we had specified the same model code without setting up forward difference contrasts, the intercept would have stood for the expected change from baseline at the reference visit (by default visit 1), while the regression coefficients would have represented the difference in the expected change at all the remaining visits compared with the reference visit 1.\nAlternatively, we could have chosen the parameterization 0 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT as our main approach. In this cell means parametrization, we would need to provide prior distributions for each visit.\nIf we make the priors for each visit independent in the latter approach, the prior distribution will specify plausible values for the mean outcome at that visit, but will suggest that all sizes of visit-to-visit changes that keep values within the range of a-priori plausible values are equally plausible. This is typically not the case.\n\n\n\n\n\n\nChange in body weight over time in the EQUIP study (data digitized from published figure)\n\n\n\n\nWith very vague priors the differences between these approach will usually be negligible. If we are interested in specifying weakly informative or even informative prior distributions, one should choose the paramterization, in which it is easiest to express the available prior information. Other ways to set-up contrasts are illustrated in the case study on time-to-event data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.4.4 Meta-analytic combined (MAC) approach to historical data for MMRMs\n\nWARNING: There is very limited experience with a MAC approach for MMRMs and intuitions from MAC/MAP for a single control group parameter may not apply to high-dimensional settings, where many parameters are assumed to be exchangeable between studies.\n\nOne source of informative prior distributions is historical data. Let us assume that we have access to the individual patient data for the placebo groups of 10 historical studies.\n\n\nShow the code\nset.seed(3547844)\nN_studies_hist &lt;- 10L\nNf_hist &lt;- 500L\nif(use_small_N) {\n    Nf_hist &lt;- 50L\n}\nNf_per_study_hist &lt;- Nf_hist/N_studies_hist\nhistorical_studies &lt;- rmvnorm(n=10 * Nf_hist,\n                              mean = zero,\n                              sigma = cov_matrix) %&gt;%\n  as_tibble() %&gt;%\n  filter(BASE&gt;0) %&gt;%\n  filter(row_number()&lt;=Nf_hist) %&gt;%\n  mutate(STUDYID = rep(1L:N_studies_hist, each=Nf_per_study_hist ), # Create a study ID\n         # Add a random study effect\n         rseff = rep(rnorm(n=N_studies_hist, mean=0, sd=0.1), each=Nf_per_study_hist),\n         USUBJID = row_number(),\n         TRT01P=factor(0, levels=c(0, 10, 20, 40))) %&gt;%\n  pivot_longer(cols=starts_with(\"visit\"), names_to = \"AVISIT\", values_to=\"AVAL\") %&gt;%\n  mutate(\n    AVAL = AVAL + rseff,\n    ADY = case_when(AVISIT==\"visit1\" ~ 2L*7L,\n                    AVISIT==\"visit2\" ~ 4L*7L,\n                    AVISIT==\"visit3\" ~ 8L*7L,\n                    AVISIT==\"visit4\" ~ 12L*7L),\n    AVISIT = factor(AVISIT, paste0(\"visit\", 1:4)),\n    CHG = AVAL - BASE, \n    STUDYID = factor(STUDYID, levels=1L:N_studies_hist+1)) %&gt;%\n  dplyr::select(-rseff) %&gt;%\n  relocate(STUDYID, USUBJID, TRT01P, AVISIT, ADY, AVAL, CHG, BASE)\n\n\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_line()`).\n\n\n\n\n\n\n\n\n\nThere are two mathematically equivalent ways of using historical data: the meta-analytic predictive (MAP) and the meta-analytic combined (MAC) approaches. The first step of the MAP approach involves fitting a model to the historical data with model parameters allowed to vary hierarchically across trials (trial is a random effect). The posterior predictive distribution for the parameters of a new trial obtained from this model is then used as the prior distribution for the analysis of our new trial of interest. In the MAC approach, a single model is fit jointly to the historical and new data.\nHere, we use the MAC approach, because it is easier to implement when there are many model parameters that vary across trials. However, the brms models we specify in either case are almost identical. It is also useful to first fit the model to only the historical data, which we could then use as an .\nFor a MAP approach we would still use the same model structure as below and use the drop_unused_levels=FALSE option in brms in combination with coding STUDYID to have an additional 11th factor level (for our new study) and TRT01P with the treatment groups of the new study included in its factor levels. That way, the number and indexing of parameters inside the generated Stan code remains unchanged and brms already samples parameter values for a new 11th study. We can easily fit such a model only to the historical data in order to inspect the resulting predictive distribution, which tells us how much information about the parameters of a new study the historical data in combination with our specified prior distributions implies.\n\ncontrasts(historical_studies$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_mac_model &lt;- bf( CHG  ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT + ( 1 + AVISIT + BASE + BASE:AVISIT | STUDYID),  \n                     autocor=~unstr(time=AVISIT, gr=USUBJID), \n                     # center = FALSE, # We would use this option, if we used a MAP approach\n                     sigma ~  1 + AVISIT + TRT01P + AVISIT:TRT01P + (1 + AVISIT + TRT01P + AVISIT:TRT01P | STUDYID))\n\n# Priors as set are based on the assumed prior unit standard deviation\n# (usd) of unity. For the random effect for the log of sigma the unit\n# standard deviation is about sqrt(2).\nmmrm_mac_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=b) +\n    prior(normal(0, 0.25), class=sd, coef=Intercept, group=STUDYID) + # moderate heterogeneity on the intercept (usd/4)\n    prior(normal(0, 0.125), class=sd, group=STUDYID) +                # smaller heterogeneity on regression coefficients (usd/8)\n\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) +\n    prior(normal(0, log(2.0)/1.64), class=b, dpar=sigma) +\n    prior(normal(0, sqrt(2)/4.0), class=sd, coef=Intercept, group=STUDYID, dpar=sigma) + # same heterogeneity logic \n    prior(normal(0, sqrt(2)/8.0), class=sd, group=STUDYID, dpar=sigma) +                 # as for main regression coefficients\n\n    prior(lkj_corr_cholesky(1), class=Lcortime) +\n    prior(lkj(2), class=cor, group=STUDYID)\n    \n\nhistfit &lt;- brm(\n    formula=mmrm_mac_model,\n    prior=mmrm_mac_prior,\n    data = historical_studies,\n    drop_unused_levels=FALSE,\n    seed = 234525,\n    control = control_args,\n    refresh = 0,\n)\n\nNote that our simulation code created a dataset with STUDYID column that labels the 10 different studies being simulated and in addition has an extra 11th factor level for a new study.\n\nlevels(historical_studies$STUDYID)\n\n [1] \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\"\n\n\nThis trick with the extra factor level (plus using the drop_unused_levels=FALSE option in brms) automatically creates samples from the predictive distribution for the parameters of a new hypothetical study. Without this trick we would have to sample from a multivariate normal distribution for each MCMC sample in order to get the correlation of predicted parameters right. Instead, we can now simply add the population-level parameter estimate plus the study specific parameter estimate for the hypothetical new (11th) study (for which we had no data). This will then already correctly reflect the correlation between the predictions for the different parameters.\n\npredictive_dist &lt;- histfit %&gt;%\n    as_draws_rvars(variable=c(\"^b_\", \"^r_STUDYID\"), regex=TRUE) %&gt;%\n    mutate_variables(b_Intercept = b_Intercept + r_STUDYID[11,\"Intercept\"],\n                     b_AVISITvisit2Mvisit1 = b_AVISITvisit2Mvisit1 + r_STUDYID[11,\"AVISITvisit2Mvisit1\"],\n                     b_AVISITvisit3Mvisit2 = b_AVISITvisit3Mvisit2 + r_STUDYID[11,\"AVISITvisit3Mvisit2\"],\n                     b_AVISITvisit4Mvisit3 = b_AVISITvisit4Mvisit3 + r_STUDYID[11,\"AVISITvisit4Mvisit3\"],\n                     b_BASE = b_BASE + r_STUDYID[11,\"BASE\"],\n                     `b_AVISITvisit2Mvisit1:BASE` = `b_AVISITvisit2Mvisit1:BASE` + r_STUDYID[11,\"AVISITvisit2Mvisit1:BASE\"],\n                     `b_AVISITvisit3Mvisit2:BASE` = `b_AVISITvisit3Mvisit2:BASE` + r_STUDYID[11,\"AVISITvisit3Mvisit2:BASE\"],\n                     `b_AVISITvisit4Mvisit3:BASE` = `b_AVISITvisit4Mvisit3:BASE` + r_STUDYID[11,\"AVISITvisit4Mvisit3:BASE\"]) %&gt;%\n    subset_draws(variable=c(\"^b_Intercept\", \"^b_AVISIT.*\", \"^b_BASE.*\"), regex=TRUE) %&gt;%\n    as_draws_df()\n\nWe could now explore and visualize the joint distribution of these predicted parameter values in order to get an idea for the prior distribution induced by the historical data.\n\n\n\n\n\n\n\n\n\nThe MAP approach has been very popular at Novartis, especially for proof of concept studies. To use it, we would now have to approximate the joint predictive distribution of the model parameters using a suitable multivariate distribution such as a multivariate Student-t distribution. Then, we would have to correctly add Stan code to the stanvars argument to the brm call in order to specify this prior distribution. We would also use center=FALSE to the brms-formula function bf() in order to make setting priors easier, which would otherwise get harder if we let brms center covariates.\nThus, implementing the MAP approach with brms is currently a quite advanced task. However, this will become a lot easier once support for it is available in the RBesT package.\n\nBelow is the code for fitting a MMRM to the historical and new trial data jointly.\n\ncombined_data &lt;- bind_rows(historical_studies %&gt;%\n                           mutate(historical=factor(1L, levels=0L:1L)),\n                           simulated_data %&gt;%\n                           mutate(STUDYID=factor(11, levels=1:11),\n                                  USUBJID=USUBJID+max(historical_studies$USUBJID),\n                                  historical=factor(0L, levels=0L:1L)))\n\ncontrasts(combined_data$AVISIT) &lt;- MASS::contr.sdif\n\nmacfit &lt;- brm(\n    formula = mmrm_mac_model,\n    prior = mmrm_mac_prior,\n    data = combined_data,       # now using the combined data\n    drop_unused_levels = FALSE, # Important, if we fit only to the historical data\n    seed = 234525,\n    control = control_args,\n    refresh = 0, iter=20, warmup=10\n)\n\nThis gives us the following inference about the treatment effects at each visit:          \n\nemmeans(macfit, ~ TRT01P | AVISIT, weights = \"proportional\") %&gt;%\n    contrast(., adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n13.4.5 Robust Meta-analytic combined (rMAC) approach to historical data for MMRMs\n\nWARNING: This section presents initial ideas for a robust MAC approach. The discussed technique is not well-understood in the MMRM setting with a larger number of parameters, about which borrowing of information is needed. The discussed approach is the subject of ongoing research and its operating characteristics are not well-understood. It is shared to illustrate features of brms and to gather feedback. Do not use without careful testing for your specific setting (e.g. via simulation).\n\nOne popular variant of the MAP approach is its robust rMAP version that adds a uninformative or weakly informative mixture component to the MAP prior. In this way the historical prior information can be discarded or donweighted when there is an apparent prior-data-conflict.\nA way of obtaining a type of robust MAC (rMAC) approach is to introduce an additional model parameter that indicates how much the parameters of the new data differ from the historical data and to assign this parameter a “slab-and-spike”-type prior. I.e. a prior distribution that is peaked at zero and heavy-tailed such as a double-exponential (DE) distribution (aka “Laplace distribution”).\n\nexpand_grid(mean=0, scale=c(0.125, 0.25, 0.5, 1), x=seq(-2.5, 2.5, 0.01)) %&gt;%\n    mutate(density = dasym_laplace(x, mean, scale),\n           scale=factor(scale)) %&gt;%\n    ggplot(aes(x=x, y=density, col=scale, label=scale)) +\n    geom_line(linewidth=2, alpha=0.7) +\n    ylab(\"DE(0, scale) density for various\\nscale parameter values\") +\n    directlabels::geom_dl(method=\"smart.grid\")\n\n\n\n\n\n\n\n\nHowever, while this approach is well understood for a single control group log-event-rate or logit-proportion, there is currently no experience with it for continuous data and multiple parameters. For example, it is not immediately obvious how to encode a prior belief that when outcomes at one visit are similar to historical data, we become more likely to believe this about other visits. Here we setup the scale of the DE distribution such that the tail probability beyond the unit standard deviation of unity is close to 5%. This will ensure that most mass of the prior is centered around 0 while very large deviations are allowed for by the long tail of the DE distribution. A more detailed analysis of this prior is subject to research.\nWith these caveats stated, here is how one can implement this model with brms:\n\nmmrm_rmac_model &lt;- bf( CHG  ~ 1 + AVISIT + BASE + historical + BASE:AVISIT + TRT01P + TRT01P:AVISIT + \n                         AVISIT:historical + BASE:historical + BASE:AVISIT:historical +\n                         ( 1 + AVISIT + BASE + BASE:AVISIT | STUDYID),  \n                     autocor=~unstr(time=AVISIT, gr=USUBJID), \n                     # center = FALSE, # We would use this option, if we used a MAP approach\n                     sigma ~  1 + AVISIT + (1 + AVISIT | STUDYID))\n\n# can be used to find out on instantiated parameters\n#get_prior(mmrm_rmac_model, combined_data)\n\nmmrm_rmac_prior &lt;- mmrm_mac_prior +\n    prior(double_exponential(0, 0.25), class=b, coef=historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit2Mvisit1:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit3Mvisit2:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit4Mvisit3:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit2Mvisit1:BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit3Mvisit2:BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit4Mvisit3:BASE:historical1)\n\nrmacfit &lt;- brm(\n    formula=mmrm_rmac_model,\n    prior=mmrm_rmac_prior,\n    data = combined_data,\n    drop_unused_levels=FALSE,\n    seed = 234525,\n    control = control_args,\n    refresh = 0\n)\n\nWarning: Rows containing NAs were excluded from the model.\n\n\nThis gives us the following inference about the treatment effects at each visit:          \n\nemmeans(rmacfit, ~ TRT01P | AVISIT, weights = \"proportional\") %&gt;%\n    contrast(., adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n13.4.6 Going beyond the traditional MMRM: monotonic effects\nNow, let us look at some ways, in which we can add some advanced features of brms on top of a MMRM.\nWhat if we wanted to make our analysis more efficient by exploiting knowledge about the structure of the experiment? E.g. what if we wanted to assume a monotonic increase of efficacy across doses (for the treatment main effect) and assume the how this effect differs at each visit is also monotonly across doses?\n\ncontrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_mo_model &lt;- bf( CHG ~ 1 + AVISIT + mo(TRT01P) + BASE + mo(TRT01P):AVISIT + BASE:AVISIT,\n                     autocor=~unstr(AVISIT, USUBJID), \n                     sigma ~1 + AVISIT + TRT01P + AVISIT:TRT01P)\n\n# use the same prior as before and use the default dirichlet(1) priors\n# of brms for the increase fractions of the monotonic effects. This\n# represents a a uniform prior over the fractions\nmmrm_mo_prior &lt;- mmrm_prior1\n\nbrmfit2 &lt;- brm(\n  formula = mmrm_mo_model,\n  prior = mmrm_mo_prior,\n  data = simulated_data %&gt;% mutate(TRT01P=ordered(TRT01P)),\n  control = control_args,\n  refresh = 0)\n\nThe syntax of emmeans would in this case still be very straightforward:\n\nemmeans(brmfit2, ~ TRT01P | AVISIT, weights=\"proportional\") %&gt;%\n    contrast(adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.4.7 Going beyond the traditional MMRM: non-linear functions\nAs an alternative to the assumptions we made regarding monotonic predictors, we can explicitly specify a functional form for how doses are related and how their effect develops over time. However, while we will assume a particular functional form for the treatment effect over time, we will keep how the control group is modeled as flexible as possible. Notice that we would want to set control = list(adapt_delta=0.999, max_treedepth=13), because we received warnings that there were divergent transitions (hence an adapt_delta closer to 1 than before) and poor sampling with many transitions hitting the maximum tree depth (hence max_treedepth increased from 10 to 13).\n\nmmrm_dr_model &lt;- bf( CHG ~ E0 + Emax * ndose^h/(ndose^h + ED50^h) * nweek^ht/(nweek^ht + ET50^ht),\n                     autocor=~unstr(AVISIT, USUBJID), \n                     sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P,\n                     nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)),\n                     nlf(ht ~ exp(loght)), nlf(ET50 ~ exp(logET50)),\n                     E0 ~ 1 + AVISIT + BASE + BASE:AVISIT,\n                     Emax ~ 1,\n                     logED50 ~ 1, logh ~ 1,\n                     logET50 ~ 1, loght ~ 1,\n                     nl=TRUE,\n                    family=gaussian())\n\nmmrm_dr_prior &lt;- prior(normal(0, 2), class=b, coef=Intercept, nlpar=E0) +\n    prior(normal(0,1), class=b, nlpar=E0) +\n    prior(normal(0, log(4.0)/1.64), nlpar=logh) +\n    prior(normal(0, log(4.0)/1.64), nlpar=loght) +\n    prior(normal(0, 1), nlpar=Emax) +\n    prior(normal(2.5, log(10.0)/1.64), nlpar=logED50) +\n    prior(normal(log(4), log(5)/1.64), nlpar=logET50) +\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) +\n    prior(normal(0, log(2.0)/1.64), class=b, dpar=sigma) +\n    prior(lkj_corr_cholesky(1), class=Lcortime)\n\nbrmfit3 &lt;- brm(\n  formula=mmrm_dr_model,\n  prior = mmrm_dr_prior,\n  data = simulated_data %&gt;%\n    mutate(nweek = ADY/7,\n           ndose = as.numeric(levels(TRT01P)[TRT01P])),\n  control = control_args,\n  refresh = 0\n)\n\nNote that here, it is hard to see how to make emmeans work. Instead, we use hypothesis to get treatment differences at each visit.\n\n\nShow the code\ncomparisons_for_sigemax &lt;- expand_grid(dose = c(10, 20, 40), week=c(2,4,8,12)) %&gt;%\n      mutate(`Hypothesis string` = paste0(\n        \"Emax_Intercept * \", dose, \"^exp(logh_Intercept)/(\", dose,\n        \"^exp(logh_Intercept) + exp(logED50_Intercept)^exp(logh_Intercept)) * \",\n        week, \"^exp(loght_Intercept)/(\",\n        week,\n        \"^exp(loght_Intercept) + exp(logET50_Intercept)^exp(loght_Intercept)) &lt; 0\"),\n        evaluation = map(`Hypothesis string`,\n                         \\(x) hypothesis(brmfit3, x)$hypothesis %&gt;%\n                           as_tibble())) %&gt;%\n      unnest(evaluation) %&gt;%\n      mutate(Comparison = case_when(week==2 ~ paste0(\"Visit 1: \", dose, \" vs. placebo\"),\n                                    week==4 ~ paste0(\"Visit 2: \", dose, \" vs. placebo\"),\n                                    week==8 ~ paste0(\"Visit 3: \", dose, \" vs. placebo\"),\n                                    TRUE ~ paste0(\"Visit 4: \", dose, \" vs. placebo\")),\n             Model = \"BMMRM (non-linear)\")\n\n# Plot the results\ncomparisons_for_sigemax %&gt;%\n    bind_rows(expand_grid(week=0, Estimate=0, dose=c(10, 20, 40))) %&gt;%\n    ggplot(aes(x=week, y=Estimate, ymin=CI.Lower, ymax=CI.Upper,\n               group=dose, col=factor(dose))) +\n    geom_hline(yintercept = 0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    geom_line(position=position_dodge(width=0.2)) +\n    theme(legend.position=\"right\") +\n    scale_colour_manual(\"Dose\", values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    ylab(\"Change from baseline\") +\n    ggtitle(\"Treatment difference to placebo\")\n\n\n\n\n\n\n\n\n\nJust note that the string specifying the hypothesis become something complex and long like\n\ncomparisons_for_sigemax$`Hypothesis string`[1]\n\n[1] \"Emax_Intercept * 10^exp(logh_Intercept)/(10^exp(logh_Intercept) + exp(logED50_Intercept)^exp(logh_Intercept)) * 2^exp(loght_Intercept)/(2^exp(loght_Intercept) + exp(logET50_Intercept)^exp(loght_Intercept)) &lt; 0\"\n\n\nfor the difference to placebo for the 10 mg dose at week 2.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#results",
    "href": "src/02h_mmrm.html#results",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.5 Results",
    "text": "13.5 Results\nHere is an overview of the results with some the different models we tried.\n\n\nShow the code\nmap2_dfr(list(brmfit1, brmfit2, macfit, rmacfit),\n         c(\"BMMRM\", \"BMMRM (monotonic)\", \"BMMRM (MAC)\", \"BMMRM (rMAC)\"),\n         \\(x, y) emmeans(x, ~ TRT01P | AVISIT, weights=\"proportional\") %&gt;%\n           contrast(adjust=\"none\", method=\"trt.vs.ctrl\", ref=1) %&gt;%\n           as_tibble() %&gt;%\n           mutate(Model=y)) %&gt;%\n  bind_rows(comparisons_for_sigemax %&gt;%\n              rename(estimate=Estimate, lower.HPD=CI.Lower, upper.HPD=CI.Upper) %&gt;%\n              mutate(AVISIT = paste0(\"visit\", case_when(week==2~1,\n                                                        week==4~2,\n                                                        week==8~3,\n                                                        week==12~4)),\n                contrast = paste0(dose,\" - 0\"))) %&gt;%\n  bind_rows(contrasts1 %&gt;%\n    confint() %&gt;% \n    as_tibble() %&gt;%\n    rename(lower.HPD=lower.CL,\n           upper.HPD=upper.CL) %&gt;%\n      mutate(Model=\"Frequentist MMRM\")) %&gt;%\n  dplyr::select(Model, contrast, AVISIT, estimate, lower.HPD, upper.HPD) %&gt;%\n  mutate(Model = factor(Model,\n                        levels=c(\"Frequentist MMRM\",\n                                 \"BMMRM\", \n                                 \"BMMRM (MAC)\", \"BMMRM (rMAC)\",\n                                 \"BMMRM (monotonic)\", \"BMMRM (non-linear)\"))) %&gt;%\n  ggplot(aes(x=contrast, y=estimate,\n             ymin=lower.HPD, ymax=upper.HPD, col=Model)) +\n  geom_hline(yintercept=0) +\n  geom_point(position=position_dodge(width=0.5)) +\n  geom_errorbar(position=position_dodge(width=0.5)) +\n  theme(legend.position=\"bottom\") +\n      guides(color=guide_legend(ncol=2, byrow=TRUE)) +\n  coord_flip() +\n  scale_color_manual(values=c(\"black\", \"#1b9e77\",\n                              \"#7570b3\", \"#e7298a\", \"#e6ab02\", \"#d95f02\")) +\n  facet_wrap(~AVISIT)\n\n\n\n\n\n\n\n\n\nUsing a BMMRM in the particular example had without informative prior distributions no particular advantage over a MMRM fit with REML. It is possible that there would be some advantage to using BMMRM with vague priors with smaller sample sizes. However, using historical control group informtion with a MAC (or rMAC) approach narrows the credible intervals. Making stronger assumptions about the dose response relationship over time turns out to also have a substantial effect on the width of the credible intervals. Of course, we could also use both prior information about the control group and assumptions about the dose response relationship for the drug.\nUnsurprisingly, putting in more information either in terms of prior knowledge about the control group or in terms of the structure of the dose response relationship leads to narrower CrIs and also affects the point estimates.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#conclusion",
    "href": "src/02h_mmrm.html#conclusion",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.6 Conclusion",
    "text": "13.6 Conclusion\nWe can easily fit Bayesian MMRM models using brms including using an unstructured covariance matrix for how the residuals are correlated. As in other cases, a benefit of brms is that we can combine this feature with the many other features available in brms to gain a lot of flexibility in our modelling. With this modeling flexibility we can evaluate varying assumptions which can lead to greater statistical efficiency in our estimation as demonstrated. Given the flexibility of brms, these assumptions can range from just monotonicity up to a full non-linear model for the shape of the dose-response curve.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#exercises",
    "href": "src/02h_mmrm.html#exercises",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "13.7 Exercises",
    "text": "13.7 Exercises\n\n13.7.1 Excercise 1\nAnalyze the following data from the ACTIVE RCT, which compared interventions focused on memory, reasoning and speed of processing with a control group. The data of the ACTIVE trial is available from the US National archive of Computerized Data on Aging and is used as an example for repeated measures on the Advanced Statistics using R website. The outcome variable of interest is the Hopkins Verbal Learning Test (HVLT) total score compared with time 1 (baseline, hvltt column) assessed immediately after training (hvltt2 column), 1 year later (hvltt3 column) and 2 years later (hvltt4 column). Consider how to structure your analysis dataset and covariates might be sensible to include in the model.\n\nactive &lt;- read_csv(\"https://advstats.psychstat.org/data/active.csv\", \n                   col_types = \"iiiiiiiiiiiiii\") %&gt;% \n  mutate(group = factor(group, levels=1:4, \n                        labels=c(\"control\", \"memory\", \"reasoning\", \"speed\"))) %&gt;%\n  relocate(id, group, hvltt, hvltt2, hvltt3, hvltt4)\n\nhead(active)\n\n\n\n13.7.2 Excercise 2\nTry changing the size of the simulated data so that there is 80 (or 90%) power at the one-sided 2.5%, 5% or 10% significance level. Do our priors become more influential as the sample size decreases (compared with a frequentist MMRM fit using the mmrm package)\n\nwhen you use the standard Bayesian MMRM and\nwhen using the MAC (or rMAC) approaches?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#references",
    "href": "src/02h_mmrm.html#references",
    "title": "13  Bayesian Mixed effects Model for Repeated Measures",
    "section": "References",
    "text": "References\n\n\nAllison, David B., Kishore M. Gadde, William Timothy Garvey, Craig A. Peterson, Michael L. Schwiers, Thomas Najarian, Peter Y. Tam, Barbara Troupin, and Wesley W. Day. 2012. “Controlled-Release Phentermine/Topiramate in Severely Obese Adults: A Randomized Controlled Trial (EQUIP).” Obesity 20 (2): 330–42. https://doi.org/10.1038/oby.2011.330.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html",
    "href": "src/02i_time_to_event.html",
    "title": "14  Time-to-event data",
    "section": "",
    "text": "14.1 Background\nThe outcome of interest in clinical trials is often the occurence of an event. Such events might be negative for patients such as a worsening of a disease, hospitalization or death. They might also be a positive such as clearing the blood of malaria parasites, or being released from hospital.\nWe use time-to-event (or survival) analysis when it is important to patients whether the event of interest occurs earlier or later. Additionally, time-to-event methods let us deal with the fact that not all patients will have the event of interest during a trial, which leads to censored observations.\nIn this case study we focus on an Oncology late phase trial evaluating a test treatment in combination with standard of care (SoC) using progression free survival as endpoint in the indication of gastric cancer. Due to geographic differences in the SoC, two different SoC treatments are considered as control treatments. One active treatment is tested in combination with each SoC and compared to the control treatment using only the SoC, which leads to four trial arms in total. From clinical experience with the two SoCs on expects a similar PFS for both therapies. In addition, it is expected that the treatment effect of the active drug is similar when combined with each SoC respectivley. It is therefore desirable to express this prior knowledge when setting up the model.\nBy way of aligning the parametrization of the model we can encode the prior knowledge on expected similarities (no difference between the two SoCs and consistent treatment effect). For example, defining the average treatment effect and the difference between the two treatment effects as parameters, one can place a prior with limited width on the difference parameter. This expresses the a-priori expectation that differences in the treatment effect per SoC are not too large. This results in efficency gains for the estimation of the average treatment effect while only partially pooling the data.\nAs furthermore historical data is available on each SoC treatment, its use may allow for un-equal randomization between active and control arms. However, the available literature data only reports outcomes for a treatment arm which lumps together data from the two SoC treatments. Since about 50% of these patients were treated with either SoC, the reported data can be considered to report the average effect of both SoCs. How this historical data can be included in the analysis is outlined below.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#sec:tte-model-brms",
    "href": "src/02i_time_to_event.html#sec:tte-model-brms",
    "title": "14  Time-to-event data",
    "section": "14.2 Modeling time-to-event data with brms",
    "text": "14.2 Modeling time-to-event data with brms\nModeling time to event data of clinical trials is oftentimes run with the semi-parametric Cox proportional hazards model. This model avoids the need to specify a hazard function \\(h(t)\\), which is the rate of events provided no event as happened yet and it crucially defines the probability density of the survival times. Using the assumption of proportional hazards allows to quantify effects of explanatory variables on the hazard and thereby one can quantify the difference between two treatment groups in a trial, for example. The hazard for a patient \\(i\\) with covariates \\(x_i\\) is often modelled as \\(h_i(t) =\n\\exp\\{x_i \\, \\beta\\} \\, h_0(t)\\). The baseline hazard function \\(h_0(t)\\) is not required in the estimation of the effects \\(\\beta\\) and the Cox model parameters are estimated using a partial likelihood approach. However, as the Bayesian framework is based on the (full) likelihood principle, the semi-parametric Cox proportional hazards model is not applicale. Nonetheless, brms does offer a variant of the Cox proportional hazards model, which is designed to result in numerically very similar results if used with non-informative priors when compared to the respective Cox proportional hazards model. Instead of marginalizing out the baseline hazard function \\(h_0(t)\\), the baseline hazard function is modelled using an almost non-parametric approach based on a parametric spline approximation to \\(h_0(t)\\). The estimated regression coefficients \\(\\beta\\) for the Cox brms family correspond to logarithmic hazard ratios.\nIn contrast to the semi-parametric Cox model, parametric modeling of time to event data assumes a functional form for the hazard function \\(h(t)\\) or equivalently for the probability density function \\(f(t)\\) of the event times at \\(t=T\\). Both functions are related to one another by the basic relationship \\(h(t) = \\frac{f(t)}{S(t)}\\), where \\(S(t) = P(T&gt;t) =\n\\int_t^\\infty f(u)\\, du\\) is the survivor function (probability that an event occurs past time \\(t\\)). In brms the convention is to define via the family argument to brm the probability density of the event times. A description of the parametrization for these densities are found in the vignette “Parametrization of Response Distributions in brms”.\nIn this case study the Weibull distribution is used as literature data suggested its appropiateness and a parametric modeling approach may lead to greater statistical efficiency. The Weibull probability density is parametrized in brms in terms of shape \\(\\alpha\\) and scale parameter \\(s\\). Instead of directly modeling the scale \\(s\\) (as done in many other time to event programs), brms models the mean of the Weibull distribution \\(\\mu\\). Therefore, the scale \\(s\\) is set equal to \\(s=\\frac{\\mu}{\\Gamma(1 + \\frac{1}{\\alpha})}\\) in the Weibull probability density\n\\[ f_{\\mbox{Weibull}}(t) = \\frac{\\alpha}{s} \\left( \\frac{t}{s}\n\\right)^{\\alpha-1} \\, \\exp\\left(-\\left(\\frac{t}{s}\\right)^\\alpha\\right).\\]\nIn this form, the model fulfills the property of an accelerated failure time (AFT) model whenever explanatory variables are introduced. This follows from considering the survivor function\n\\[S_{\\mbox{Weibull}}(t) =\n\\exp\\left(-\\left(\\frac{t}{s}\\right)^\\alpha\\right)\\]\nand modeling the mean \\(\\mu_i\\) linearly on the \\(\\log\\) scale as a function of covariates \\(x_i\\) for subject \\(i\\), \\(\\log(\\mu_i) = \\beta_0 + x_i' \\,\n\\beta\\). Defining as the reference covariate level \\(x_i = 0\\) motivates the definition of a reference survivor function \\(S_{\\mbox{Weibull},0}(t)\\) for which the linear predictor \\(\\log(\\mu_0)\\) is equal to the intercept \\(\\beta_0\\). As a consequence, the survivor function of any patient \\(i\\) is related to the reference survivor function by\n\\[ S_{i}(t) = S_{\\mbox{Weibull},0}\\left(\\frac{t}{\\exp(x_i' \\, \\beta)} \\right),\\]\nwhich is the defining property of AFT models. The regression coefficients \\(\\beta\\) are then interpretable as relative speedup/slowdown of the process progression. That is, an increased time scale (slowdown) leads to a delay of an event. Given that modeling the scale of the Weibull distribution is a common convention (instead of the mean \\(\\mu\\) as in brms), it is useful to recall that this merely means that we need to offset the intercept as estimated by brms with \\(-\\log\\Gamma\\left(1 + \\frac{1}{\\alpha} \\right)\\) in order to obtain the scale on the log scale, \\(\\log(s)\\) (which is the what the survival R package would report with its survreg routine). The survival R package does futhermore use a log-linear representation of the statistical problem leading to an estimation of the inverse shape parameter \\(\\sigma=\\frac{1}{\\alpha}\\).\nThe Weibull AFT model as estimated by brms can be converted into the respective proportional hazard model by transforming the scale \\(s\\) with the relation \\(s = \\lambda^{-\\frac{1}{\\alpha}}\\) to an alternative scale parameter \\(\\lambda\\). Doing so casts the hazard function into\n\\[h_{\\mbox{Weibull}}(t) = \\lambda \\, \\alpha \\, t^{\\alpha-1}.\\]\nWhen we now model \\(\\lambda\\) as a function of covariates \\(x_i\\) linearly on the \\(\\log\\) scale one arrives at\n\\[ h_i(t) = \\exp(x_i' \\, \\xi) \\, h_{\\mbox{Weibull},0}(t),\\]\nwhere \\(h_{\\mbox{Weibull},0}(t)\\) is defined by the reference covariate level \\(x_i=0\\) such that \\(\\lambda = \\exp(\\xi_0)\\) for \\(h_{\\mbox{Weibull},0}(t)\\). To now convert the AFT regression coefficients \\(\\beta\\) as estimated by brms to their respective proportional hazard coefficients \\(\\xi\\) one may just apply the transformation resulting in the relationship\n\\[\\xi = -\\alpha \\, \\beta,\\]\nwhich converts from logarithmic speedup/slowdown \\(\\beta\\) (AFT model) into logarithmic hazard ratios \\(\\xi\\) (proportional hazard model).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#data",
    "href": "src/02i_time_to_event.html#data",
    "title": "14  Time-to-event data",
    "section": "14.3 Data",
    "text": "14.3 Data\nWe demonstrate here the trial analysis using a fake data simulation of the trial design. As key modeling choices for the parametric modeling approach are motivated from the historical data we start with the presentation of the historical data set and then describe the details of the trial simulation.\n\n14.3.1 Historical data\nThe CheckMate 649 trial (Janjigian et al. 2021) included 789 gastric cancer patients which were treated with either chemoA or chemoB and their progression free survival (PFS) was reported. Both chemotherapies were used in a roughly 1:1 ratio such that we will consider the reported data as “average” between both chemo therapies (despite lack of randomization occured wrt to chemotherapy assignment).\nThe data of trial has here been reconstructed from the published survival curves:\n\n\n\n\n\n\n\n\n\nTo now evaluate if the Weibull probability density function is an appropiate choice for this data we consider if the non-parametric estimate of the survivor function is compatible with properties of Weibull distributed event times. Specifically, if we transform the survivor function \\(S_{\\mbox{Weibull}}(t)\\) of the Weibull probability density function with the complementary log-log transformation, we obtain that\n\\[ \\mbox{cloglog}(S_{\\mbox{Weibull}}(t)) = \\log( - \\log(S_{\\mbox{Weibull}}(t))) =\n-\\alpha \\, \\log(s)  + \\alpha \\, \\log(t)\\]\nholds. Therefore, we can visualize an estimate of the survivor function on a transformed scale as a function of \\(\\log(t)\\) and we should observe a straight line with slope \\(\\alpha\\) and intercept \\(-\\alpha \\, \\log(s)\\):\n\nkm &lt;- survfit(Surv(time, status) ~ 1, data=hdata2)\nggsurvplot(km, data=hdata2, fun = \"cloglog\")\n\n\n\n\n\n\n\n\nWhile the line is not perfectly straight over the entire follow-up, the assumption of a straight line within the time period including the bulk of all events falls in the range of a good approximation with a line (as can be seen from the numbers at risk of the plot above).\nAs an additional evaluation of the Weibull distributional form we model the CheckMate 649 data via brms and perform a posterior predictive check. The priors are specified as illustrated in the later section on the used priors. The result looks reasonable.\n\nmodel_weibull_hist &lt;- bf(time | cens(1-status) ~ 1, family=weibull())\n\nprior_weibull_hist &lt;- prior(normal(log(6), log(4)/1.64), class=Intercept) +\n    prior(gamma(3, 2.7), class=shape)\n\nfit_hist_checkmate &lt;- brm(model_weibull_hist,\n                          data=hdata2,\n                          prior=prior_weibull_hist,\n                          seed=234235,\n                          refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.3 seconds.\nChain 2 finished in 1.3 seconds.\nChain 3 finished in 1.3 seconds.\nChain 4 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.3 seconds.\nTotal execution time: 5.5 seconds.\n\nfit_hist_checkmate\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: time | cens(1 - status) ~ 1 \n   Data: hdata2 (Number of observations: 789) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.44      0.04     2.37     2.51 1.00     2794     2455\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.22      0.04     1.14     1.30 1.00     2865     2574\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWith the posterior we can now perform a posterior predictive check. We do so by drawing 100 samples of the model parameters from the posterior and draw for each sample a realization of the event times for all patients. Hence, for each sample one obtains a fake data set simulated according to the Weibull distribution. Finally, each fake data set is summarized by a Kaplan-Meier estimate and drawn in light color while the actually observed data is summarized in the same way and drawn using a thick line. If the fitted model is appropiate, then the observed Kaplan Meier estimate should look just like a random realization among the many fake data generated ones. However, we observe that over the entire follow-up time the observed data is leaving for late follow-up times the bulk of the fake data generated Kaplan Meier curves. This may suggest a deviation from the Weibull distribution or the possibility that time-varying covariates are helpful. Nonetheless, if one focusses on the follow-up time frame, which includes the majoritiy of events (up to 15 month), then the posterior predictive check does look reasonable. This is why we use the Weibull distribution without a time-varying covariate for simplicity.\n\np_full_fup &lt;- pp_check(fit_hist_checkmate, status_y=hdata2$status,\n             type=\"km_overlay\", ndraws=100) +\n    scale_y_continuous(breaks=seq(0,1,by=0.1)) +\n  xlab(\"Time [month]\") + coord_cartesian(xlim=c(0, 35))\np_sub_fup &lt;- p_full_fup + coord_cartesian(xlim=c(0, 15))\nggpubr::ggarrange(p_full_fup, p_sub_fup, common.legend = TRUE)\n\n\n\n\n\n\n\n\nNote that the above predictive check does not simulate the censoring process. Thus, the censoring process is ignored and non-informative censoring is thereby assumed.\n\n\n14.3.2 Trial simulation\nHere we consider a fake data simulation of the trial of interest evaluating a novel treatment of gastric cancer for which an established treatment is available already. The trial is a randomized trial to study the efficacy and safety of adding a drug CompoundX in combination with a monoclonal antibody mAb + standard of care (SoC). Two chemotherapy backbones, ChemoA and ChemoB were used as SoC. There are in total four treatment arms including two control groups, which are mAb+ChemoA (controlChemoA) and mAb+ChemoB (controlChemoB), and two corresponding active arms with the test treatment administrated in addition, that is CompoundX+mAb+ChemoA (activeChemoA), CompoundX+mAb+ChemoB (activeChemoB).\nSynthetic data were generated for the randomised study. We let the sample size of both active and control group be 100, respectively. The sample size has been chosen to ensure adequate trial power of 80%. Efficacy was assessed using progression-free survival (PFS) measured in units of months. For patients not experiencing a progression event at the time of the analysis, their event time will be right censored at the time of their last valid tumor assessment. For patients experiencing an event, their event time will be interval censored, with the upper limit of the interval being the study day on which the progression event is observed, and the lower limit being the day of the last preceding valid tumor assessment at which the patient’s progression-free status was confirmed. While interval censoring is a supported feature of brms, this case study will for simplicity use the actual event times and leave it to the reader to explore interval censoring with brms.\nThere are two historical datasets available. The first one with 400 patients on a control arm, mAb+ChemoA (controlChemoA), is simulated together with the randomised study. And the PFS data of 789 patients receiving first-line programmed cell death (PD-1) inhibitor+chemotherapy are available from a randomised, open-label, phase 3 trial, CheckMate 649 (Janjigian et al. 2021), and it will be introduced later. These data are highly relevant and we would like to integrate the historical information into our analysis.\nSimulated data will be generated using simsurv function via simulating survival times from the Weibull model using the proportional hazard form of the model. We assumed a 20% hazard rate (HR) reduction due to active treatment administration (CompoundX) to the active group in comparison to the control group and a 5% difference in HR for the two chemotherapy SoCs (ChemoB is 5% worse than ChemoA on the hazard scale). We also assumed that both of the historical data have a 2% worse outcome on the hazards scale and we represent the two studies with hist1 and hist2 respectively (hist1 for simulated historidal data and hist2 for CheckMate 649).\n\n\nShow the code\nset.seed(46767)\n# n per group\nn_grp &lt;- 100\nn_hist &lt;- 400\n# use month as time-unit\nrate_1 &lt;- 1 / 6\nrate_cens &lt;- 1 / 10\nbeta &lt;- c(trt=-0.2,        ## roughly 20% HR reduction\n          soc_alt=0.05,    ## alternative chemotherapy is 5% worse\n          hist1=0.02)      ## simulated historical data has a 2% worse outcome\n# covariates of simulated trial data\ncovs &lt;- data.frame(id = seq(1, 2*n_grp), trt = c(0L, 1L),\n                   soc_alt=rbinom(2*n_grp, 1L, 0.3), hist1=0L, hist2=0L)\n\n# covariates of historical data\nhcovs &lt;- data.frame(id = seq(2*n_grp+1, 2*n_grp+1 +n_hist - 1),\n                    trt = c(0), soc_alt=0, hist1=1L, hist2=0L)\nsimulate_trial &lt;- function(lambda, gamma, lambda_cens, x, betas) {\n    ## simulate censoring times, note that we do not simulate end of\n    ## trial with maxt for now. Also note the different parametrization of\n    ## simsurv which models log(shape) with a linear predictor.\n    cens &lt;- simsurv(lambdas = lambda_cens, gammas = 1, x=x)\n    events &lt;- simsurv(lambdas = lambda, gammas = gamma, x=x, betas=betas)\n    names(cens) &lt;- paste0(names(cens), \"_cens\")\n    bind_cols(events, select(cens, -id_cens), select(x, -id)) %&gt;%\n        rename(censtime=eventtime_cens) %&gt;%\n    mutate(event=1L*(eventtime &lt;= censtime),\n           y=if_else(event==1, eventtime, censtime),\n           status=NULL, status_cens=NULL) %&gt;%\n        relocate(id, y, event) %&gt;%\n        mutate(soc=factor(soc_alt, c(0, 1), c(\"ChemoA\", \"ChemoB\")),\n               trt_ind=trt,\n               trt=factor(trt_ind, c(0,1), c(\"control\", \"active\")),\n               arm=factor(paste0(c(\"control\", \"active\")[trt_ind + 1], soc),\n                          levels=c(\"activeChemoA\", \"controlChemoA\",\n                                   \"activeChemoB\", \"controlChemoB\")))\n}\n\n# using shape=1 for simulation (corresponding to exponential survival times)\nsim &lt;- simulate_trial(rate_1, 1, rate_cens, covs, beta)\nhdata1 &lt;- simulate_trial(rate_1, 1, rate_cens, hcovs, beta) %&gt;%\n  mutate(id=id+max(sim$id))\n\n\nWe can check how the simulated data looks like and its sample size.\n\ngt_preview(sim) %&gt;% fmt_number(where(is.double), decimals=1)\n\n\n\n\n\n\n\n\nid\ny\nevent\neventtime\ncenstime\ntrt\nsoc_alt\nhist1\nhist2\nsoc\ntrt_ind\narm\n\n\n\n\n1\n1\n7.7\n0\n12.3\n7.7\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n2\n2\n0.1\n0\n20.4\n0.1\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n3\n3\n4.7\n0\n10.3\n4.7\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n4\n4\n2.7\n0\n19.0\n2.7\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n5\n5\n3.6\n1\n3.6\n7.5\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n6..199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n200\n200\n11.8\n0\n17.1\n11.8\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n\n\n\n\ntable(sim$arm)\n\n\n activeChemoA controlChemoA  activeChemoB controlChemoB \n           72            73            28            27 \n\ntable(hdata1$arm)\n\n\n activeChemoA controlChemoA  activeChemoB controlChemoB \n            0           400             0             0",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#models",
    "href": "src/02i_time_to_event.html#models",
    "title": "14  Time-to-event data",
    "section": "14.4 Models",
    "text": "14.4 Models\n\n14.4.1 Model specification\nSurvival data will be fitted by a Bayesian generalized linear model.\nThe survival function follows a Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(s\\),\n\\[T|\\alpha,s \\sim f_{\\mbox{Weibull}}(T).\\]\nA common shape parameter \\(\\alpha\\) is assumed for all treatment arms, with the scale parameter allowed to vary by treatment arm. In place of directly modeling the scale parameter \\(s\\), the mean of the Weibull distribution is modelled on the logarithmic scale such that the scale is set to \\(s = \\frac{\\mu}{\\Gamma(1 + \\frac{1}{\\alpha})}\\) and \\(\\log(\\mu)\\) is modelled as a linear function of the covariates. We use \\(\\lambda_{\\mbox{activeChemoA}}\\) to represent the logarithm of the mean survival time for patients receiving CompoundX+mAb+ChemoA (\\(\\lambda_{\\mbox{activeChemoA}} = \\log(\\mu_{\\mbox{activeChemoA}})\\)), and \\(\\lambda_{\\mbox{controlChemoA}}\\) for patients receiving mAb+ChemoA. We define \\(\\lambda_{\\mbox{activeChemoB}}\\) and \\(\\lambda_{\\mbox{controlChemoB}}\\) similarly for patients receiving the ChemoB chemotherapy backbone.\nWith 4 different patient groups we may define 4 parameters in total. The default parametrization in R for categorical variables is the treatment contrast representation. This representation delcares a reference category to form the overall intercept and defines differences to the overall intercept for each categorical level. As this does automatically define the parametrization of the model, we here define the contrasts in a customized manner. Doing so ensures that we control directly the exact parametrization of the model. This is important in this problem as we can then setup priors in a tailored manner. More details in defining custom contrasts are explained by Ben Bolker here.\nIn addition to the overall mean (global intercept)\n\\[\\gamma = \\frac{1}{4}( \\lambda_{\\mbox{activeChemoA}} +\n\\lambda_{\\mbox{controlChemoA}} + \\lambda_{\\mbox{activeChemoB}} +\n\\lambda_{\\mbox{controlChemoB}} ), \\]\nwe define for the four groups the following contrasts of interest:\n\nAverage difference between the active and control arms in each chemotherapy backbone:\n\n\\[\\delta_{\\mbox{EffectAvg}} = \\frac{1}{2} {\n(\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) + (\n\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}} ) } \\]\n\nHalf of the difference in treatment effect between the two chemotherapy backbones:\n\n\\[\\delta_{\\mbox{effect}} = \\frac{1}{2} { (\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) - ( \\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}}) }\\]\n\nDifference between the two control arms:\n\n\\[\\delta_{\\mbox{control}} = - \\lambda_{\\mbox{controlChemoA}}  +\n\\lambda_{\\mbox{controlChemoB}} \\]\nTo formalize these contrast definitions, we summarize the arms to parameter mapping as matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n$\\lambda_{\\mbox{activeChemoA}}$\n$\\lambda_{\\mbox{controlChemoA}}$\n$\\lambda_{\\mbox{activeChemoB}}$\n$\\lambda_{\\mbox{controlChemoB}}$\n\n\n\n\n\\(\\gamma\\)\n1/4\n1/4\n1/4\n1/4\n\n\n\\(\\delta_{\\mbox{EffectAvg}}\\)\n1/2\n-1/2\n1/2\n-1/2\n\n\n\\(\\delta_{\\mbox{effect}}\\)$\n1/2\n-1/2\n-1/2\n1/2\n\n\n\\(\\delta_{\\mbox{control}}\\)\n0\n-1\n0\n1\n\n\n\n\n\n\n\nIn this form it is straightforward to derive the definition of a parameter and how it relates to the strata of the data based on the parameter definitions. However, in order to define the design matrix, which maps parameters to data strata, we will need the inverse of the above matrix. By convention R requires for the specification of custom contrats, the matrix mapping parameters to the data means such that the above matrix is referred to as \\(C^{-1}\\) in the following.\nThe linear model for \\(\\log(\\mu)\\) is parameterised in terms of the defined contrasts with additional parameters allowing for heterogeneity between data from the two separate historical data strata. For the 4 different treatment arms the linear predictor is then given by:\n\\[\\begin{align}\n\\begin{bmatrix}\n\\lambda_{\\mbox{activeChemoA}} \\\\\n\\lambda_{\\mbox{controlChemoA}} \\\\\n\\lambda_{\\mbox{activeChemoB}} \\\\\n\\lambda_{\\mbox{controlChemoB}}\n\\end{bmatrix}\n&= C \\,\n\\begin{bmatrix}\n    \\gamma \\\\\n    \\delta_{\\mbox{EffectAvg}} \\\\\n    \\delta_{\\mbox{effect}} \\\\\n    \\delta_{\\mbox{control}}\n        \\end{bmatrix}\n+ I_{\\mbox{hist1}} \\,\n\\beta_{\\mbox{hist1}} + I_{\\mbox{hist2}} \\, \\beta_{\\mbox{hist2}}\n\\end{align}\\]\nThe indicators \\(I_{\\mbox{hist1}}\\) and \\(I_{\\mbox{hist2}}\\) represent that a patient was in the simulated historical stratum, or in the historical CheckMate 649 stratum, respectively. The corresponding parameters \\(\\beta_{\\mbox{hist1}}\\) and \\(\\beta_{\\mbox{hist2}}\\) capture differences in outcome between these two historical strata, and patients in our randomised comparison. In order to borrow strength from the historical data, the prior on these two regression coefficients are set reasonably narrow which in effect emulates a random effects meta-analysis.\nAs the historical data set reported in CheckMate 649 reported the outcome for the control treatment with a mixed population of 50% being on either SoC, we consider this data to report an average SoC effect. Basic algebra shows that \\[\\frac{1}{2} \\,\n(\\lambda_{\\mbox{activeChemoA}} + \\lambda_{\\mbox{activeChemoB}}) = \\gamma -\n\\frac{1}{2} \\, \\delta_{\\mbox{effect}}. \\] Thus, we may simply add an additional row to the matrix \\(C\\) such that the full matrix \\(C\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment arm\n$\\gamma$\n$\\delta_{\\mbox{EffectAvg}}$\n$\\delta_{\\mbox{effect}}$\n$\\delta_{\\mbox{control}}$\n\n\n\n\nactiveChemoA\n1\n1/2\n1\n-1/2\n\n\ncontrolChemoA\n1\n-1/2\n0\n-1/2\n\n\nactiveChemoB\n1\n1/2\n-1\n1/2\n\n\ncontrolChemoB\n1\n-1/2\n0\n1/2\n\n\ncontrolAverage\n1\n-1/2\n0\n0\n\n\n\n\n\n\n\n\n\n14.4.2 Priors\nPriors for each random variable are described in the following table.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPrior\n\n\n\n\n\\(\\alpha\\)\n\\(\\Gamma(3, 2.7)\\)\n\n\n\\(\\gamma\\)\n\\(\\mbox{Normal}( \\log(\\frac{8}{\\log(2)}), \\frac{\\log(4)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{effectAvg}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(2)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{effect}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.25)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{control}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.25)}{1.64})\\)\n\n\n\\(\\beta_{\\mbox{hist1}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.8)}{1.64})\\)\n\n\n\\(\\beta_{\\mbox{hist2}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.8)}{1.64})\\)\n\n\n\n\n\n\n\nThese priors parametrize the central 90% credible interval for the respective normal priors used. As the parameters are additive on the \\(\\log\\) scale, the standard deviation of \\(\\log(2)/\\Phi^{-1}(95\\%)\n\\approx \\log(2)/1.64\\) defines a 90% credible interval of \\([-\\log(2), \\log(2)]\\) and implies on the back-transformed scale a respective interval of \\([1/2, 2]\\), i.e. a doubling or a halving of the progression speed.\nThe common shape parameter, \\(\\alpha\\), is set to have median value of unity and it’s 90% credible interval is \\([0.3, 2.33]\\), which admits substantial deviation from unity, which is the case of exponential survival time distribution.\nThe prior for \\(\\gamma\\), the intercept, is set with a mean approximately corresponding to expected median PFS on anti-PD-1 with SoC, as estimated from CheckMate 649 (approx. 8 months). The width of the central 90% credible interval for the intercept prior \\(\\gamma\\) is set to admit for a 4 fold increase or decrease. A priori it is expected that the outcome for controlChemoAand controlChemoB will be similar (Janjijian et al, mPFS by chemotherapy not provided, but mOS was 14 months for both anti-PD1+Chemotherapy backbones (considered as similar to mAb+ChemoA andmAb+ChemoB in our simulated data), hence a prior for \\(\\delta_{\\mbox{control}}\\) is centered on 0 with a standard deviation corresponding to a 25% increase or decrease is chosen for the 90% credible interval. Similarly, the treatment benefit from adding CompoundX is expected to be similar for both arms, so the prior for \\(\\delta_{\\mbox{effect}}\\) is again centered on 0 with the prior admitting a 25% increase or decrease a priori in terms of the 90% central credible interval. Priors for \\(\\beta_{\\mbox{hist1}}\\) and \\(\\beta_{\\mbox{hist2}}\\) are centered at a mean of 0, corresponding to no difference in outcome for the two historical studies as compared to this trial’s randomised part, but with the same variance allowing substantial differences between the randomised study and historical data. The use of Student-t priors for these parameters also robustifies against possible data discordance between the randomised comparison and historical data.\nThese prior definitions provide an uninformative prior, that allows for a broad range of shapes for the survival curve.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#implementationresults",
    "href": "src/02i_time_to_event.html#implementationresults",
    "title": "14  Time-to-event data",
    "section": "14.5 Implementation/Results",
    "text": "14.5 Implementation/Results\n\n14.5.1 Construct customized coded contrasts\nFirst we define customized contrasts which define the parametrization of the model as defined in the Model sepcification section.\nThe mapping of data strata to parameters is:\n\ncc_inv &lt;- matrix(c(1/4,  1/4,  1/4,  1/4,\n                   1/2, -1/2,  1/2, -1/2,\n                   1/2, -1/2, -1/2,  1/2,\n                   0,   -1,    0,    1),\nnrow=4, ncol=4, byrow=TRUE,\ndimnames=list(contrast=c(\"intercept\", \"deltaEffectAvg\",\n                         \"deltaEffect\", \"deltaControl\"),\n              arm=c(\"activeChemoA\", \"controlChemoA\",\n                    \"activeChemoB\", \"controlChemoB\")))\nMASS::fractions(cc_inv)\n\n                arm\ncontrast         activeChemoA controlChemoA activeChemoB controlChemoB\n  intercept       1/4          1/4           1/4          1/4         \n  deltaEffectAvg  1/2         -1/2           1/2         -1/2         \n  deltaEffect     1/2         -1/2          -1/2          1/2         \n  deltaControl      0           -1             0            1         \n\n\nintercept (\\(\\gamma\\)) is the overall mean while deltaControl (\\(\\delta_{\\mbox{control}}\\)) is the difference between the two control arms.\nBy way of defining deltaEffectAvg (\\(\\delta_{\\mbox{EffectAvg}}\\)) we obtain the average treatment effect. Setting deltaEffect (\\(\\delta_{\\mbox{effect}}\\)) to half of the difference between the two treatment effects one arrives at the following symmetric relations to obtain the chemo therapy specific treatment effect:\n\\[\\delta_{\\mbox{EffectAvg}} = \\frac{1}{2} \\, [\n(\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) +\n(\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}})]\\]\n\\[\\delta_{\\mbox{effect}} = \\frac{1}{2} \\, [ (\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) - (\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}})]\\]\n\\[ \\Leftrightarrow \\delta_{\\mbox{effectChemoA}}  =\n\\delta_{\\mbox{effectAvg}} + \\delta_{\\mbox{effect}}\\]\n\\[ \\Leftrightarrow \\delta_{\\mbox{effectChemoB}}  = \\delta_{\\mbox{effectAvg}} - \\delta_{\\mbox{effect}}\\]\nR requires to define the inverse relationship, i.e. the mapping from parameters to data strata (as needed to define the design matrix). Therefore, the contrast matrix \\(C\\) is:\n\ncc &lt;- solve(cc_inv)\nMASS::fractions(cc)\n\n              intercept deltaEffectAvg deltaEffect deltaControl\nactiveChemoA     1       1/2              1        -1/2        \ncontrolChemoA    1      -1/2              0        -1/2        \nactiveChemoB     1       1/2             -1         1/2        \ncontrolChemoB    1      -1/2              0         1/2        \n\n\nIn order to setup custom contrasts of discrete explanatory variables like treatment arms, R requires to define the explanatory variable as factor, which has been defined accordingly in the simulation routine of this case study:\n\nlevels(sim$arm)\n\n[1] \"activeChemoA\"  \"controlChemoA\" \"activeChemoB\"  \"controlChemoB\"\n\n\nAs R sets up the overall intercept separatley, we need to drop the overall intercept definition from \\(C\\) and define the contrast of the factor variable to be equal to the matrix \\(C\\):\n\ncontrasts(sim$arm) &lt;- cc[,-1]\n\nThen to check if the contrasts work as expected, we can use a dummy fit to see if things get used correctly and the results here are consistent with our definitions:\n\nlfit1 &lt;- lm(y ~ 1 + arm, sim)\nsummary(lfit1)\n\n\nCall:\nlm(formula = y ~ 1 + arm, data = sim)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.506 -2.909 -1.461  1.261 17.064 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         4.5023     0.3349  13.444   &lt;2e-16 ***\narmdeltaEffectAvg   0.9641     0.6698   1.439    0.152    \narmdeltaEffect     -0.4516     0.6698  -0.674    0.501    \narmdeltaControl     0.6574     0.9526   0.690    0.491    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.229 on 196 degrees of freedom\nMultiple R-squared:  0.02433,   Adjusted R-squared:  0.009398 \nF-statistic: 1.629 on 3 and 196 DF,  p-value: 0.1839\n\n\nWe can see that above model specification for lm estimates the parameter coefficient for the three contrasts armdeltaEffectAvg (\\(\\delta_{\\mbox{EffectAvg}}\\)), armdeltaEffect (\\(\\delta_{\\mbox{effect}}\\)) and armdeltaControl (\\(\\delta_{\\mbox{control}}\\)).\n\n\n14.5.2 Model fit with brms\nNow we start with analyzing the simulated randomized study and specify the brms model using a Weibull distribution. In a first version, we do not include the literature historical data which requires the definition of a custom design matrix as will be explained below.\nFirst we define with a call to bf the model formula and model family. This defines the model’s likelihood and the parametrization.\n\nmodel_weibull &lt;- bf(y | cens(1-event) ~ 1 + arm, family=weibull())\n\nThe left hand side of the formula, y | cens(1-event) ~ ..., denotes with \\(y\\) the data being modeled - the time variable -, while cens is a response modifier added with a vertical bar| to the left hand side. Here, cens defines which data rows correspond to (right) censored observations. The censoring information is passed into cens by the variable 1-event. The right hand side defines the linear regressor formula used for the mean parameter of the model, which is in this case modeled by default with a \\(\\log\\) link function. The formula 1 + arm defines an overall intercept and the covariate arm (coded as factor in the data) is used with it’s custom constrasts as explanatory variable. Finally, the family argument specifies that the event and censoring times stored in \\(y\\) are distributed according to a weibull probability density distribution.\nWith the model (and data) being defined, we are left to specify the priors for the model. With the help of the get_prior function we can report which default priors and parameters were instantiated by brms and hence need an assignment of a prior.\n\ngt(get_prior(model_weibull, sim))\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\nHere we define the priors with standard deviations stored in variables, which are passed as stanvars to the brms model. Doing so allows to change the prior quickly, since the Stan model as generated by brms remains unchanged whenever the standard deviation of a prior is changed. A priori it is expected that the outcome for the two control groups will be similar, so the prior on deltaControl (\\(\\delta_{\\mbox{control}}\\)) is centered on \\(0\\). The same is a priori assumed for the deltaEffect (\\(\\delta_{\\mbox{effect}}\\)) parameter.\n\nmodel_weibull_prior &lt;- prior(normal(meanInter, log(4)/1.64),\n                             class=\"Intercept\") +\n  prior(normal(0, sdEffectAvg), coef=armdeltaEffectAvg) +\n  prior(normal(0, sdDeltaEffect), coef=armdeltaEffect) +\n  prior(normal(0, sdDeltaControl), coef=armdeltaControl) +\n  prior(gamma(3, 2.7), class=shape)\n\nThe prior for the intercept, \\(\\gamma\\), is set to a mean approximately corresponding to an expected median PFS on anti-PD-1 + SoC, as estimated from CheckMate 649 (here 8 month)\n\nprior_mean &lt;- stanvar(log(8/log(2)), \"meanInter\")\n\nThe standard deviations are setup with reference to a \\(90 \\%\\) credible interval. Hence, \\(log(2)/1.64\\) for the standard deviation of the prior for deltaEffectAvg (\\(\\delta_{\\mbox{effectAvg}}\\)) means that the event process can increase/decrease in progression speed by a factor of \\(2\\) at most.\n\nprior_sd &lt;- stanvar(log(2)/1.64, \"sdEffectAvg\") +\n  stanvar(log(1.25)/1.64, \"sdDeltaEffect\")+\n  stanvar(log(1.25)/1.64, \"sdDeltaControl\")\n\nNow the model and prior specifications are complete and we are ready to run the model in brms. First fit the model using the randomized study data\n\nfit_weibull &lt;- brm(model_weibull, data=sim, prior=model_weibull_prior,\n               stanvars=prior_sd + prior_mean,\n               seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.5 seconds.\n\n\nWhen looking at the model summary, it is preferable to use robust=TRUE to get the median estimate of the posterior rather than the mean (which would change due to transforms)\n\nsummary(fit_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm \n   Data: sim (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.16      0.12     1.98     2.38 1.00     3998     2848\narmdeltaEffectAvg     0.26      0.19    -0.05     0.58 1.00     3806     2833\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4514     3328\narmdeltaControl       0.06      0.12    -0.13     0.26 1.00     4265     3056\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     0.97      0.08     0.85     1.10 1.00     4014     3098\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSummarize posterior draws based on point estimates, estimation errors and quantiles. And we transform the estimates to the original scale\n\npost_sum &lt;- posterior_summary(fit_weibull, prob=c(0.05, 0.95), robust=TRUE)\nprint(exp(post_sum[,\"Estimate\"]), digits=2)\n\n        b_Intercept b_armdeltaEffectAvg    b_armdeltaEffect   b_armdeltaControl               shape           Intercept \n            8.6e+00             1.3e+00             1.0e+00             1.1e+00             2.6e+00             8.5e+00 \n             lprior                lp__ \n            1.0e+00            5.4e-140 \n\n\nThese estimates are the AFT regression coefficients such that the effect of treatment is to slowdown the time to an event and thus the treatment effect estimate \\(\\delta_{\\mbox{effectAvg}}\\) is positive. To convert to hazard ratios, one needs to multiply with the negative of the estimated shape parameter. Doing so for all 3 regression coefficients by using the rvar facility (provided from the posterior R package) yields:\n\npost_rv &lt;- as_draws_rvars(fit_weibull)\npost_rv_HR &lt;- with(post_rv, exp(-1 * shape * c(b_armdeltaEffectAvg, b_armdeltaEffect, b_armdeltaControl)))\nnames(post_rv_HR) &lt;- c(\"deltaEffectAvg\", \"deltaEffect\", \"deltaControl\")\nprint(summarize_draws(post_rv_HR), digits=2)\n\n# A tibble: 3 × 10\n  variable                    mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;                      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 post_rv_HR[deltaEffectAvg] 0.788  0.777 0.146 0.145 0.568  1.05  1.00    3872.    2876.\n2 post_rv_HR[deltaEffect]    1.00   0.995 0.104 0.103 0.839  1.18  1.00    4515.    3209.\n3 post_rv_HR[deltaControl]   0.950  0.947 0.110 0.107 0.777  1.13  1.00    4282.    2919.\n\n\nObtain posterior mean estimates (mean rate). Median of the posterior estimates indicated a roughly 20% increase in survival in activeChemoA compared to controlChemoA.\n\nnd &lt;- data.frame(y=0, event=0, arm=levels(sim$arm))\npost_mean &lt;- posterior_epred(fit_weibull, newdata=nd)\ncolnames(post_mean) &lt;- nd$arm\nquantile(post_mean[,\"activeChemoA\"] / post_mean[,\"controlChemoA\"],\n         probs=c(0.05, 0.5, 0.95))\n\n       5%       50%       95% \n0.9425351 1.3043160 1.8179356 \n\n\nNow extract posterior coefficients with a 90% credible interval\n\npost_est &lt;- fixef(fit_weibull, prob=c(0.05, 0.95), robust=TRUE)\ngt(as.data.frame(post_est)) %&gt;% fmt_number(everything(), decimals=2)\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ5\nQ95\n\n\n\n\n2.16\n0.12\n1.98\n2.38\n\n\n0.26\n0.19\n−0.05\n0.58\n\n\n0.01\n0.11\n−0.17\n0.18\n\n\n0.06\n0.12\n−0.13\n0.26\n\n\n\n\n\n\n\nDefine trial successfull if the deltaEffectAvg exceed 0 (which means the survival time is increased) and the result is not significant here.\n\npost_est[\"armdeltaEffectAvg\",\"Q5\"] &gt; 0\n\n[1] FALSE\n\n\n\n\n14.5.3 Incoporate simulated historical data\nSince we have the simulated historical data hist1 on one control arm, we can incorporate the information by adding the corresponding covariate hist1 (indicator variable being \\(1\\) for the historical study and \\(0\\) otherwise) when specifying the formula for meta-analytic-combined (MAC) analysis:\n\nmodel_mac_weibull &lt;- bf(y | cens(1-event) ~ 1 + arm + hist1, family=weibull())\n\nAnd get_prior function shows that in addition to the priors for the weibull model on randomised data, the co-data model needs one more prior on the historical data.\n\ngt(get_prior(model_mac_weibull, sim))\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\n\nb\nhist1\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\nWe use a student with 6 degrees of freedom for the regression coefficient for the historical data prior. Using a Student-t distribution robustifies against possible data discordance between the randomised study and the historical data, since very large deviations between the two data sets are admitted by the heavy tails of the prior. The prior is centered at a mean of \\(0\\), corresponding to no expected difference in the outcome for historical and current study.\n\nmodel_weibull_mac_prior &lt;- model_weibull_prior +\n  prior(student_t(6, 0, sdHist), coef=hist1)\n\nA variance allowing substantial differences, a \\(1.8\\) fold of increase/decrease, between the randomised study and historical data is set.\n\nprior_sdHist &lt;- stanvar(log(1.8)/1.64, \"sdHist\")\n\nNow we combine the simulated trial data and historical data to fit the co-data MAC model. Don’t forget to set contrasts of arm in the combined data to the contrast matrix.\n\ncomb_data &lt;- bind_rows(sim, hdata1)\ncontrasts(comb_data$arm) &lt;- cc[,-1]\nfit_mac_weibull &lt;- brm(model_mac_weibull, data=comb_data,\n                       prior=model_weibull_mac_prior,\n                   stanvars=prior_sd + prior_mean + prior_sdHist,\n                   seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.7 seconds.\nChain 2 finished in 1.7 seconds.\nChain 3 finished in 1.6 seconds.\nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 7.1 seconds.\n\n\nAnd we can compare the results from two models. Inclusion of historical data on controlChemoA affect the estimates of coefficients for overall mean and EffectAverage.\n\nsummary(fit_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm \n   Data: sim (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.16      0.12     1.98     2.38 1.00     3998     2848\narmdeltaEffectAvg     0.26      0.19    -0.05     0.58 1.00     3806     2833\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4514     3328\narmdeltaControl       0.06      0.12    -0.13     0.26 1.00     4265     3056\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     0.97      0.08     0.85     1.10 1.00     4014     3098\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit_mac_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm + hist1 \n   Data: comb_data (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.30 1.00     4909     3769\narmdeltaEffectAvg     0.29      0.18    -0.01     0.59 1.00     4051     2881\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4748     3261\narmdeltaControl       0.07      0.12    -0.13     0.27 1.00     4343     2769\nhist1                -0.20      0.14    -0.43     0.02 1.00     4039     2944\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.94     1.07 1.00     4712     3074\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNext, we check the informativeness of the historical data by only including the historical data. Note that we have to request that factor levels for which no realization is present in the data set must be kept, since only the controlChemoA arm is present in the historical data.\n\ncontrasts(hdata1$arm) &lt;- cc[,-1]\nfit_mac_prior_weibull &lt;- brm(model_mac_weibull, data=hdata1,\n                             prior=model_weibull_mac_prior,\n                             drop_unused_levels=FALSE,\n                         stanvars=prior_sd + prior_mean + prior_sdHist,\n                         seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.1 seconds.\nChain 2 finished in 1.1 seconds.\nChain 3 finished in 1.1 seconds.\nChain 4 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.1 seconds.\nTotal execution time: 4.8 seconds.\n\ngt(as.data.frame(fixef(fit_mac_prior_weibull, robust=TRUE))) %&gt;% fmt_number(everything(), decimals=2)\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\n1.73\n0.47\n0.76\n2.69\n\n\n0.01\n0.42\n−0.80\n0.85\n\n\n0.00\n0.14\n−0.28\n0.27\n\n\n0.00\n0.14\n−0.27\n0.27\n\n\n0.01\n0.38\n−0.86\n0.87\n\n\n\n\n\n\n\nWe see that we essentially sample the prior for most regression coefficients, but we do substantially reduce the standard error of the intercept estimate which is almost halved in comparison to the prior standard deviation of 0.85.\n\n\n14.5.4 Including historical data of average SoC\nSo far we have setup a custom parametrization allowing to define priors aligned with prior expectations (small difference of the treatment effect and small difference between SoC). However, the CheckMate 649 historical data reports the effect of an average SoC treatment. Thus, the data of CheckMate 649 is incompatible with the 4 defined categorical factor levels so far as it does not correspond to any of the 4 treatment arms in the current data set. However, as the custom factor contrasts and factor level definitions are translated by R into a design matrix, we can customize the created design matrix directly. This requires to setup the design matrix manually giving us the freedom to define the design matrix appropiatley for the CheckMate 649 data set.\nThere is an alternative way to use the customized contrast and setup analysis by using respective indicator flags, which is created using model.matrix. The model.matrix function is used internally by brms to create the design matrix for the regression problem\n\nXind &lt;- model.matrix(~ 1 + arm, comb_data)\ngt_preview(Xind)\n\n\n\n\n\n\n\n\n(Intercept)\narmdeltaEffectAvg\narmdeltaEffect\narmdeltaControl\n\n\n\n\n1\n1\n-0.5\n0\n-0.5\n\n\n2\n1\n0.5\n1\n-0.5\n\n\n3\n1\n-0.5\n0\n-0.5\n\n\n4\n1\n0.5\n1\n-0.5\n\n\n5\n1\n-0.5\n0\n-0.5\n\n\n6..599\n\n\n\n\n\n\n600\n1\n-0.5\n0\n-0.5\n\n\n\n\n\n\n\nBy adding the generated design matrix to the original data set, we can then simply delcare the model formula using these created indicator variables:\n\ncomb_data_ind &lt;- cbind(comb_data, Xind[,-1])\n\nThus we use those indicator variables in the fit as if these were continuous covariates which replaces the use of the categorical factor variable arm used in the first approach.\n\nmodel_mac_weibull_ind &lt;- bf(y | cens(1-event) ~ 1 + armdeltaEffectAvg +\n                            armdeltaEffect + armdeltaControl + hist1,\n                            family=weibull())\n\nAnd query what prior parameters need to be set\n\ngt(get_prior(model_mac_weibull_ind, comb_data_ind))\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\n\nb\nhist1\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\nFit the model using indicator flags with the same priors as before and compare with the previous one fitted with arm variable, the results are the same.\n\nmodel_weibull_mac_prior_ind &lt;- prior(normal(meanInter, log(4)/1.64),\n                                     class=\"Intercept\") +\n    prior(normal(0, sdEffectAvg), coef=armdeltaEffectAvg) +\n    prior(normal(0, sdDeltaEffect), coef=armdeltaEffect) +\n    prior(normal(0, sdDeltaControl), coef=armdeltaControl) +\n    prior(normal(0, sdHist), coef=hist1)\n\nfit_mac_weibull_ind &lt;- brm(model_mac_weibull_ind, data=comb_data_ind,\n                           prior=model_weibull_mac_prior_ind,\n                       stanvars=prior_sd + prior_mean + prior_sdHist,\n                       seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.8 seconds.\nChain 2 finished in 1.6 seconds.\nChain 3 finished in 1.7 seconds.\nChain 4 finished in 1.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 7.1 seconds.\n\nsummary(fit_mac_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm + hist1 \n   Data: comb_data (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.30 1.00     4909     3769\narmdeltaEffectAvg     0.29      0.18    -0.01     0.59 1.00     4051     2881\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4748     3261\narmdeltaControl       0.07      0.12    -0.13     0.27 1.00     4343     2769\nhist1                -0.20      0.14    -0.43     0.02 1.00     4039     2944\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.94     1.07 1.00     4712     3074\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit_mac_weibull_ind, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 \n   Data: comb_data_ind (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.29 1.00     5075     3553\narmdeltaEffectAvg     0.28      0.18     0.00     0.57 1.00     4184     3537\narmdeltaEffect        0.01      0.10    -0.17     0.17 1.00     4688     3177\narmdeltaControl       0.07      0.11    -0.12     0.25 1.00     5187     3251\nhist1                -0.19      0.14    -0.43     0.02 1.00     4334     3137\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.93     1.07 1.00     4680     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that the equality holds exactly for the entire posterior, since the very same problem (using the same random seed) has been fitted by brms:\n\npost_mac &lt;- as_draws_matrix(fit_mac_weibull)\npost_mac_ind &lt;- as_draws_matrix(fit_mac_weibull_ind)\nall(post_mac == post_mac_ind)\n\n[1] FALSE\n\n\nThe benefit of this approach is that we can then include external data corresponding to the controlAverage from the second historical study CheckMate 649, which is not one of the treatment groups but the average effect of control groups, \\(\\frac{1}{2} \\, (\\lambda_{\\mbox{controlChemoA}}+\\lambda_{\\mbox{controlChemoB}})\\).\nTo now find the indicators needed for the effect of controlAverage in the given parametrization so far, we can obtain this via simple algebra as:\n\ncc &lt;- MASS::fractions(solve(cc_inv))\ncc\n\n              intercept deltaEffectAvg deltaEffect deltaControl\nactiveChemoA     1       1/2              1        -1/2        \ncontrolChemoA    1      -1/2              0        -1/2        \nactiveChemoB     1       1/2             -1         1/2        \ncontrolChemoB    1      -1/2              0         1/2        \n\n0.5*(cc[\"controlChemoA\",] + cc[\"controlChemoB\",])\n\n     intercept deltaEffectAvg    deltaEffect   deltaControl \n             1           -1/2              0              0 \n\n\nHence we must set the deltaEffectAvg column to \\(-\\frac{1}{2}\\) and the others to \\(0\\) to get the indicator for controlAverage (the overall intercept is always added to the linear predictor as we define 1 as the first term of the model formula) in order to include the new historical data from CheckMate649.\n\nhdata2_ind &lt;- rename(hdata2, y=time, event=status) %&gt;%\n  mutate(hist1=0, hist2=1, armdeltaEffectAvg=-0.5,\n         armdeltaEffect=0, armdeltaControl=0)\nall_comb_data_ind &lt;- bind_rows(comb_data_ind, hdata2_ind)\n\nTo include the second historical data, we specify the formula and the priors as we did for one historical study.\n\nmodel_mac_weibull_ind_all &lt;- bf(y | cens(1-event) ~ 1 + armdeltaEffectAvg +\n                              armdeltaEffect + armdeltaControl + hist1+ hist2,\n                              family=weibull())\nmodel_weibull_mac_prior_all &lt;- model_weibull_mac_prior +\n  prior(normal(0, sdHist), coef=hist2)\n\nFit the model and compare:\n\nfit_all_mac_weibull_ind &lt;- brm(model_mac_weibull_ind_all, data=all_comb_data_ind,\n                               prior=model_weibull_mac_prior_all,\n                               stanvars=prior_sd + prior_mean + prior_sdHist,\n                               seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 6.0 seconds.\nChain 2 finished in 6.1 seconds.\nChain 3 finished in 6.0 seconds.\nChain 4 finished in 5.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 6.0 seconds.\nTotal execution time: 24.2 seconds.\n\nfit_mac_weibull_ind\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 \n   Data: comb_data_ind (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.93     2.33 1.00     5075     3553\narmdeltaEffectAvg     0.29      0.17    -0.05     0.63 1.00     4184     3537\narmdeltaEffect        0.01      0.10    -0.20     0.21 1.00     4688     3177\narmdeltaControl       0.07      0.11    -0.16     0.29 1.00     5187     3251\nhist1                -0.20      0.14    -0.47     0.06 1.00     4334     3137\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.92     1.09 1.00     4680     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nfit_all_mac_weibull_ind\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 + hist2 \n   Data: all_comb_data_ind (Number of observations: 1389) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.09      0.09     1.92     2.27 1.00     4002     3141\narmdeltaEffectAvg     0.23      0.16    -0.09     0.55 1.00     2743     2920\narmdeltaEffect        0.00      0.10    -0.19     0.20 1.00     3616     2884\narmdeltaControl       0.09      0.12    -0.14     0.32 1.00     3555     3039\nhist1                -0.23      0.13    -0.49     0.00 1.00     2287     2849\nhist2                 0.48      0.12     0.24     0.71 1.00     2108     1868\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.12      0.03     1.07     1.18 1.00     4034     3292\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#conclusion",
    "href": "src/02i_time_to_event.html#conclusion",
    "title": "14  Time-to-event data",
    "section": "14.6 Conclusion",
    "text": "14.6 Conclusion\nHere we showed how brms enabled us to model time-to-event data with censoring and incorporate historical data into the analysis. By use of custom contrasts we controlled the parametrization of the problem allowing for the specification of priors aligned with the prior knowledge. This is possible in R using the contrasts function to setup custom contrast matrices used to setup the design matrix of the model. Going beyond that we also discussed that in some situations the facilities in R are too limiting and a customized setup of the model design matrix allows for even greater flexibility. This allowed in this case study the inclusion of historical data which corresponds to the average effect of standard of care.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#exercises",
    "href": "src/02i_time_to_event.html#exercises",
    "title": "14  Time-to-event data",
    "section": "14.7 Exercises",
    "text": "14.7 Exercises\n\nIn brms various families can be used to model time-to-event data. Re-fit the model using exponential distribution and compare with the weibull results.\nCox regression is implemented in brms using M-splines, which is a very close approximation to the semi-parametric Cox model. Show that the estimates for the covariate effect we get from brms are quite similar to what we get from survival package. Also compare the estimate of the overall intercept which needs to account for an offset as detailled in section @ref(sec:tte-model-brms).\n\n\n\n\n\n\n\nJanjigian, Yelena Y., Kohei Shitara, Markus Moehler, Marcelo Garrido, Pamela Salman, Lin Shen, Lucjan Wyrwicz, et al. 2021. “First-Line Nivolumab Plus Chemotherapy Versus Chemotherapy Alone for Advanced Gastric, Gastro-Oesophageal Junction, and Oesophageal Adenocarcinoma (CheckMate 649): A Randomised, Open-Label, Phase 3 Trial.” The Lancet 398 (July): 27–40. https://doi.org/10.1016/S0140-6736(21)00797-2.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html",
    "href": "src/02j_network_meta_analysis.html",
    "title": "15  Network meta-analysis",
    "section": "",
    "text": "15.0.1 Background: Why network meta-analysis?\nNetwork (or multi-treatment) meta-analysis (NMA) is a term for analyses that combine the results of studies that compared several treatments and in which not all studies included all comparisons (Lu and Ades 2004). This is a frequent situation, because new drugs are often only compared to a placebo or to one commonly used current standard of care, but not to all existing current treatment options. As a result, physicians that want to choose between two treatment options often find that there are no or very few studies directly comparing them. NMA tries to address this issue by combining evidence from direct comparisons within each trial with indirect chains of comparisons across multiple trials.\nNetwork meta-analyses have their limitations. One particular concern would be factors that modify the effects of treatments resulting in treatment by trial interactions, e.g. if pathogens developing resistance to some drugs over time, if the presence of some background therapies has a synergistic or antagonistic interaction with a drug, or if some drugs work better or worse depending on disease severity.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#data",
    "href": "src/02j_network_meta_analysis.html#data",
    "title": "15  Network meta-analysis",
    "section": "15.1 Data",
    "text": "15.1 Data\nWe will use smoking cessation data (Hasselblad 1998), which consists of the number of patients \\(r\\) amongst the total in an study arm \\(n\\) that manage to stop smoking. There are 24 studies, in which different combinations of 4 interventions (no contact, self-help, individual counseling and group counseling) were tested. The aim of a network meta-analysis here would be to compare these different treatment in terms of the odds of patients managing to stop smoking.\n\ndata(\"smokingcessation\", package = \"netmeta\")\n\nrecode_trt &lt;- c(\"A\" = \"No_intervention\",\n                \"B\" = \"Self_help\",\n                \"C\" = \"Individual_counselling\",\n                \"D\" = \"Group_counselling\")\n\nsmoking &lt;- smokingcessation %&gt;%\n  mutate(studyn = 1:n()) %&gt;%\n  pivot_longer(-studyn,\n               names_to = c(\".value\", \"trtid\"),\n               names_pattern = \"(.*)([1-9])\") %&gt;%\n  filter(!is.na(n)) %&gt;%\n  transmute(\n    study = factor(studyn),\n    trtc = factor(recode_trt[treat], unname(recode_trt)),\n    trtn = as.numeric(trtc),\n    r = event,\n    n\n  )\n\ngt_preview(smoking)\n\n\n\n\n\n\n\n\nstudy\ntrtc\ntrtn\nr\nn\n\n\n\n\n1\n1\nNo_intervention\n1\n9\n140\n\n\n2\n1\nIndividual_counselling\n3\n23\n140\n\n\n3\n1\nGroup_counselling\n4\n10\n138\n\n\n4\n2\nSelf_help\n2\n11\n78\n\n\n5\n2\nIndividual_counselling\n3\n12\n85\n\n\n6..49\n\n\n\n\n\n\n\n50\n24\nGroup_counselling\n4\n3\n26\n\n\n\n\n\n\n\n\nsmoking %&gt;%\n  ggplot(aes(x=study, y=r/n, fill=trtc, group=trtc,\n             ymin=qbeta(p=0.025, shape1=r, shape2=1+n-r),\n             ymax=qbeta(p=0.975, shape1=r+1, shape2=n-r))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(position = \"dodge\", alpha=0.3) +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\nThe totality of evidence about the treatments and their relative efficacy is commonly known as the evidence network. A common graphical representation, shown below, visualizes this network of treatments (the nodes) along with edges indicating the presence or absence of direct evidence comparing pairs of treatments from randomized trials. Below, the thickness of the edges is determined by the number of trials comparing each pair of treatments.\n\nnm &lt;- netmeta(pairwise(treat = trtc, event = r, n = n, studlab = study,\n                       data = smoking))\nnetgraph(nm)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#model-description",
    "href": "src/02j_network_meta_analysis.html#model-description",
    "title": "15  Network meta-analysis",
    "section": "15.2 Model description",
    "text": "15.2 Model description\nWe will first describe the arm-based (aka “two-way mixed model”) approach to NMA advocated by Piepho (Piepho, Williams, and Madden 2012). It has some advantages over more commonly used NMA approach that analyzes treatment contrasts (Lu and Ades 2004), in terms of ease of use and familiarity (the two-way linear mixed model is of course very familiar). The latter approach, however, is widely used in health technology assessments, for example by the UK NICE. The NICE decision support unit has published methodological guidance on how to use this method in practice (Dias et al. 2014).\nSuppose we had individual patient data for patients \\(k=1,\\ldots,N_{ij}\\) randomly assigned to treatment \\(j\\) in trial \\(i\\), for \\(i = 1,\\ldots, n\\) and \\(j = 1,\\ldots, m\\). A two-way linear mixed model would model the outcome of interest \\(Y_{ijk}\\) as \\[ g(E(Y_{ijk})) = \\underbrace{\\alpha_i}_{\\text{fixed main effect of trial }i} + \\underbrace{\\beta_j}_{\\text{fixed main effect of treatment }j} + \\underbrace{u_{ij}}_{\\text{random effect of trial on treatment}},\\] where \\(g\\) is a link function such as the logit link in case of bernoulli data.\nThe random effects \\(u_{ij}\\) have mean zero and describe that the effect of treatments is allowed to vary across trials. If we wanted to assume the same effect of all treatment across all trials, we could instead omit this term. When we include it, we will ideally want to allow for the possibility that if one treatment has better outcomes in a trial, then other treatments might also. To achieve that we assume that for the trials \\(i=1,\\ldots,n\\) there is a multivariate normal random effect \\(\\boldsymbol{U}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\), where \\(\\Sigma\\) is some covariance matrix that is either unstructured or has some particular structure (e.g. compound symmetric). In other words, the components of the random vector \\(\\boldsymbol{U}_i\\) for a trial \\(i\\) are correlated.\n\n15.2.1 Network meta-analysis with summaries for each arm\nIn case we have summary information by treatment arm, we will (just as for traditional arm-based meta-analyses) often be able to assume a normal sampling distribution for least squares means for each arm from linear models, log-odds, log-rates or log-hazard rates. We will denote these estimates for each arm by \\(\\hat{\\theta}_{ij}\\) and their standard error by \\(\\text{SE}(\\hat{\\theta}_{ij})\\). We then assume \\(\\theta_{ij} = \\alpha_i + \\beta_j + u_{ij}\\) as above and that \\(\\hat{\\theta}_{ij} \\sim N(\\theta_{ij}, \\text{SE}(\\hat{\\theta}_{ij}))\\).\nAlternatively, for a binary outcome, we can use that the number of patients with a success follows a binomial distribution and we would use a regression equation like above for the logit-probability \\(\\theta_{ij} = \\alpha_i + \\beta_j + u_{ij}\\) for each arm.\nTO BE DETERMINE WHAT TO DO FOR SURVIAL: Less clear, could one work with log-cumulative-hazard function (under a parametric model?)? Maybe one could get that from KM curves? Log-(or logit-?)survival probabilities at fixed points of time (e.g. 1-year)? Discussed by some papers such as this one (Combescure et al. 2012). Perhaps models fitted to reconstructed patient-level data/KM curves via, standard parametric models, fractional polynomial models and/or spline-based models? What about non-proportional hazards?\n\n\n15.2.2 Patient-, arm- or trial-level covariates\nAs needed, the regression equation \\(\\alpha_i + \\beta_j + u_{ij}\\) can be extended by allowing for patient level covariates \\(\\boldsymbol{x}_{ijk}\\) (in case we work with individual patient data), arm-level covariates \\(\\boldsymbol{x}_{ij}\\) or study-level covariates \\(\\boldsymbol{x}_i\\).\n\n\n15.2.3 Borrowing information from real-world data or other trials\nIf we have a fixed effect for the \\(\\alpha_i\\), then borrowing information between trials runs into the identifiability issues between the \\(\\alpha_i\\) and the \\(u_{ij}\\). In a frequentist approach we cannot resolve these issues. When we take a Bayesian approach, this identifiability issue will still severely limit how much information we can obtain about the distribution of the \\(u_{ij}\\) (while we would by definition not borrow information about the \\(\\alpha_i\\)) and the results will be sensitive to the choice of prior distributions.\nWe could borrow information across trials, we replace the trial main effect by an intercept term and a random trial effect on the intercept. However, then we will be conducting a meta-analysis, in which the causal effects of treatments are no longer just judged based on randomized within-trial comparisons. We would then also to an extent do comparisons across treatment arms from different trials. Besides the limitations any NMA is subject to, such comparisons are additionally affected by any prognostic factors that change the expected outcomes for a treatment in different trials. Such prognostic factors are much more common than effect modifiers that would cause trial by treatment interactions, and we know that to some extent such differences across trials will exist. Examples include improving (or worsening) patient outcomes due to better treatments or access to healthcare, different inclusion/exclusion criteria of different trials, and differences in recruited patient populations in terms of e.g. background therapy, disease severity, country/region and comorbidities. While we may hope that a random trial effect on the intercept would capture random variation in these aspects across trials, we have to be concerned that there could be a systematic bias in favor of some treatments in a NMA - especially if studies for some treatments were conducted later in time than for other treatments.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#implementation",
    "href": "src/02j_network_meta_analysis.html#implementation",
    "title": "15  Network meta-analysis",
    "section": "15.3 Implementation",
    "text": "15.3 Implementation\nIn this case, we have a binomial outcome. brms provides the y | trials(n) notation to indicate y successes out of n trials, while in stats::glm the way of specifying this is a little less intuitive: glm( cbind(y, n-y)~... ).\nWe will now try implement the network meta-analysis model we described above. In the process, we will discover some things that did not make a difference in a frequentist setting, but matter in the Bayesian setting with respect to how we can the most easily specify our prior knowledge (or lack thereof).\n\n15.3.1 Reference categories matter for setting priors\nFirstly, before we even think too much about any prior distributions, let see how brms parameterizes the model, if we specify it in the most obvious way by just writing 0 + study + trtc + (0 + trtc |study):\n\nbrmfit &lt;- brm(\n  data = smoking,\n  formula = r | trials(n) ~ 0 + study + trtc + (0 + trtc || study),\n  family = binomial(),\n  control = control_args,\n  prior = prior(class = b, normal(0, 3.14)),\n  silent = 2,\n  refresh = 0\n)\n\nAs we can see from the summary of the fitted model, there are coefficients for all study main effects, but not for all treatments: treatment 1 is omitted, and the coefficients for remaining treatments now represent contrasts against treatment 1. (Conversely, if we had instead written 0 + trtc + study + (0+trtc|study), then there would have been a coefficient for all treatments, but the study effects would have been parametrized in terms of contrasts with study 1.\n\nsummary(brmfit)\n\nWarning: There were 11 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 0 + study + trtc + (0 + trtc || study) \n   Data: smoking (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 24) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(trtcNo_intervention)            0.68      0.36     0.05     1.47 1.01      333      807\nsd(trtcSelf_help)                  0.42      0.38     0.01     1.38 1.00     1055     1428\nsd(trtcIndividual_counselling)     0.52      0.29     0.03     1.08 1.01      334      864\nsd(trtcGroup_counselling)          0.91      0.64     0.05     2.48 1.00     1029     1077\n\nRegression Coefficients:\n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nstudy1                        -2.60      0.47    -3.52    -1.69 1.01      829     1745\nstudy2                        -2.25      0.41    -3.06    -1.43 1.01     1547     2396\nstudy3                        -1.19      0.68    -2.36    -0.02 1.01      388     1369\nstudy4                        -3.74      0.55    -4.90    -2.69 1.00     2080     1905\nstudy5                        -2.20      0.39    -3.01    -1.42 1.00     1358     1866\nstudy6                        -2.64      0.69    -4.08    -1.41 1.00      760     2111\nstudy7                        -2.05      0.71    -3.45    -0.81 1.01      375     1535\nstudy8                        -2.01      0.61    -3.30    -0.92 1.00      605     1484\nstudy9                        -1.79      0.47    -2.76    -0.89 1.00     2481     2584\nstudy10                       -2.21      0.47    -3.11    -1.15 1.00     1316     1605\nstudy11                       -3.46      0.50    -4.40    -2.32 1.00     1399     1531\nstudy12                       -2.27      0.40    -3.07    -1.44 1.00     1465     1685\nstudy13                       -2.72      0.51    -3.74    -1.72 1.00     2601     2363\nstudy14                       -2.33      0.42    -3.16    -1.46 1.00     1625     2314\nstudy15                       -2.17      1.05    -4.60    -0.36 1.00     1301     2046\nstudy16                       -2.31      0.51    -3.30    -1.24 1.00     1858     2006\nstudy17                       -2.35      0.39    -3.13    -1.56 1.00     1789     2169\nstudy18                       -2.84      0.45    -3.69    -1.91 1.00     1104     2419\nstudy19                       -2.31      0.46    -3.18    -1.46 1.00      712     2125\nstudy20                       -3.03      0.42    -3.86    -2.17 1.00     1069     2224\nstudy21                       -0.84      0.48    -1.83     0.08 1.01     1529     1908\nstudy22                       -2.18      0.60    -3.34    -0.97 1.00     1336     1960\nstudy23                       -2.04      0.55    -3.15    -0.93 1.00     1722     2249\nstudy24                       -2.39      0.61    -3.60    -1.21 1.00     1642     1979\ntrtcSelf_help                  0.26      0.40    -0.59     1.02 1.00     1082     1701\ntrtcIndividual_counselling     0.61      0.27     0.07     1.12 1.01      766     1169\ntrtcGroup_counselling          0.80      0.56    -0.27     1.96 1.01     1415     2180\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIs this a problem? In the frequentist context, this issue with the form of the design matrix for population-level effects is not important, because the pairwise treatment contrasts are uniquely estimable regardless of whether constraints such as \\(\\beta_1 = 0\\) are employed.\nHowever, if we wanted to imply some prior information or lack thereof, it may impact our preference for the specification of the population-level terms. For example, if we had some prior information (external to the likelihood) on a particular contrast between a pair of treatments, we would favor the parametrization above, in which the treatment terms are parametrized in terms of contrasts.\n\n\n15.3.2 Setting priors is easier in an over-parameterized model\nAnother possibility is to over-parametrize the model and have a main effect for each study (i.e. main effects for all studies study1 to study24 are present) and each treatment. The main rationale for this would be symmetry in how we set our prior distributions. I.e. we do not want to set our priors in a way that would somehow favor one treatment (or study - which will have studied some subset of treatments = might again favor somehow treatment) over any of the others. Unlike the specification above, the marginal variance of the each arm of each study will be constant in this approach.\nThis can be achieved by creating dummy variables for each treatment group and thereby enforcing the paramterization. We then specify our model as 0 + study + trtcA + trtcB + trtcC + trtcD + (0 + trtc | study). The code below constructs the dummy variables and corresponding formula in an automated way.\n\nB &lt;- model.matrix(~ 0 + trtc, data = smoking)\nS &lt;- model.matrix(~ 0 + study, data = smoking)\n\nsmoking_with_dummies &lt;- bind_cols(\n  dplyr::select(smoking, r, n, study, trtc),\n  as_tibble(B),\n  as_tibble(S)\n)\n\nf &lt;- as.formula(paste(\n  \"r | trials(n) ~ 0 + study +\",\n  paste(colnames(B), collapse = \" + \"),\n  \"+ (0 + trtc || study)\"\n))\n\n# could also do:\n# f &lt;- as.formula(paste(\n#     \"r | trials(n) ~ 0 + trtc +\",\n#     paste(colnames(S), collapse = \" + \"),\n#     \"+ (0 + trtc || study)\"\n# ))\n\nbrmfit_with_dummies &lt;- brm(data = smoking_with_dummies,\n                           formula = f,\n                           family = binomial(),\n                           control = control_args,\n                           prior = prior(class=b, normal(0, 3.14)),\n                           silent = 2,\n                           refresh = 0)\n\nWhen we now summarize the model fit, we have regression coefficients for all treatment groups and studies.\n\nsummary(brmfit_with_dummies)\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 0 + study + trtcNo_intervention + trtcSelf_help + trtcIndividual_counselling + trtcGroup_counselling + (0 + trtc || study) \n   Data: smoking_with_dummies (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 24) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(trtcNo_intervention)            0.61      0.34     0.04     1.29 1.01      409      950\nsd(trtcSelf_help)                  0.44      0.40     0.02     1.46 1.00     1552     2127\nsd(trtcIndividual_counselling)     0.56      0.32     0.04     1.20 1.01      336     1200\nsd(trtcGroup_counselling)          1.05      0.74     0.08     2.87 1.00     1171     1069\n\nRegression Coefficients:\n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nstudy1                        -0.49      0.73    -1.94     0.93 1.01      444     1102\nstudy2                        -0.22      0.71    -1.64     1.12 1.01      437      906\nstudy3                         0.85      0.85    -0.73     2.49 1.01      376      866\nstudy4                        -1.66      0.78    -3.17    -0.16 1.01      474     1403\nstudy5                        -0.06      0.70    -1.42     1.31 1.01      422     1002\nstudy6                        -0.65      0.91    -2.52     1.06 1.01      549     1154\nstudy7                        -0.05      0.89    -1.76     1.60 1.01      418      963\nstudy8                         0.01      0.84    -1.66     1.60 1.01      432     1024\nstudy9                         0.33      0.76    -1.15     1.79 1.01      478     1227\nstudy10                       -0.11      0.73    -1.58     1.28 1.01      452     1121\nstudy11                       -1.39      0.73    -2.81     0.04 1.01      457     1169\nstudy12                       -0.14      0.72    -1.56     1.23 1.01      403     1062\nstudy13                       -0.60      0.77    -2.10     0.91 1.01      538     1286\nstudy14                       -0.22      0.70    -1.57     1.11 1.01      429     1204\nstudy15                       -0.36      1.24    -3.11     1.74 1.00      968     1564\nstudy16                       -0.23      0.73    -1.62     1.13 1.01      489     1309\nstudy17                       -0.21      0.68    -1.50     1.16 1.01      389      927\nstudy18                       -0.68      0.76    -2.23     0.80 1.01      400     1053\nstudy19                       -0.13      0.78    -1.71     1.38 1.01      402      989\nstudy20                       -0.89      0.74    -2.29     0.56 1.01      394      978\nstudy21                        1.21      0.74    -0.28     2.61 1.01      453     1161\nstudy22                       -0.25      0.80    -1.85     1.28 1.00      559     1452\nstudy23                       -0.08      0.80    -1.70     1.46 1.00      545     1371\nstudy24                       -0.41      0.84    -2.09     1.17 1.00      599     1550\ntrtcNo_intervention           -2.31      0.62    -3.55    -1.10 1.01      354      834\ntrtcSelf_help                 -1.77      0.68    -3.07    -0.44 1.01      394     1029\ntrtcIndividual_counselling    -1.47      0.62    -2.68    -0.28 1.01      348      752\ntrtcGroup_counselling         -1.10      0.80    -2.59     0.60 1.01      547     1077\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhile the coefficients for each treatment have changed compared to the previous section, we should not actually look at the coefficients for each treament main effect, but at their contrasts. When we look at those, we see that the models lead to very similar inference (despite us not even setting any priors for Model 0).\n\n# utility function to estimate all pairwise contrasts between the main effects\n# for the variable named \"trtc\"\ncontrast_draws &lt;- function(brmfit, variable = \"trtc\"){\n  \n  trts &lt;- levels(brmfit$data[[variable]])\n  ntrt &lt;- nlevels(brmfit$data[[variable]])\n  \n  all_pairs &lt;- combn(1:ntrt, 2)\n  L_trt &lt;- t(apply(\n    all_pairs, 2, function(x){\n      out &lt;- numeric(ntrt)\n      out[x[1]] &lt;- 1\n      out[x[2]] &lt;- -1\n      out\n    }\n  ))\n  colnames(L_trt) &lt;- paste0(variable, trts)\n  \n  X &lt;- standata(brmfit)$X\n  L_study &lt;- matrix(0, nrow = nrow(L_trt), ncol = sum(!grepl(variable, colnames(X))))\n\n  # if the treatment variable is not dummy coded for every level then\n  # we need to drop the base category as this is masked by study\n  if(ncol(L_trt) == sum(grepl(variable, colnames(X)))){\n    L &lt;- cbind(L_study, L_trt)\n  } else{\n    L &lt;- cbind(L_study, L_trt[,-1])\n  }\n  \n  colnames(L) &lt;- colnames(X)\n  \n  # labels for the contrasts\n  labs &lt;- apply(combn(trts, 2), 2, paste, collapse = \" vs \")\n  rownames(L) &lt;- labs\n  \n  B &lt;- as_draws_matrix(brmfit, variable = \"^b_\", regex = TRUE)\n  \n  gamma_draws &lt;- B %*% t(L)\n  \n  # convert to posterior object\n  as_draws_matrix(gamma_draws)\n  \n}\n\nsummarize_contrasts &lt;- function(brmfit, model_name = \"model\", variable = \"trtc\", ...){\n  relocate(\n    mutate(summarize_draws(contrast_draws(brmfit, variable = variable), ...), model = model_name),\n    model\n  )\n}\n\nbind_rows(\n  summarize_contrasts(brmfit, \"Full-rank model\"),\n  summarize_contrasts(brmfit_with_dummies, \"Overparametrized model\")\n) %&gt;%\n    gt() %&gt;%\n    fmt_number(where(is.numeric), decimals=2)\n\n\n\n\n\n\n\nmodel\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nFull-rank model\nNo_intervention vs Self_help\n−0.26\n−0.26\n0.40\n0.37\n−0.90\n0.41\n1.00\n1,034.62\n1,648.43\n\n\nFull-rank model\nNo_intervention vs Individual_counselling\n−0.61\n−0.61\n0.27\n0.25\n−1.05\n−0.17\n1.00\n749.41\n1,151.44\n\n\nFull-rank model\nNo_intervention vs Group_counselling\n−0.80\n−0.80\n0.56\n0.51\n−1.72\n0.06\n1.00\n1,352.63\n2,140.11\n\n\nFull-rank model\nSelf_help vs Individual_counselling\n−0.35\n−0.34\n0.41\n0.38\n−1.00\n0.30\n1.00\n1,747.21\n2,063.66\n\n\nFull-rank model\nSelf_help vs Group_counselling\n−0.54\n−0.54\n0.59\n0.50\n−1.54\n0.38\n1.00\n1,991.52\n2,126.90\n\n\nFull-rank model\nIndividual_counselling vs Group_counselling\n−0.19\n−0.17\n0.55\n0.47\n−1.12\n0.63\n1.00\n2,144.98\n2,188.75\n\n\nOverparametrized model\nNo_intervention vs Self_help\n−0.54\n−0.52\n0.41\n0.37\n−1.23\n0.06\n1.00\n2,408.11\n2,258.03\n\n\nOverparametrized model\nNo_intervention vs Individual_counselling\n−0.84\n−0.82\n0.26\n0.24\n−1.28\n−0.43\n1.00\n3,690.12\n3,404.89\n\n\nOverparametrized model\nNo_intervention vs Group_counselling\n−1.21\n−1.14\n0.62\n0.52\n−2.32\n−0.32\n1.00\n2,793.18\n2,014.06\n\n\nOverparametrized model\nSelf_help vs Individual_counselling\n−0.29\n−0.30\n0.42\n0.38\n−0.94\n0.38\n1.00\n2,684.36\n2,583.99\n\n\nOverparametrized model\nSelf_help vs Group_counselling\n−0.67\n−0.63\n0.66\n0.57\n−1.84\n0.33\n1.00\n2,637.09\n2,268.98\n\n\nOverparametrized model\nIndividual_counselling vs Group_counselling\n−0.37\n−0.30\n0.61\n0.51\n−1.50\n0.50\n1.00\n2,778.94\n2,051.12\n\n\n\n\n\n\n\nNote also that it is hard to interpret the study main effects in these models. We could try to make them a little more interpretable by making them represent something like the average expected outcome across treatment groups (i.e. if we leave out the treatment main effects, we get a predicted outcome that is between the predicted outcomes for the treatment groups). For this purpose we encode the dummy for each treatment group as \\(0.5\\) vs. \\(-0.5\\) (if it is not that treatment group). However, treatment main effect coefficients should still be looked at in contrast to each other.\n\nsmoking_with_centered_dummies &lt;- bind_cols(\n  dplyr::select(smoking, r, n, study, trtc),\n  as_tibble(B - 0.5),\n  as_tibble(S)\n)\n\nbrmfit_with_centered_dummies &lt;- brm(\n    data = smoking_with_centered_dummies,\n    formula = f,\n    family = binomial(),\n    control = control_args,\n    prior = prior(class=b, normal(0, 3.14)),\n    silent = 2,\n    refresh = 0\n)\n\nWith the relatively low-information priors we’ve chosen here, all three of these models produce similar results\n\nall_contrasts &lt;- tibble(\n  model_name = c(\"Full-rank model\",\n                 \"Overparametrized model\",\n                 \"Overparametrized model with centering\"),\n  brmfit = list(brmfit, brmfit_with_dummies, brmfit_with_centered_dummies)\n) %&gt;%\n    mutate(contrasts = map2(brmfit, model_name, summarize_contrasts)) %&gt;%\n    select(-brmfit) %&gt;%\n    unnest(contrasts)\n\nall_contrasts %&gt;%\n    mutate(model=str_remove_all(model, \" model\"),\n           model=str_replace_all(model, \"with\", \"\\nwith\"),\n           variable=str_replace_all(variable, \"vs\", \"\\nvs\\n\"),\n           variable=str_replace_all(variable, \"counselling\", \"couns.\"),\n           variable=str_replace_all(variable, \"Individual\", \"Indiv.\"),\n           ) %&gt;%\n    ggplot(aes(x=median, y=model, xmin=q5, xmax=q95)) +\n    geom_vline(xintercept=0, color=\"darkred\", linetype=2) +\n    geom_point() +\n    geom_errorbarh() +\n    xlab(\"log-odds ratio\") +\n    facet_wrap(~variable)\n\n\n\n\n\n\n\n\n\n\n15.3.3 Ranking treatments and the probability of being the best treatment\nProbability that each treatment is best is straightforward in any of these approaches. To illustrate:\n\nB &lt;- as_draws_matrix(brmfit_with_dummies, variable = \"b_trtc\", regex = TRUE)\nbest_idx &lt;- apply(B, 1, which.max)\ntrts &lt;- levels(smoking$trtc)\nbest_trt &lt;- factor(trts, levels=trts)[best_idx]\nprob_best &lt;- prop.table(table(best_trt))\n\ngt(arrange(as.data.frame(prob_best), -Freq)) %&gt;% fmt_percent(Freq)\n\n\n\n\n\n\n\nbest_trt\nFreq\n\n\n\n\nGroup_counselling\n70.65%\n\n\nIndividual_counselling\n22.57%\n\n\nSelf_help\n6.78%\n\n\nNo_intervention\n0.00%\n\n\n\n\n\n\n\n\n\n15.3.4 Detailed look at a pairwise contrast\nIn some situations, there may be a particular treatment contrast that is of special interest. In some such scenarios, where we have several This is to illustrate that by including indirect evidence, we can enhance our inference for a pairwise contrast of interest, relative to a simple pairwise meta-analysis.\nThe following analyses are compared, in terms of their estimates of the relative effect (log odds ratio) of individual counselling versus no intervention:\n\nNetwork meta-analysis model (brmfit1 above)\nPairwise meta-analysis model (currently RBesT is used here, but we could equally use brms)\n“Stratified” analysis of the contrast separately in each study, using independent beta-binomial models\n\n\n\nShow the code\n# identify the A vs C studies\nac_studies &lt;- smoking %&gt;%\n  filter(trtc %in% c(\"No_intervention\", \"Individual_counselling\")) %&gt;%\n  add_count(study, name = \"num_treatments\") %&gt;%\n  filter(num_treatments == 2) %&gt;%\n  arrange(study, trtc)\n\n# summarize the A vs C log odds ratio using Beta-Binomial models independently\n# across studies and arms\narm_level_beta_binom &lt;- ac_studies %&gt;%\n  rowwise() %&gt;%\n  mutate(p = map2(r, n, ~ rbeta(10000, r + 0.5, n - r + 0.5))) %&gt;%\n  dplyr::select(study, trtc, p) %&gt;%\n  pivot_wider(names_from = trtc, values_from = p)\n\nstudy_effects &lt;- arm_level_beta_binom %&gt;%\n  mutate(log_odds_ratio = map2(No_intervention, Individual_counselling,\n                               ~ log(.x) + log(1 - .y) - log(1 - .x) - log(.y)),\n         out = map(log_odds_ratio, ~ summarize_draws(matrix(.), mean, sd, ~ quantile(., probs = c(0.025, 0.5, 0.975))))) %&gt;%\n  transmute(study = factor(study), out) %&gt;%\n  unnest(out) %&gt;%\n  dplyr::select(-variable)\n\n# pairwise meta-analysis of the log odds ratios based on summary statistics\nrbest_fit &lt;- RBesT::gMAP(cbind(mean, sd) ~ 1 | study,\n                         data = study_effects,\n                         tau.dist = \"HalfNormal\",\n                         tau.prior = 1,\n                         beta.prior = cbind(0,2))\n\n# study-level treatment-effect estimates from RBesT\nfitted_rbest &lt;- bind_cols(dplyr::select(study_effects, study), fitted(rbest_fit))\n\n# study-level treatment effect estimates from the NMA model\nnd &lt;- inner_join(smoking_with_dummies,\n                 dplyr::select(ac_studies, study, trtc),\n                 c(\"study\", \"trtc\")) %&gt;%\n  mutate(n = 1)\n\nlp &lt;- posterior_epred(brmfit, newdata = nd)\nlpA &lt;- lp[,nd$trtc == \"No_intervention\"]\nlpC &lt;- lp[,nd$trtc == \"Individual_counselling\"]\nlor &lt;- log(lpA) + log(1 - lpC) - log(1 - lpA) - log(lpC)\n\nfitted_nma &lt;- bind_cols(\n  dplyr::select(study_effects, study),\n  dplyr::select(summarize_draws(lor, mean, sd, ~ quantile(., probs = c(0.025, 0.5, 0.975))), -variable)\n)\n\n# mean and MAP treatment effect estimates from RBesT\nmean_rbest &lt;- summary(rbest_fit, type=\"response\")[c(\"theta.pred\", \"theta\")] %&gt;% \n  do.call(what = \"rbind\") %&gt;%\n  as_tibble(rownames = \"study\") %&gt;% \n  mutate(type = \"pairwise meta\",\n         study = c(\"theta_resp_pred\" = \"MAP\", \"theta_resp\" = \"Mean\")[study])\n\n# mean treatment effect estimate from NMA model\nmean_nma &lt;- summarize_contrasts(brmfit, variable = \"trtc\", model_name = \"nma\", mean, sd,\n                                ~ quantile(., probs = c(0.025, 0.5, 0.975))) %&gt;%\n  filter(variable == \"No_intervention vs Individual_counselling\") %&gt;%\n  mutate(study = \"Mean\", type = \"network meta\")\n\nall_ests &lt;- bind_rows(\n  mutate(study_effects, type = \"stratified\", estimate_type = \"Study-level\"),\n  mutate(fitted_rbest, type = \"pairwise meta\", estimate_type = \"Study-level\"),\n  mutate(fitted_nma, type = \"network meta\", estimate_type = \"Study-level\"),\n  mutate(mean_rbest, estimate_type = \"Mean\"),\n  mutate(mean_nma, estimate_type = \"Mean\")\n)\n\nggplot(all_ests, aes(y = study, x = mean, xmin = `2.5%`, xmax = `97.5%`,\n                     group = type, color = type)) +\n    geom_pointrange(position = position_dodge(0.4)) +\n    facet_grid(estimate_type ~ ., space = \"free\", scales = \"free\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    scale_x_continuous(breaks=seq(-5,1,by=1)) +\n    coord_cartesian(xlim=c(-5,0.5)) +\n    labs(x = \"Log odds ratio for individual counselling vs no intervention\",\n         y = \"Study\", color = \"Analysis\\nmethod\") +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#conclusion",
    "href": "src/02j_network_meta_analysis.html#conclusion",
    "title": "15  Network meta-analysis",
    "section": "15.4 Conclusion",
    "text": "15.4 Conclusion\nArm-based (network-)meta-analysis approaches compare favorably with contrast based approaches in many situations and fit very easily into the Bayesian regression modeling machinery provided by brms. The main challenge we face is how one should parametrize the model in order to be able to specify prior distributions easily, for which we showed several options.\nFor NMA, Bayesian approaches are very popular due to the ease with which one can obtain inference about various quantities of interest (e.g. ranking of treatments, probability that a treatment is the best one) via transformations of the MCMC samples from the posterior distribution.\nTBD: Further topics might include including observational data (aka real-world Exploiting that some drugs are in the same class, that some arms are different doses of the same drug, or when we have subgroup results from the same study.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#exercises",
    "href": "src/02j_network_meta_analysis.html#exercises",
    "title": "15  Network meta-analysis",
    "section": "15.5 Exercises",
    "text": "15.5 Exercises\n\n15.5.1 Excercise 1\nWe will use the data on the occurrence of malignancy anti-tumour necrosis factor (anti-TNF) drugs used in the paper by Warren, Abrams, and Sutton (2014). We have information for each arm of 13 studies of three different anti-TNF drugs (etanercept, adalimumab and infliximab) on how many patients developed a malignancy y out of the total number of patients in an arm n. It is common in drug development to simplistically reduce what is in truth a time-to-event process of adverse event occurrence into a binomial problem. In practice, we should be cautious about this simplification, because differential drop-out could lead to biased conclusions and we will by necessaity get variation in effect measures for binomial outcomes across studies of different duration even if the time-to-event distributions are identical across studies.\n\nantiTNF &lt;- tibble(\n  study = c(\"Ericson (1999)\", \"Ericson (1999)\", \"Ericson (1999)\", \n            \"Moreland (1999)\", \"Moreland (1999)\", \"Moreland (1999)\", \n            \"Genovese (2002)\", \"Genovese (2002)\", \"Genovese (2002)\", \n            \"Combe (2006)\", \"Combe (2006)\", \"Van der Heijde (2006)\", \n            \"Van der Heijde (2006)\", \"Weisman (2007)/Baumgartner (2004)\", \n            \"Weisman (2007)/Baumgartner (2004)\", \"Furst (2003)\", \"Furst (2003)\", \n            \"Weinblatt (2003)\", \"Weinblatt (2003)\", \"Weinblatt (2003)\", \n            \"Weinblatt (2003)\", \"Keystone (2004)\", \"Keystone (2004)\", \n            \"Van de Putte (2004)\", \"Van de Putte (2004)\", \"Van de Putte (2004)\", \n            \"Van de Putte (2004)\", \"Breedveld (2006)\", \"Breedveld (2006)\", \n            \"Maini (2004)\", \"Maini (2004)\", \"Maini (2004)\", \"St Clair (2004)\", \n            \"St Clair (2004)\", \"St Clair (2004)\"), \n  treatment = c(\"Control\", \"Etanercept\", \"Etanercept\", \"Control\", \"Etanercept\", \n                \"Etanercept\", \"Control\", \"Etanercept\", \"Etanercept\", \"Control\", \n                \"Etanercept\", \"Control\", \"Etanercept\", \"Control\", \"Etanercept\", \n                \"Control\", \"Adalimumab\", \"Control\", \"Adalimumab\", \"Adalimumab\", \n                \"Adalimumab\", \"Control\", \"Adalimumab\", \"Control\", \"Adalimumab\", \n                \"Adalimumab\", \"Adalimumab\", \"Control\", \"Adalimumab\", \n                \"Control\", \"Infliximab\", \"Infliximab\", \"Control\", \"Infliximab\", \n                \"Infliximab\"), \n  regimen = c(NA, \"25 mg biw\", \"10 mg qw or 25 mg qw or 10 mg biw\", NA, \n              \"25 mg biw\", \"10 mg biw\", NA, \"25 mg biw\", \"10 mg biw\", NA, \n              \"25 mg biw\", NA, \"25 mg biw\", NA, \"25 mg biw\", NA, \"40 mg eow\", \n              NA, \"40 mg eow\", \"20 mg eow\", \"80 mg eow\", NA, \n              \"20 mg qw or 40 mg eow\", NA, \"20 mg qw or 40 mg eow\", \n              \"20 mg eow\", \"40 mg qw\", NA, \"40 mg eow\", NA, \"3 mg/kg q8w\", \n              \"3 mg/kg q4w or 10 mg/kg q8w or 10 mg/kg q4w\", NA, \"3 mg/kg q8w\", \n              \"6 mg/q8w\"), \n  dose = c(NA, \"Rec\", \"Low\", NA, \"Rec\", \"Low\", NA, \"Rec\", \"Low\", NA, \"Rec\", NA, \n           \"Rec\", NA, \"Rec\", NA, \"Rec\", NA, \"Rec\", \"Low\", \"High\", NA, \"Rec\", NA, \n           \"Rec\", \"Low\", \"High\", NA, \"Rec\", NA, \"Rec\", \"High\", NA, \"Rec\", \n           \"High\"), \n  n = c(105L, 111L, 343L, 80L, 78L, 76L, 217L, 207L, 208L, 50L, 204L, 228L, \n        454L, 269L, 266L, 318L, 318L, 62L, 67L, 69L, 73L, 200L, 419L, 110L, \n        225L, 106L, 103L, 257L, 542L, 88L, 86L, 254L, 291L, 372L, 377L), \n  y = c(0L, 0L, 2L, 0L, 1L, 0L, 4L, 5L, 5L, 0L, 1L, 1L, 10L, 2L, 2L, 0L, 4L, 0L, \n        0L, 0L, 1L, 1L, 8L, 1L, 2L,  1L, 1L, 4L, 6L, 1L, 1L, 8L, 0L, 0L, 4L))\n\nIf we look at the data, we can see that the proportion of patients with an event is quite low (&lt;4%) across all studies and arms. I.e. we are in a rare event setting, where a Bayesian approach with sensibly chosen priors might be helpful.\n\nantiTNF %&gt;%\n  mutate(study = factor(study),\n         arm = paste0(treatment, ifelse(is.na(dose),\"\",paste0(\" \",dose)))) %&gt;%\n  ggplot(aes(x=study, y=y/n, \n             fill=arm, \n             group=arm,\n             ymin=qbeta(p=0.025, shape1=y, shape2=1+n-y),\n             ymax=qbeta(p=0.975, shape1=y+1, shape2=n-y))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(position = \"dodge\", alpha=0.3) +\n  theme(legend.position=\"bottom\") +\n  guides(fill=guide_legend(ncol=2)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nTry fitting models of increasing complexity (slightly different to those done in the Warren, Abrams, and Sutton (2014) paper, which you could also choose to reproduce):\n\nA simple meta-analysis of anti-TNF therapy vs. non-anti-TNF controls.\nA network meta-analysis with each anti-TNF drug having a separate fixed treatment effects parameter, but ignoring informaiton on doses and regimens.\nA network meta-analysis that treats all unique combinations of drug and dose/regimen as a separate treatment.\nA network meta-analysis that assumes that each drug may have a different effect on malignancy with this effect being monotonic across doses. Note the mo() notation provided by brms to define monotonic effects. Look into borrowing information about the maximum effect across drugs.\n\n\n\n\n\n\n\n\nCombescure, C, DS Courvoisier, G Haller, and TV Perneger. 2012. “Meta-Analysis of Two-Arm Studies: Modeling the Intervention Effect from Survival Probabilities.” Statistical Methods in Medical Research 25 (2): 857–71. https://doi.org/10.1177/0962280212469716.\n\n\nDias, S., N. J. Welton, A. J. Sutton, and A. Ades. 2014. “NICE DSU Technical Support Document 2: A Generalised Linear Modelling Framework for Pairwise and Network Meta-Analysis of Randomised Controlled Trials.” Technical report. NICE Decision Support Unit.\n\n\nHasselblad, Vic. 1998. “Meta-Analysis of Multitreatment Studies.” Medical Decision Making 18 (1): 37–43. https://doi.org/10.1177/0272989x9801800110.\n\n\nLu, G., and A. E. Ades. 2004. “Combination of Direct and Indirect Evidence in Mixed Treatment Comparisons.” Statistics in Medicine 23 (20): 3105–24. https://doi.org/10.1002/sim.1875.\n\n\nPiepho, H. P., E. R. Williams, and L. V. Madden. 2012. “The Use of Two-Way Linear Mixed Models in Multitreatment Meta-Analysis.” Biometrics 68 (4): 1269–77. https://doi.org/10.1111/j.1541-0420.2012.01786.x.\n\n\nWarren, Fiona C., Keith R. Abrams, and Alex J. Sutton. 2014. “Hierarchical Network Meta-Analysis Models to Address Sparsity of Events and Differing Treatment Classifications with Regard to Adverse Outcomes.” Statistics in Medicine 33 (14): 2449–66. https://doi.org/10.1002/sim.6131.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/03a_stan_code.html",
    "href": "src/03a_stan_code.html",
    "title": "16  Exposing stan code",
    "section": "",
    "text": "In some situations it can be useful to make use of the Stan functions brms uses inside the Stan programs it creates. This is for example the case when creating a custom family as described in the vignette on custom response distributions. Whenever defining a custom family, one does provide Stan code snippets to brms which are used to setup the respective Stan model. Since the same functions are then useful for post-proccessing of the model posterior, the created functions can be used in R as well using the expose_functions from brms. Please refer to the vignette on custom families and the help on the expose function utility function ?expose_functions.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exposing stan code</span>"
    ]
  },
  {
    "objectID": "src/03b_parallel.html",
    "href": "src/03b_parallel.html",
    "title": "17  Parallel computation",
    "section": "",
    "text": "17.1 Implementation of cmq_brm\n# executes brm with parallelization via clustermq\ncmq_brm &lt;- function(..., seed, control=list(adapt_delta=0.9), cores, file, chains=4, .log_worker=FALSE) {\n    checkmate::assert_integer(as.integer(seed), lower=1, any.missing=FALSE, len=1)\n    brms_global &lt;- options()[grep(\"^brms\", names(options()), value=TRUE)]\n    cmdstanr_global &lt;- options()[grep(\"^cmdstanr\", names(options()), value=TRUE)]\n    dots &lt;- rlang::enquos(...)\n    brms_args &lt;- lapply(dots, rlang::eval_tidy)\n    suppressWarnings(model &lt;- do.call(brm, modifyList(brms_args, list(chains=0, cores=1))))\n    update_args &lt;- list(object=model, seed=seed, cores=1, chains=1, control=control)\n    if(!missing(file)) {\n        update_args$file &lt;- sub(\"\\\\.rds$\", \"\", file)\n    }\n    if(!missing(file)) {\n        brms_args$file &lt;- sub(\"\\\\.rds$\", \"\", file)\n    }\n    master_lib_paths &lt;- .libPaths()\n    update_model &lt;- function(chain_id) {\n        .libPaths(master_lib_paths)\n        library(brms)\n        # in case file is part of extra-arguments, we add here the chain_id\n        # to get correct by-chain file caching\n        if(\"file\" %in% names(update_args)) {\n            update_args &lt;- modifyList(update_args, list(file=paste0(update_args$file, \"-\", chain_id)))\n        }\n        if(\"file\" %in% names(brms_args)) {\n            brms_args &lt;- modifyList(brms_args, list(file=paste0(brms_args$file, \"-\", chain_id)))\n        }\n        update_args$chain_id &lt;- chain_id\n        brms_args$chain_id &lt;- chain_id\n        # ensure the same brms & cmdstanr global options are set\n        options(brms_global)\n        options(cmdstanr_global)\n        ## for the rstan backend we do an update while for cmdstanr we\n        ## have to avoid this for whatever reason\n        if(model$backend == \"cmdstanr\") {\n            msg &lt;- capture.output(fit &lt;- do.call(brm, modifyList(brms_args, list(chains=1))))\n        } else {\n            msg &lt;- capture.output(fit &lt;- do.call(update, update_args))\n        }\n        list(fit=fit, msg=msg)\n    }\n    n_jobs &lt;- chains\n    backend &lt;- getOption(\"clustermq.scheduler\", \"multiprocess\")\n    if(backend %in% c(\"multiprocess\", \"multicore\")) {\n        n_jobs &lt;- min(chains, getOption(\"mc.cores\", 1))\n    }\n    cores_per_chain &lt;- 1\n    if(!is.null(model$threads$threads)) {\n        cores_per_chain &lt;- model$threads$threads\n    }\n    if(chains == 1 & cores_per_chain == 1) {\n        ## looks like a debugging run...avoid clustermq\n        return(update_model(1)$fit)\n    }\n    message(\"Starting \", chains, \" chains with a concurrency of \", n_jobs, \" and using \", cores_per_chain, \" cores per chain with backend \", backend, \"...\\n\")\n    cluster_update &lt;- clustermq::Q(update_model, chain_id=1:chains, n_jobs=n_jobs, export=list(update_args=update_args, brms_args=brms_args, brms_global=brms_global, cmdstanr_global=cmdstanr_global, master_lib_paths=master_lib_paths, model=model),\n                                   template=list(cores=cores_per_chain),\n                                   log_worker=.log_worker)\n    fit &lt;- combine_models(mlist=lapply(cluster_update, \"[[\", \"fit\"))\n    msg &lt;- lapply(cluster_update, \"[[\", \"msg\")\n    for(i in seq_len(length(msg))) {\n        if(length(msg[[i]]) == 0)\n            next\n        cat(paste0(\"Output for chain \", i, \":\\n\"))\n        cat(paste(msg[[i]], collapse=\"\\n\"), \"\\n\")\n    }\n    fit$file &lt;- NULL\n    fit\n}",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Parallel computation</span>"
    ]
  }
]