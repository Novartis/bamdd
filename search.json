[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "(Bayesian) Applied Modelling in Drug Development (BAMDD)",
    "section": "",
    "text": "Preface\nThis website contains materials that were developed to accompany a series of workshops held in the Analytics department at Novartis over 2022-2024, illustrating the utility of the R package brms for solving drug- development problems. In order to highlight the versatility of that package, we have developed a collection of case studies covering a diverse set of clinical questions that were addressed via Bayesian modelling with the brms package.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#navigating-the-site",
    "href": "index.html#navigating-the-site",
    "title": "(Bayesian) Applied Modelling in Drug Development (BAMDD)",
    "section": "Navigating the site",
    "text": "Navigating the site\nThe material is organized as follows:\n\n1  Introduction introduces brms and the objectives of these vignettes\n2  Basic workflow highlights the basic workflow (analysis steps and syntax) for carrying out an analysis with brms\nThe next series of sections contain the case studies. You can find a listing with descriptions on this page.\nSections 12  Exposing stan code and 13  Parallel computation cover more technical topics, such as exposing and injecting custom stan code to brms models, and efficient parallel computation to support sampling in brms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "(Bayesian) Applied Modelling in Drug Development (BAMDD)",
    "section": "Updates",
    "text": "Updates\nThis web-site is intended as a live document with updates as appropiate. Key changes to the web-site are tracked here:\n\n19th April 2024: First public version at opensource.nibr.com/bamdd\n5th January 2024: Release web-site in more modern quarto book based format\n26th April 2023: Second edition course from Paul Bürkner with the new case studies on MMRM, time-to-event data, surrogate endpoint meta-regression and network meta-analysis\n27th April 2022: First release with course from Paul Bürkner",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license-information",
    "href": "index.html#license-information",
    "title": "(Bayesian) Applied Modelling in Drug Development (BAMDD)",
    "section": "License information",
    "text": "License information\n\nThe material on this website is provided under a CC BY 4.0 license\nThe source code is provided under a GPL-2 license",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "src/01a_introduction.html",
    "href": "src/01a_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Public documentation\nThere are many resources available online for learning about brms. We list a few:\nAn excellent place to ask questions on brms in the public is the Stan discourse forum.\nMore useful ressources from the Stan community:",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/01a_introduction.html#public-documentation",
    "href": "src/01a_introduction.html#public-documentation",
    "title": "1  Introduction",
    "section": "",
    "text": "The homepage for the package, https://paul-buerkner.github.io/brms/\nBürkner, P.-C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\nPaul-Christian Bürkner, Advanced Bayesian Multilevel Modeling with the R Package. The R Journal (2018) 10:1, pages 395-411. https://journal.r-project.org/archive/2018/RJ-2018-017/index.html\n\n\n\n\nFAQ on cross-validation\nStan runtime warnings and convergence problems explained",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html",
    "href": "src/01b_basic_workflow.html",
    "title": "2  Basic workflow",
    "section": "",
    "text": "2.1 Simple example\nHere we will use a simplified version of the case study presented in Chapter 3. Specifically, we will run a random-effects meta-analysis for a binary responder variable which has been measured in multiple trials:\n# load historical data on responses by arm from the RBesT package:\narm_data &lt;- RBesT::AS\nknitr::kable(arm_data)\n\n\n\n\nstudy\nn\nr\n\n\n\n\nStudy 1\n107\n23\n\n\nStudy 2\n44\n12\n\n\nStudy 3\n51\n19\n\n\nStudy 4\n39\n9\n\n\nStudy 5\n139\n39\n\n\nStudy 6\n20\n6\n\n\nStudy 7\n78\n9\n\n\nStudy 8\n35\n10\nWhile in the case study an informative prior for a future study will be derived, we here restrict the analysis to a random-effects meta-analysis with the goal to infer the mean response rate of the control arm as measured in the 8 reported studies.\nThe statistical model we wish to fit to this data is a random-effects varying intercept model\n\\[ y_i|\\theta_i,n_i \\sim \\mbox{Binomial}(\\theta_i,n_i) \\] \\[ \\mbox{logit}{(\\theta_i)}|\\beta,\\eta_i = \\beta + \\eta_i \\] \\[ \\eta_i|\\tau \\sim \\mbox{Normal}(0, \\tau^2).\\]\nThus, each study \\(i\\) will recieve it’s study-specific response rate \\(\\theta_i\\) as given by the sum of the study-specific random effect \\(\\eta_i\\) and the overall typical responder rate \\(\\beta\\), which are inferred on the logit scale. We choose the priors in a conservative manner aligned with the case study\n\\[ \\beta \\sim \\mbox{Normal}(0, 2^2)\\] \\[ \\tau \\sim \\mbox{Normal}^+(0, 1).\\]",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#r-session-setup",
    "href": "src/01b_basic_workflow.html#r-session-setup",
    "title": "2  Basic workflow",
    "section": "2.2 R session setup",
    "text": "2.2 R session setup\nIdeally your R session will have access to sufficient computational resources, e.g. at least 4 CPU cores and 8000 MB of RAM. The 4 cores are needed to run multiple chains in parallel and the RAM is used during compilation of Stan models.\nWhen working with brms in an applied modeling setting, one often wishes to fit various related models and compare their outputs. With this in mind a recommended preamble for an analysis R script may start with:\n\nlibrary(brms)\n# tools to process model outputs\nlibrary(posterior)\n# useful plotting facilities\nlibrary(bayesplot)\n# further customization of plots\nlibrary(ggplot2)\n# common data processing utilities\nlibrary(dplyr)\nlibrary(tidyr)\n# other utilities\nlibrary(knitr) # used for the \"kable\" command\nlibrary(here)  # useful to specify path's relative to project\n# ...\n\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\n\n# set the R random seed of the session for reproducibility\nset.seed(5886935)\n\nAs brms models are translated to Stan model files, which must be compiled as a C++ program before the model is run, it is useful to cache this compilation step such that repeated model evaluation avoids the compilation (speeding up model reruns with different data or model reruns after restarting R). Thus, the brms.backend option cmdstanr is configured as a default. Doing so allows to cache the binary Stan executables in the directory configured with the option cmdstanr_write_stan_file_dir.\nIn case the model fitted is taking a long time for one chain (more than a minute at least), then it is advisable to turn on parallelization of the multiple chains by default by adding to the preamble:\n\n# use 4 cores for parallel sampling by default - only recommended if\n# model takes &gt; 1 minute to run for each chain otherwise the\n# parallelization introduces substantial overhead\noptions(mc.cores = 4)\n\nIt is discouraged to run chains always in parallel, since the parallelization itself consumes some ressources and is only beneficial if the model runtime is at least at the order of minutes. In case the model runtime becomes excessivley long, then each chain can itself be run with multiple cores as discussed in the section Parallel computation.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-formula",
    "href": "src/01b_basic_workflow.html#model-formula",
    "title": "2  Basic workflow",
    "section": "2.3 Model formula",
    "text": "2.3 Model formula\nWith brms statistical models are specified via a model formula and specification of a response family. The family encodes the likelihood and link functions used for the distribution parameters. A distribution parameter is, for example, the response rate of a binomial outcome, the counting rate for a negative binomial endpoint or the overdispersion of a negative binomial.\nIn general formulaes can be recognized by a ~ sign. Anything on the left-hand side of the ~ describes the response (outcome) while terms on the right-hand side setup the design matrix, random effects or even a non-linear model formula. The left-hand side and the right-hand side can contain special terms which encode additional information and the chosen example makes use of this feature. It’s good practice to store the brms model formula in a separate R variable like:\n\nmodel_meta_random &lt;- bf(r | trials(n) ~ 1 + (1 | study), family=binomial)\n\nThe left-hand side of the formula r | trials(n) encodes the main outcome response variable r (number of responders) and it passes along additional information needed for the binomial response here, the number of respondents using trials(n). Using the same notation, the case-study “Meta-analysis to estimate treatment effects” shows how the exposure time can be varyied for count outcomes. The right-hand side of the formula specifies a linear predictor 1. This defines the fixed effect design matrix, which is the intercept only term in this example, but covariates could be included here. In addition, the term (1 | study) defines a random effect with a by study varying intercept. Finally, the family argument specifies the statistical family being used. The link function is by default a logit link for the binomial family. Note that brms supports a large set of families, please refer to\n\n?brmsfamily\n\nfor an overview on the available families in brms. In case your specific family neededed is not available, users even have the option to define their own family. This is a somewhat advanced use of brms, but as brms is designed to accomodate many different families this is in fact not too difficult and a full vignette is available online.\nAn alternative model in this context could be a fixed effect only model:\n\nmodel_meta_fixed &lt;- bf(r | trials(n) ~ 1, family=binomial)\n\nNote that for some models one may even need to specify multiple model formulas. This can be the case whenever multiple distribution parameters must be specified (a negative binomial needs a mean rate model and a model for the dispersion parameter) or whenever a non-linear model is used, see the case study on dose finding in section @ref(dose-finding).",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#prior-specification",
    "href": "src/01b_basic_workflow.html#prior-specification",
    "title": "2  Basic workflow",
    "section": "2.4 Prior specification",
    "text": "2.4 Prior specification\nThe specification of priors is not strictly required for generalized linear models in brms. In this case very wide defaults priors are setup for the user. However, these only work well whenever a lot of data is available and it is furthermore a much better practice to be specific about the priors being used in a given analysis.\nIn order to support the user in specifying priors, the brms package provides the get_prior function. We start with the simple fixed effect model:\n\nkable(get_prior(model_meta_fixed, arm_data))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\n\n\n\nNote that brms requires the model formula and the data to which the model is being fitted to. This is due to fact that only model and data together define all parameters fully. For example, the model formula could contain a categorical variable and then only knowing all categories as defined by the data allows to define the full model with all parameters. For the fixed effect model only a single parameter, the intercept only term is defined. To now define a prior for the intercept, we may use the prior command as:\n\nprior_meta_fixed &lt;- prior(normal(0,2), class=\"Intercept\")\n\nThe prior for the random effects model is slightly more complicated as it involves a heterogeniety parameter \\(\\tau\\) (standard deviation of the study random effect):\n\nkable(get_prior(model_meta_random, arm_data))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\nstudy\n\n\n\n\n\ndefault\n\n\n\nsd\nIntercept\nstudy\n\n\n\n\n\ndefault\n\n\n\n\n\nGiven that the random effects model is a generalization of the fixed effect model, it is natural to write the prior in a way which expands the fixed effect case as:\n\nprior_meta_random &lt;- prior_meta_fixed +\n    prior(normal(0,1), class=\"sd\", coef=\"Intercept\", group=\"study\")\n\nThe logic to defined priors on specifc parameters is to use the different parameter identifiers as provided by the get_prior command. In this context class, coef and group is used for parameter class, parameter coefficient and grouping, respectivley. The identifiers resp, dlpar and nlpar are used in the context of multiple responses, multiple distribution parameters oe non-linear parameters, respectivley. It is preferable to be specific as above in the definition of priors while it is admissable to be less specific like:\n\nprior_meta_random &lt;- prior_meta_fixed +\n    prior(normal(0,1), class=\"sd\")\n\nThis statement assign to all random effect standard deviations of the model the same prior, which can be convenient in some situations.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#fitting-the-model",
    "href": "src/01b_basic_workflow.html#fitting-the-model",
    "title": "2  Basic workflow",
    "section": "2.5 Fitting the model",
    "text": "2.5 Fitting the model\nHaving defined the model formula, the family and the priors we can now fit the models with the brm command.\n\nfit_meta_fixed  &lt;- brm(model_meta_fixed, data=arm_data, prior=prior_meta_fixed,\n                       ## setup Stan sampler to be more conservative, but more robust\n                       control=list(adapt_delta=0.95),\n                       ## these options silence Stan\n                       refresh=0, silent=TRUE,\n                       seed=4658758)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nfit_meta_random  &lt;- brm(model_meta_random, data=arm_data, prior=prior_meta_random,\n                        control=list(adapt_delta=0.95),\n                        refresh=0, silent=TRUE,\n                        seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\n\nWhen calling brm a Stan model file will be created, compiled and run with the data provided. The arguments used are:\n\nformula is the first argument here. It specifies the model. Since we have in this case provided family as part of the formula, we do not need to specify it as argument to brm, which is also possible to do.\ndata the data used to fit.\nprior definition of the priors for all model parameters\ncontrol defines additional control arguments to the Stan sampler. A higher than standard adapt_delta (defaults to 0.8) causes the sampler to run less aggressively which makes the sampler more robust, but somewhat slower.\nrefresh & silent are used here to suppress Stan sampling output.\nseed sets the seed use for the fit.\n\nIn the course of an exploratory analysis one often wishes to modify a given model slightly to study, for example, the sensitivity to different assumptions. For this purpose the update mechanism is a convenient way to simply change certain arguments specified previously. To change the prior on the study level random effect parameter one may use:\n\nprior_meta_random_alt &lt;- prior_meta_fixed +\n    prior(normal(0,0.5), class=\"sd\", coef=\"Intercept\", group=\"study\")\n    \nfit_meta_random_alt  &lt;- update(fit_meta_random, prior=prior_meta_random_alt,\n                               control=list(adapt_delta=0.95),\n                               refresh=0, silent=TRUE,\n                               seed=6845736)\n\nThe desired updates require recompiling the model\n\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\n\nAs changing the prior results in this case in a need to recompile the model, the benefits of using update are not large in this case. However, whenever a model recompilation is not needed, then using update significantly speeds up the workflow as the compilation step is avoided, e.g. when looking at data subsets:\n\nprior_meta_random_alt &lt;- prior_meta_fixed +\n    prior(normal(0,0.5), class=\"sd\", coef=\"Intercept\", group=\"study\")\n    \nfit_meta_random_alt2  &lt;- update(fit_meta_random, newdata=slice_head(arm_data, n=4),\n                                control=list(adapt_delta=0.95),\n                                refresh=0, silent=TRUE,\n                                seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\nWarning: 1 of 4000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nNow the fit starts instantly leading to a faster and more convenient modeling workflow.\nWhen working with brms you will encounter warnings now and then which originate from Stan itself. While brms attempts to add explanations to these warnings as to what they mean and how to avoid them, the Stan community has provided an online help-page on possible warnings occuring during a Stan fit.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-summary",
    "href": "src/01b_basic_workflow.html#model-summary",
    "title": "2  Basic workflow",
    "section": "2.6 Model summary",
    "text": "2.6 Model summary\nOnce models are fit, we obtain - by default - a posterior sample of the model. Given that priors are used in such an analysis it can be convenient to first consider the priors which were used to create a given fitting object like:\n\nkable(prior_summary(fit_meta_random))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nnormal(0, 2)\nIntercept\n\n\n\n\n\n\n\nuser\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\nstudy\n\n\n\n\n\ndefault\n\n\nnormal(0, 1)\nsd\nIntercept\nstudy\n\n\n\n\n\nuser\n\n\n\n\n\nAn even more explicit way to analyze the prior of a model is to sample it directly:\n\nprior_meta_random  &lt;- update(fit_meta_random, sample_prior=\"only\",\n                             control=list(adapt_delta=0.95),\n                             refresh=0, silent=TRUE,\n                             seed=5868467)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\n\nThis will sample the model in the usual way, but simply leave out the terms implied by the data likelihood. The resulting model sample can be analyzed in exactly the same way as the model posterior itself. This technique is called prior (predictive) checks.\nReturning to the model posterior, the obvious first thing to do is to simply print the results:\n\nprint(fit_meta_random)\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 1 + (1 | study) \n   Data: arm_data (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.39      0.21     0.04     0.89 1.00     1040      969\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.10      0.19    -1.46    -0.71 1.00     1491     1834\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe see that we fitted the default 4 chains and ran each chain for 1000 warmup iterations and 2000 total iterations such that we are left with 4k iterations overall. brms reports by default as estimate the mean of the posterior with it’s standard error and the 95% credible interval. The Rhat column is a convergence diagnostic which should be close to 1.0. Values above 1.1 indicate non-convergence of the Markov Chain for the particular parameter. The additional two columns on bulk and tail ESS indicate statistical information on central moments and tail properties of the target quantitiy. The ESS is the effective sample size of the MC estimator, which assumes that the estimation error of the mean scales with \\(1/\\sqrt{ESS}\\). An ESS of ~200 is usually sufficient for a precise estimate of the mean. It is important to note that the Stan HMC sampler often requires fewer iterations (for which more time is spent) as compared to BUGS or JAGS in order to reach a comparable ESS. For more details on the ESS, please refer to the Stan reference manual on ESS.\nbrms supports the common R functions to extract model results:\n\nfitted to get the posterior estimates of expected values of for each data row (no sampling uncertainty)\npredict to get the posterior predictive estimates of the response for each data row (includes sampling uncertainty)\ncoef/ranef to get the random effect estimates including/excluding the linear predictor part\nmany more, please refer to the reference manual of brms\n\nOf note is the argument summary, which is used in a number of these functions. The default is set to TRUE such that the output are summaries of the posterior sample in the form of means, credible intervals, etc. When setting the argument summary=FALSE then the posterior sample is returned in the format of a matrix. Each row of the matrix is one iteration while the columns of the matrix run with the rows of the input data-set.\nFor example, we can compare the estimated mean response fo the two different models as:\n\nfitted(fit_meta_random)\n\n      Estimate Est.Error      Q2.5     Q97.5\n[1,] 24.331054  3.735007 17.183654 31.875063\n[2,] 11.544028  2.191310  7.565441 16.378010\n[3,] 16.002830  2.961917 11.039539 22.567141\n[4,]  9.412544  1.869250  5.841321 13.179245\n[5,] 37.769373  4.748738 29.170767 47.811069\n[6,]  5.359016  1.232845  3.174211  8.080594\n[7,] 13.531353  3.402142  7.355087 20.241300\n[8,]  9.335581  1.922931  6.012867 13.677123\n\nfitted(fit_meta_fixed)\n\n      Estimate Est.Error      Q2.5     Q97.5\n[1,] 26.533687 2.0380505 22.574910 30.640824\n[2,] 10.911049 0.8380768  9.283141 12.599965\n[3,] 12.646898 0.9714072 10.760004 14.604505\n[4,]  9.671157 0.7428408  8.228238 11.168151\n[5,] 34.468995 2.6475609 29.326285 39.804435\n[6,]  4.959568 0.3809440  4.219609  5.727257\n[7,] 19.342314 1.4856817 16.456477 22.336302\n[8,]  8.679243 0.6666520  7.384316 10.022699\n\n\nAs expected, the standard errors for the fixed effect analysis are considerably smaller.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/01b_basic_workflow.html#model-analysis",
    "href": "src/01b_basic_workflow.html#model-analysis",
    "title": "2  Basic workflow",
    "section": "2.7 Model analysis",
    "text": "2.7 Model analysis\nHow to analyze a given model obviously depends very much on details of the problem at hand. Often we wish to evaluate how well the model describes the problem data used to fit the model. Thus, the ability of the model to describe certain summaries of the data set accuratley is one way to critize the model. Such an approach requires to compare predictions of the data by the model \\(y_{rep}\\) to the actual data \\(y\\). This procedure is referred to as posterior predictive checks. A key feautre of the approach is to account for sampling uncertainty implied by the endpoint and sample size.\nFor a meta-analysis we may wish to check how well the summary statistics of the historical data is predicted by the model. The brms package integrates with the bayesplot package for the purpose of posterior predictive checks, which could look in this case like:\n\n# create intervals plot and label plot accordingly\npp_check(fit_meta_random, type=\"intervals\", ndraws=NULL) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"Random effects model\")\n\n\n\n\n\n\n\npp_check(fit_meta_fixed, type=\"intervals\", ndraws=NULL) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"Fixed effects model\")\n\n\n\n\n\n\n\n\nShown is a central 50% credible interval with a thick line, a thinner line shows the 90% credible interval, the open dot marks the median prediction and the filled dark dot marks the observed value of the data. We see is that in particular study 7 is predicted unsatisfactory by the fixed effect model. An additional interesting predictice check in this case is in view of a possibile use as historical control information part of a new study. While the above predictive check of the random effects was conditional on the fitted data, we can instead use the random effects model and predict each study as if it were a new study:\n\n# create intervals plot and label plot accordingly\npp_arm_data_new &lt;- posterior_predict(fit_meta_random,\n                                     newdata=mutate(arm_data, study=paste0(\"new_\", study)),\n                                     allow_new_levels=TRUE,\n                                     sample_new_levels=\"gaussian\")\n\n# use bayesplot::ppc_intervals directly here\nppc_intervals(y=arm_data$r, yrep=pp_arm_data_new) +\n    scale_x_continuous(\"Study\", breaks=1:nrow(arm_data), labels=arm_data$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip(ylim=c(0, 50)) +\n    theme(legend.position=\"right\",\n          ## suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check - new studies\", \"Random effects model\")\n\n\n\n\n\n\n\n\nNow the credible intervals are wider as these contain additional variability. As for study 7 the random effect is not anymore conditioned on the data of the study, we see again that the model struggles to account for the study nicely. However, with the additional heterogeniety the result of study 7 is at least plausible given that it is contained in the 90% credible interval.\nIn the above discussion we have ensured the comparability of the plots via matching the axis limits. A more straightforward is a side by side comparison of the models, which can be achieved like:\n\n# use bayesplot::ppc_intervals_data directly here\nmodel_cmp &lt;- bind_rows(random_new=ppc_intervals_data(y=arm_data$r, yrep=pp_arm_data_new),\n                       random=ppc_intervals_data(y=arm_data$r, yrep=posterior_predict(fit_meta_random)),\n                       fixed=ppc_intervals_data(y=arm_data$r, yrep=posterior_predict(fit_meta_fixed)),\n                       .id=\"Model\") %&gt;%\n    # add in labels into the data-set\n    left_join(select(mutate(arm_data, x=1:8), x, study), by=\"x\")\n\nggplot(model_cmp, aes(study, m, colour=Model)) +\n    geom_pointrange(aes(ymin=ll, ymax=hh), position=position_dodge(width=0.4)) +\n    geom_point(aes(y=y_obs), colour=\"black\") +\n    ylab(\"Number of Responders\") +\n    xlab(\"Study\") +\n    coord_flip() +\n    theme(legend.position=\"right\",\n          # suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank()) +\n    ggtitle(\"Posterior predictive check\", \"All models\")\n\n\n\n\n\n\n\n\nA more formal way to compare these models is to consider their ability to predict new data. In absence of new data we may instead turn to scores which evaluate the ability of the model to predict data which has been left out when fitting the model. The leave-one-out scores can be calculated using fast approximations (see loo command) whenever the data-set is large enough. We refer the reader to the case study on Dose finding where these model comparisons are discussed in greater detail.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic workflow</span>"
    ]
  },
  {
    "objectID": "src/02_case_studies.html",
    "href": "src/02_case_studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Problem\nTechnique\n\n\n\n\nUse of historical control data\nnested random effects\n\n\nMeta-analysis to estimate treatment effects\naggregate data modeling & varying exposure times of count data\n\n\nDose finding\nnon-linear models\n\n\nOncology dose escalation\nconstrained parameters\n\n\nMultiple imputation\nmulti-variate outcome modeling\n\n\nLongitudinal data\nlongitudinal modeling with different covariance structures (MMRM)\n\n\nBayesian Mixed effects Model for Repeated Measures (MMRM)\nunstructured MMRM for a continuous endpoint\n\n\nTime-to-event data\nparametric time-to-event modeling with customized parametrization by using user-defined contrasts\n\n\nNetwork meta-analysis\narm based network meta-analysis",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html",
    "href": "src/02a_meta_analysis.html",
    "title": "3  Use of historical control data",
    "section": "",
    "text": "3.1 Background\nGiven the relevance of the use of historical control data problem for drug development, a full R package RBesT (R Bayesian evidence synthesis tools) is available on CRAN. Here we will re-implement the example of the vignette of RBesT for the binary case and will illustrate how brms can be used in a more complex setting as a case study. In particular, we are going to assume as a complication that the historical trial data has been collected in specific regions of the world and how this can be used to borrow strength between regions. As a simplifying assumption it is assumed that trials are nested within regions thereby implying that trials are conducting exclusively in specific regions.\nFor details on the RBesT R package, please refer to",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#background",
    "href": "src/02a_meta_analysis.html#background",
    "title": "3  Use of historical control data",
    "section": "",
    "text": "Weber et al. (2021) doi:10.18637/jss.v100.i19 for details on applying the RBesT package, and\nNeuenschwander et al. (2010) doi:10.1177/1740774509356002 and\nSchmidli et al. (2014) doi:10.1111/biom.12242 for details on the MAP methodology.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#data",
    "href": "src/02a_meta_analysis.html#data",
    "title": "3  Use of historical control data",
    "section": "3.2 Data",
    "text": "3.2 Data\nA Phase II study is planned to evaluate the efficacy of a test treatment in a randomized comparison with placebo in the disease ankylosing spondilityis. At the design stage of the trial control group data were available from a total of eight historical studies.\nThis data-set is part of the RBesT package as the AS data-set and here we add as additional column a randomly assigned region variable:\n\nlibrary(RBesT)\nAS_region &lt;- bind_cols(AS, region=sample(c(\"asia\", \"europe\", \"north_america\"), 8, TRUE))\nkable(AS_region)\n\n\n\n\nstudy\nn\nr\nregion\n\n\n\n\nStudy 1\n107\n23\nasia\n\n\nStudy 2\n44\n12\nnorth_america\n\n\nStudy 3\n51\n19\nasia\n\n\nStudy 4\n39\n9\nnorth_america\n\n\nStudy 5\n139\n39\neurope\n\n\nStudy 6\n20\n6\neurope\n\n\nStudy 7\n78\n9\neurope\n\n\nStudy 8\n35\n10\neurope\n\n\n\n\n\nThe total number of 513 patients in the 8 trials is quite substantial.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#model-description",
    "href": "src/02a_meta_analysis.html#model-description",
    "title": "3  Use of historical control data",
    "section": "3.3 Model description",
    "text": "3.3 Model description\nThe RBesT package implements the MAP approach following a standard generalized linear modeling framework for a random-effects meta-analysis:\n\n\\(Y\\) is the (control) group summary data for \\(H\\) historical trials\n\\(Y_{h}|\\theta_{h} \\sim f(\\theta_{h})\\)\n\\(g(\\theta_{h}) = \\beta + \\eta_h\\)\n\\(\\eta_h|\\tau \\sim \\mbox{Normal}(0, \\tau^2)\\)\n\\(f\\) likelihood: Binomial, Normal (known \\(\\sigma\\)) or Poisson\n\\(g\\) link function for each likelihood \\(f\\): \\(\\mbox{logit}\\), identity or \\(\\log\\)\n\\(\\beta\\) population mean with prior \\(\\mbox{Normal}(m_{\\beta}, s_{\\beta}^2)\\)\n\\(\\tau\\) between-trial heterogeneity with prior \\(P_\\tau\\)\n\nThe priors used for this data-set will be:\n\n\\(\\beta \\sim \\mbox{Normal}(0, 2^2)\\)\n\\(\\tau \\sim \\mbox{Normal}^+(0, 1)\\)\n\nWe will first run the analysis with the RBesT command gMAP. As a next step we will convert the analysis to use brms for the inference. Finally, we will add an additional random effect for the region \\(j\\) and treat the random effect for the studies to be nested within the region. As the more general model requires two levels of random effects, it is outside the possible models of RBesT. Such a more general region specific model can be useful in various situations whenever we wish to borrow strength across regions. Denoting with \\(j\\) specific regions, the more general model is then:\n\n\\(Y_{h,j}|\\theta_{h,j} \\sim f(\\theta_{h,j})\\)\n\\(g(\\theta_{h,j}) = \\beta + \\eta_h + \\nu_j\\)\n\\(\\eta_h|\\tau \\sim \\mbox{Normal}(0, \\tau^2)\\)\n\\(\\nu_j|\\omega \\sim \\mbox{Normal}(0, \\omega^2)\\)\n\nIn our case study we make a simplifying assumption that any trial \\(h\\) is run entirely within a given region \\(j\\). Therefore we have a nested structure (trials within regions) such that no correlation is modeled between region and trial. This would be different if some trials were run across different regions and trial results would be reported by region.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#implementation",
    "href": "src/02a_meta_analysis.html#implementation",
    "title": "3  Use of historical control data",
    "section": "3.4 Implementation",
    "text": "3.4 Implementation\nWith the gMAP command in RBesT we can obtain MCMC samples from posterior for the first model as follows:\n\nset.seed(34767)\nmap_mc_rbest &lt;- gMAP(cbind(r, n-r) ~ 1 | study,\n                     family=binomial,\n                     data=AS_region,\n                     tau.dist=\"HalfNormal\", tau.prior=1,\n                     beta.prior=cbind(0,2))\n\nmap_mc_rbest\n\nGeneralized Meta Analytic Predictive Prior Analysis\n\nCall:  gMAP(formula = cbind(r, n - r) ~ 1 | study, family = binomial, \n    data = AS_region, tau.dist = \"HalfNormal\", tau.prior = 1, \n    beta.prior = cbind(0, 2))\n\nExchangeability tau strata: 1 \nPrediction tau stratum    : 1 \nMaximal Rhat              : 1 \n\nBetween-trial heterogeneity of tau prediction stratum\n  mean     sd   2.5%    50%  97.5% \n0.3730 0.2040 0.0441 0.3490 0.8450 \n\nMAP Prior MCMC sample\n  mean     sd   2.5%    50%  97.5% \n0.2560 0.0863 0.1090 0.2470 0.4710 \n\n\nUsing brms we now specify the MAP model step by step. Binomial data is specified slightly different in brms. We first define the model:\n\nmodel &lt;- bf(r | trials(n) ~ 1 + (1 | study), family=binomial)\n\nThe left hand side of the formula, r | trials(n) ~ ..., denotes with r the data being modeled - the number of responders - and adds with a bar | additional information on the response, which are the number of overall trials, needed to interpret the binomial likelihood.\nWith the model (and data) being defined, we are left to specify the model priors. With the help of the call\n\nget_prior(model, AS_region)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd           study                  0   \n student_t(3, 0, 2.5)        sd Intercept study                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n\n\nwe can ask brms as to what model parameters it has detected for which priors should be specified. In this example, we need to define the population mean intercept (\\(\\beta\\)) and the between-study heterogeneity parameter (\\(\\tau\\)):\n\nmodel_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=sd, coef=Intercept, group=study)\n\nNow we are ready to run the model in brms (we are setting refresh=1000 to suppress most progress output):\n\nmap_mc_brms  &lt;- brm(model, AS_region, prior=model_prior,\n                    seed=4767, refresh=1000)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\n\nThe model is compiled and then run. Occasionally one observes a warning on divergent transitions after warmup reported like:\n\n## Warning: There were 1 divergent transitions after warmup.\n\nThis is caused in this case by the choice of very conservative priors, which lead to a difficult to sample posterior. As a quick fix we may reduce the aggressiveness of the sampler and increase the sampler parameter on the target acceptance probability adapt_delta from it’s default value \\(0.8\\) to a value closer to the maximum possible value of \\(1.0\\). For most analyses with weak priors using a value of \\(0.95\\) can be used as a starting value. This is at the cost of some sampling speed as the sampler will take smaller steps, but the choice of a higher than default acceptance probability results in more robust inference and avoids in many instances the warning about divergences. For a more comprehensive overview on possible warnings, their meanings and how to address these, please refer to the online help of the Stan project on possible Stan stampler warnings and messages.\nIn order to also avoid having to compile the Stan code for the model once more, we use the update functionality of brms:\n\nmap_mc_brms_2 &lt;- update(map_mc_brms, control=list(adapt_delta=0.95),\n                        # the two options below only silence Stan sampling output\n                        refresh=0, silent=0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\nmap_mc_brms_2\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 1 + (1 | study) \n   Data: AS_region (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.38      0.22     0.03     0.89 1.00      787      667\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -1.10      0.19    -1.48    -0.74 1.00     1512     1537\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can see that the estimate of the between-study heterogeneity \\(\\tau\\) is very similar between RBesT and brms. However, the MAP prior is not apparent from the output of brms directly (as it’s not designed with this specific application in mind).\nTo obtain the MAP prior from brms, we have to predict the response rate of a new study. To do so, a new data set with the same columns as the modeling data sets needs to be created.\n\nAS_region_new &lt;- data.frame(study=\"new_study_asia\", r=0, n=6, region=\"asia\")\npost_map_mc_brms &lt;- posterior_linpred(map_mc_brms_2,\n                                      newdata=AS_region_new,\n                                      # apply inverse link function\n                                      transform=TRUE,\n                                      # allows new studies\n                                      allow_new_levels = TRUE,\n                                      # and samples these according to the model\n                                      sample_new_levels = \"gaussian\"\n                                      )\n# Let's have a look at what we got:\nstr(post_map_mc_brms)\n\n num [1:4000, 1] 0.352 0.412 0.195 0.305 0.331 ...\n\n\nModel outputs are returned in the standard format of a matrix which contains the model simulations. While the rows label the draws, the columns go along with the rows of the input data set. As in this case we have as input data set a 1-row data frame AS_region_new corresponding to predictions for a (single) new study, the output is a 1 column matrix with 4000 rows, since 4000 draws in total were obtained from the sampler run with 4 chains and 1000 draws per chain from the sampling phase.\nNote the following important arguments used to obtain the posterior:\n\ntransform=TRUE applies automatically the inverse link function such that we get response rates rather than logit values.\nallow_new_levels=TRUE is needed to instruct brms that new levels of the fitted random effects are admissible in the data. In this case we sample a new study random effect level.\nsample_new_levels=\"gaussian\" ensures that the new random effect is sampled according to normal distributions as specified with the model. The default option \"uncertainty\" samples for each draw from the fitted random effect levels one realization, which is essentially bootstrapping random effects on the level of posterior draws. The option \"old_levels\" samples a random effect level and substitutes all draws for the new level corresponding to bootstrapping the existing levels. While this avoids normality assumptions, it can only work well in situations with many levels of the random effect. The option \"gaussian\" is for most models the preferred choice and for more details, please refer to the brms help page on prepare_predictions.\n\nA convenient way to get a summary of the samples is to use the summarize_draws function from the posterior package (used as a helper package in brms already):\n\nsummarize_draws(post_map_mc_brms)\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ...1     0.258  0.250 0.0859 0.0650 0.134 0.408  1.00    2688.    2727.\n\n\nThese estimates are now very similar to the results reported from RBesT reported above (up to sampling error).\nExpanding the model to include region would only be possible in RBesT via the use of an additional fixed effect. However, this would essentially refit the model for each region separately and hence limit the amount of information we can borrow among regions. With brms it is straightforward to specify the nested random effects structure described in the Model Details Section. Following the same steps as before, setting up the brms model may look like:\n\nregion_model &lt;- bf(r | trials(n) ~ 1 + (1 | region/study), family=binomial)\nget_prior(region_model, AS_region)\n\n                prior     class      coef        group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                             \n student_t(3, 0, 2.5)        sd                                         0   \n student_t(3, 0, 2.5)        sd                 region                  0   \n student_t(3, 0, 2.5)        sd Intercept       region                  0   \n student_t(3, 0, 2.5)        sd           region:study                  0   \n student_t(3, 0, 2.5)        sd Intercept region:study                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\nregion_model_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 0.5), class=sd, coef=Intercept, group=region) +\n    prior(normal(0, 0.25), class=sd, coef=Intercept, group=region:study)\nregion_map_mc_brms  &lt;- brm(region_model, AS_region, prior=region_model_prior, seed=4767,\n                           control=list(adapt_delta=0.99),\n                           refresh=0, silent=0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\npost_region_map_mc_brms &lt;- posterior_linpred(region_map_mc_brms,\n                                             newdata=AS_region_new,\n                                             transform=TRUE,\n                                             allow_new_levels = TRUE,\n                                             sample_new_levels = \"gaussian\"\n                                             )\n# Let's have a look at what we got:\nsummarize_draws(post_region_map_mc_brms)\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ...1     0.262  0.257 0.0666 0.0568 0.162 0.381  1.00    3236.    3586.\n\n\nThe key difference to the previous model is the nested random effect specification term (1 | region/study) of the model formula. This syntax denotes a random intercept term for region and study in a way which assumes a nested data structure in that a given study is only run in a single region.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#results",
    "href": "src/02a_meta_analysis.html#results",
    "title": "3  Use of historical control data",
    "section": "3.5 Results",
    "text": "3.5 Results\nOnce the MAP prior is obtained in MCMC form a model check of is recommended. In RBesT a forest plot augmented with model shrinkage estimates is suggested for this purpose:\n\nplot(map_mc_rbest)$forest_model\n\n\n\n\n\n\n\n\nThe dashed lines show the 95% confidence intervals of each study estimate on it’s own while the solid line shows the respective shrinkage estimate of the MAP model. This plot is useful to assess the plausibility of the results and may unveil possible issues with the model specification. In brms model diagnostic functions are directly available and essentially expose the functionality found in the bayesplot R package. A suitable bayesplot plot in this situation could be an intervals plot as:\n\npp_check(map_mc_brms_2, type=\"intervals\") +\n    scale_x_continuous(\"Study\", breaks=1:nrow(AS_region), labels=AS_region$study) +\n    ylab(\"Number of Responders\") +\n    coord_flip() +\n    theme(legend.position=\"right\",\n          # suppress vertical grid lines for better readability of intervals\n          panel.grid.major.y = element_blank())\n\nUsing all posterior draws for ppc type 'intervals' by default.\n\n\n\n\n\n\n\n\n\nThe call of the pp_check function is forwarded to the respective ppc_* functions for posterior predictive checks from bayesplot (depending on the type argument). The plots are designed to compare the posterior predictive distribution to the observed data rather than comparing mean estimates to one another. Thus, the outcome of each trial in the original data set is sampled according to the fitted model and the resulting predictive distribution of the outcome (number of responders) is compared to the observed outcome. The intervals predictive probability check summarises the predictive distributions using a light color for an outer credible interval range and a darker line for an inner credible interval. The outer defaults to a 90% credible interval (prob_outer argument) while the inner uses a 50% credible interval (prob argument). The light dot in the middle is the median of the predictive distribution and the dark dot is the outcome \\(y\\). As we can observe, the outcomes \\(y\\) of the trials all are contained within outer credible intervals of the predictive distributions for the simulated replicate data \\(y_{rep}\\). However, one may critizise that also the 50% credible intervals contain all but two trials (study 3, study 7). Hence, the calibration of the model with the data is possibly not ideal given that every other trial outcome should be outside (or inside) of the 50% predictive interval. Comparing with a binomial distribution one can find that such an outcome can occur in 14% of the cases and does not represent an extreme finding such that we can conclude that the model is consistent with the data.\nOnce the model has been checked for plausibility, we can proceed and derive the main target of the MAP analysis, which is the MAP prior in parametric form. RBesT provides a fitting procedure, based in the EM algorithm, for approximating the MCMC output of the MAP prior in parametric form using mixture distributions. In the case of a binomial response Beta mixtures are being estimated:\n\nmap_rbest &lt;- automixfit(map_mc_rbest)\n\nAnd a comparison of the fitted density vs the histogram of the MCMC sample is available as:\n\nplot(map_rbest)$mix\n\n\n\n\n\n\n\n\nThe automixfit function above recognizes that the map_mc_rbest object is a gMAP analysis object and automatically calls the correct Beta EM mixture algorithm for proportions. When working with brms we also do obtain the MAP prior in MCMC form on the response scale, but we need to provide automixfit additional information on the provided MCMC sample like this:\n\nmap_brms &lt;- automixfit(post_map_mc_brms[,1], type=\"beta\")\n\nAt this stage we can work with map_brms_2 just like we would when using RBesT directly such that the graphical diagnostic of the fit still works:\n\nplot(map_brms)$mix\n\n\n\n\n\n\n\n\nComparing the results of using either packages shows that the two resulting MAP prior distributions are representing the same evidence (up to MCMC sampling error):\n\nkable(rbind(rbest=summary(map_rbest),\n            brms=summary(map_brms)),\n      digits=3)\n\n\n\n\n\nmean\nsd\n2.5%\n50.0%\n97.5%\n\n\n\n\nrbest\n0.256\n0.086\n0.105\n0.247\n0.472\n\n\nbrms\n0.259\n0.086\n0.111\n0.249\n0.465\n\n\n\n\n\nFor the region specific model, two different types of priors can be derived. One may wish to obtain a MAP prior for one of the considered regions or for a new region:\n\n# predict a new study for all fitted region and other (=a new region)\nAS_region_all &lt;- data.frame(region=c(\"asia\", \"europe\", \"north_america\", \"other\")) %&gt;%\n    mutate(study=paste(\"new_study\", region, sep=\"_\"), r=0, n=6)\n\npost_region_all_map_mc_brms &lt;- posterior_linpred(region_map_mc_brms,\n                                                 newdata=AS_region_all,\n                                                 transform=TRUE,\n                                                 allow_new_levels = TRUE,\n                                                 sample_new_levels = \"gaussian\"\n                                                 )\n\n\n# name columns according to their region...\ncolnames(post_region_all_map_mc_brms) &lt;- AS_region_all$region\n\n#...to obtain nice labels in a visualization with bayesplot\nbayesplot::mcmc_intervals(post_region_all_map_mc_brms)\n\n\n\n\n\n\n\n# obtain parametric mixture for each region, always using \n# 3 mixture components (often sufficient) to speed up inference\nmap_region &lt;- list()\nfor(r in AS_region_all$region) {\n    map_region[[r]] &lt;- mixfit(post_region_all_map_mc_brms[,r], type=\"beta\", Nc=3, constrain_gt1=TRUE)\n}\n\nThese MAP priors summaries are:\n\nkable(bind_rows(lapply(map_region, summary), .id=\"MAP\"), digits=3)\n\n\n\n\nMAP\nmean\nsd\n2.5%\n50.0%\n97.5%\n\n\n\n\nasia\n0.264\n0.067\n0.147\n0.258\n0.426\n\n\neurope\n0.248\n0.064\n0.139\n0.242\n0.411\n\n\nnorth_america\n0.255\n0.071\n0.135\n0.248\n0.431\n\n\nother\n0.260\n0.088\n0.112\n0.251\n0.487\n\n\n\n\n\nThe summaries show that we have higher precision for regions with more trials and the least precision for the MAP prior for a new (\"other\") region, for which there were no trials. An alternative way to quantify the informativeness of the MAP prior is the effective sample size as provided by RBesT:\n\nsapply(map_region, ess)\n\n         asia        europe north_america         other \n     51.22837      61.71589      46.61322      33.76255 \n\n\nAt this point the tools from RBesT can be used to assess further properties of trial designs which use these MAP priors. Please refer to the getting started vignette of RBesT.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#conclusion",
    "href": "src/02a_meta_analysis.html#conclusion",
    "title": "3  Use of historical control data",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nThe random-effects meta-analysis model implemented in RBesT has been re-implemented with brms. In a second step the meta-analysis has been extended to account for trial regions. This enables stronger borrowing within regions and hence a more informative MAP prior as can be seen by the effective sample size measure. Moreover, the case study also demonstrates how posterior samples produced with brms can be used as an input to RBesT such that both tools can be used in combination.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02a_meta_analysis.html#exercises",
    "href": "src/02a_meta_analysis.html#exercises",
    "title": "3  Use of historical control data",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\nCreate a posterior predictive check based on the predictive distribution for the response rate.\nSteps:\n\nUse posterior_predict to create samples from the predictive distribution of outcomes per trial.\nUse sweep(predictive, 2, AS_region$n, \"/\") to convert these samples from the predictive distribution of the outcome counts to samples from the predictive distribution for responder rates.\nUse ppc_intervals from bayesplot to create a plot showing your results.\n\nRedo the analysis with region, but treat region as a fixed effect. Evaluate the informativeness of the obtained MAP priors. The model formula for brms should look like region_model_fixed &lt;- bf(r | trials(n) ~ 1 + region + (1 | study),    family=binomial). Steps:\n\nConsider the prior for the region fixed effect first. The reference region is included in the intercept. The reference region is implicitly defined by the first level of the variable region when defined as factor.\n\nDefine asia to be the reference region in the example. Also include a level other in the set of levels.\nAssume that an odds-ratio of \\(2\\) between regions can be seen as very large such that a prior of \\(\\mbox{Normal}(0, (\\log(2)/1.96)^2)\\) for the region main effect is adequate.\n\nObtain the MAP prior for each region by using the AS_region_all data frame defined above and apply posterior_linpred as shown above.\nConvert the MCMC samples from the MAP prior distribution into mixture distributions with the same code as above.\nCalculate the ESS for each prior distribution with the ess function from RBesT.\n\nRun the analysis for the normal endpoint in the crohn data set of RBesT. Refer to the RBesT vignette for a normal endpoint on more details and context. Steps:\n\nUse as family=gaussian and use the se response modifier in place of trials to specify a known standard error.\nUse the same priors as proposed in the vignette.\nCompare the obtained MAP prior (in MCMC sample form) from RBesT and brms.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use of historical control data</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html",
    "href": "src/02ab_meta_analysis_trtdiff.html",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "",
    "text": "4.1 Background\nMeta-analyses have become a key tool of evidence based medicine (Higgins and Green, 2011, Section 1.2.2) for when we have multiple studies that address the same (or closely related) question(s). Each study may not be able to conclusively answer these questions on its own or the results may vary across the studies. In this scenario, a meta-analysis can quantitatively combine the results of these multiple studies to provide an overall result. Literally, a meta-analysis is a statistical analysis of the analysis results from individual studies (for the purpose of combining the results) (Glass, 1976).\nOne of most common types of meta-analysis in evidence based medicine is that of treatment effects estimated from randomized controlled trials. Meta-analyses of treatment effects from randomized controlled trials importantly respect within study randomization (“analyze as you randomize”). In contrast, simply pooling data from all trial arms ignoring from what study they come, is usually inappropriate and can lead to wildly inappropriate conclusions.\nIn this section we focus on meta-analyses for assessing treatment effects, but meta-analytic methods can be used for combining data across studies for many other purposes. For example, in the next section we willcover predicting control group outcomes for new trials from historical control groups of historical trials. Meta-analyses can be conducted using treatment differences, using outcomes from each trial arm or using individual patient data. We will illustrate all of these approaches.\nIn this section we focus on how to conduct meta-analysis in brms, but ignore a wide range of other important topics. These include the selection of trials for inclusion in meta-analyses, how to handle unavailabile study results, the potential for there to be studies that we are not aware of, and indirect treatment comparisons using network meta-analysis. These topics are covered in-depth in the Cochrane handbook for systematic reviews of interventions, as well as in the book by Borenstein et al. (2009).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#data",
    "href": "src/02ab_meta_analysis_trtdiff.html#data",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "4.2 Data",
    "text": "4.2 Data\nLet us look at a meta-analysis for the effect of sodium-glucose transport protein 2 (SGLT-2) inhibitors on major adverse cardiovascular events (MACE). MACE is a composite endpoint defined by the occurrence of either cardiovascular death, myocardial infarction or stroke. It is usually analyzed using time-to-first-event methods. We will use the data from all 7 SGLT-2 inhibitor trials in the meta-analysis of Barbarawi et al., for which MACE results have been published. This example lets us try out different approaches and input data formats.\nThe first way of inputting data that we will look at is using estimated treatment differences from each study and their standard erorrs. Here, the treatment effect estimate from each study is a hazard ratio from a Cox regression model. The figure below shows the estimate hazard ratios with their 95% confidence intervals from each of the 7 studies with a log-scaled x-axis. For hazard ratios (and similarly for odds and rate ratios) it is preferable to work on the log-scale, because using a normal approximation to the sampling distribution of the estimator works a lot better for smaller sample sizes. Thus, we work with log-hazard ratios and their standard errors.\n\nsglt2_hr = tibble(trial = c(\"DECLARE–TIMI 58\", \"EMPA-REG OUTCOME\",\n                            \"CANVAS\", \"CANVAS-R\", \"CREDENCE\", \n                            \"SCORED\", \"VERTIS CV\"),\n                  drug = c(\"dapagliflozin\", \"empagliflozin\",\n                           \"canagliflozin\", \"canagliflozin\", \"canagliflozin\",\n                           \"sotagliflozin\", \"ertugliflozin\"),\n                  studyno = c(5, 4, 1, 2, 3, 6, 7),\n                  hr = c(0.93, 0.86, 0.88, 0.82, 0.80, 0.77, 0.97),\n                  hr_lcl = c(0.84, 0.74, 0.75, 0.66, 0.67, 0.65, 0.85),\n                  hr_ucl = c(1.03, 0.99, 1.03, 1.01, 0.95, 0.91, 1.11)) %&gt;%\n  arrange(desc(studyno)) %&gt;%\n  mutate(studyno = factor(studyno, levels=studyno, labels=trial),\n         log_hr = log(hr),\n         log_hr_stderr = (log(hr_ucl)-log(hr_lcl))/2/qnorm(0.975))\n\nsglt2_hr %&gt;%\n  ggplot(aes(x=hr, y=studyno, xmin=hr_lcl, xmax=hr_ucl, \n             col=drug, label=drug)) +\n  geom_vline(xintercept=1, linewidth=0.25, linetype=2) +\n  geom_point(shape=19) +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10() +\n  geom_text_repel(data=.%&gt;% filter(studyno %in% c(\"CANVAS\", \"DECLARE–TIMI 58\",\n                                                  \"EMPA-REG OUTCOME\", \"SCORED\",\n                                                  \"VERTIS CV\"))) +\n  xlab(\"Hazard ratio for MACE (&lt;1 indicates\\nhazard rate reduction vs. placebo)\") +\n  ylab(\"Study\")\n\n\n\n\n\n\n\n\nThe second type of input to meta-analyses that we will use is outcomes for each treatment group. While meta-analyses more commonly use treatment differences as an input, we do not have to use that approach. Instead, we can use outcomes from each treatment group in each study as an input. Often, the two approaches are essentially equivalent, but especially when there is more than two treatment groups per study an approach using treatment differences does need modification in order to correctly capture the correlation of multiple treatment differences compared with the same control group. In contrast, an arm-based approach will automatically reflect this correlation.\nIn the SGLT-2 inhibitor case study, we will assume that event times approximately follow an exponential distribution, which allows us to conduct an arm-based meta-analysis with the number of cases per arm and the total follow-up time to firstevent or censoring per arm as inputs. These are the sufficient statistics for exponentially distributed event times with right-censoring.\n\nsglt2 = \"record, study, trialid, studyno, drugname, treatment, cases, patients, py_to_evt_or_cens, cases_per_py\n1,CANVAS,NCT01032629, 1,canagliflozin, 1, 425, 2888, 15799.25651, 0.0269\n2,CANVAS,NCT01032629, 1,canagliflozin, 0, 233, 1442, 7664.473684, 0.0304\n3,CANVAS-R,NCT01989754, 2,canagliflozin, 1, 160, 2907, 5904.059041, 0.0271\n4,CANVAS-R,NCT01989754, 2,canagliflozin, 0, 193, 2905, 5848.484848, 0.0330\n5,CREDENCE,NCT02065791, 3,canagliflozin, 1, 217, 2202, 5607.235142, 0.0387\n6,CREDENCE,NCT02065791, 3,canagliflozin, 0, 269, 2199, 5523.613963, 0.0487\n7,EMPA-REG OUTCOME,NCT01131676, 4,empagliflozin, 1, 490, 4687, 13102, 0.0374\n8,EMPA-REG OUTCOME,NCT01131676, 4,empagliflozin, 0, 282, 2333, 6424, 0.0439\n9,DECLARE–TIMI 58,NCT01730534, 5,dapagliflozin, 1, 756, 8582, 33451.32743, 0.0226\n10,DECLARE–TIMI 58,NCT01730534, 5,dapagliflozin, 0, 803, 8578, 33181.81818, 0.0242\n11,SCORED,NCT03315143, 6,sotagliflozin, 1, 343, 5292, 7145.833333, 0.048\n12,SCORED,NCT03315143, 6,sotagliflozin, 0, 442, 5292, 7015.873016, 0.063\n13,VERTIS CV,NCT01986881, 7,ertugliflozin, 1, 653, 5493, 16743.58974, 0.039\n14,VERTIS CV,NCT01986881, 7,ertugliflozin, 0, 327, 2745, 8175, 0.04\" %&gt;%\n  read.csv(text=.) %&gt;%\n  tibble() %&gt;% \n  mutate(treatment = factor(treatment, levels=0L:1L, \n                            labels=c(\"Placebo\", \"SGLT2 inhibitor\")))\n\nsglt2 %&gt;%\n  arrange(desc(studyno)) %&gt;%\n  mutate(studyno = factor(studyno, levels=studyno, labels=study)) %&gt;%\n  ggplot(aes(x=cases_per_py, y=studyno, col=treatment, label=treatment)) +\n  geom_vline(xintercept=0, linewidth=0.25, linetype=2) +\n  geom_point(shape=19, size=3, alpha=0.6) +\n  geom_text_repel(data=.%&gt;% filter(studyno==\"EMPA-REG OUTCOME\")) +\n  xlab(\"Exponential hazard rate for MACE\") +\n  ylab(\"Study\")\n\n\n\n\n\n\n\n\nLet us also briefly illustrate why naively pooling data instead of using meta-analytic approaches is inappropriate. Take e.g. the CANVAS and CANVAS-R studies, which both have hazard ratios &lt;1 favoring canagliflozin over placebo. A naive pooling of these two studies followed by calculating the proportion of patients with an event would seem to indicate that placebo performs better! This effect is called Simpson’s paradox.\n\nsglt2 %&gt;% \n  filter(str_detect(study, \"CANVAS\")) %&gt;% \n  rename(Treatment=treatment) %&gt;%\n  group_by(Treatment) %&gt;% \n  summarize(`Proportion of cases` = sum(cases)/sum(patients)) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nTreatment\nProportion of cases\n\n\n\n\nPlacebo\n0.098\n\n\nSGLT2 inhibitor\n0.101",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#model-description",
    "href": "src/02ab_meta_analysis_trtdiff.html#model-description",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "4.3 Model description",
    "text": "4.3 Model description\n\n4.3.1 Fixed- and random-effects meta-analysis\nTaking a Bayesian approach with at least weakly informative priors tends to be very useful to avoid various issues that arise with frequentist methods e.g. when there are very few studies, or no events in one or all arms of a study in a meta-analysis of event occurrence. Commonly, two types of meta-analysis model are distinguished: fixed- and random-effects meta-analysis models. Both can expressed as regression models. Fitting such regression models is straightforward in brms. While this is even arguably useful in simple situations due to the understanding and insights it provides, its true value lies in making it easier to deal with more complex situations. E.g. when we wish to adjust estimates or predictions for some study-level covariates (“meta-regression”), we simply add an additional term to our meta-analytic regression model.\nA “fixed-effects meta analysis” assumes that there is a single fixed parameter that describes the treatment effect in all trials. For example, the very simple inverse-variance approach uses estimates \\(\\hat{\\delta}_i\\) from each trial \\(i=1,\\ldots,I\\) and their standard errors \\(\\text{SE}(\\hat{\\delta}_i)\\) as its inputs, and results in an estimate that is the weighted mean of the estimates from each study with the weights of each study proportional to the inverse of the squared standard errors \\[\\hat{\\delta}_\\text{inv.-var.} = \\frac{1}{{\\sum_{j=1}^I \\frac{1}{\\text{SE}(\\hat{\\delta}_j)^{2}}}} \\times \\sum_{i=1}^I \\frac{\\hat{\\delta}_i}{\\text{SE}(\\hat{\\delta}_i)^2}.\\]\nExpressed as a regression model, the inverse-variance approach corresponds to a regression model for the observed treatment effect estimates that only has an intercept term, i.e.: \\[\\text{E} \\hat{\\delta}_i = \\delta\\] with \\(\\hat{\\delta}_i \\sim N(\\text{E} \\hat{\\delta}_i, \\text{SE}(\\hat{\\delta}_i))\\) (conditioning on the observed value of the standard error). In this model, inference is about \\(\\delta\\). In brms formula syntax, this model can be specified as log_hr | se(log_hr_stderr) ~ 1.\nIn contrast, a “random-effects meta analysis” assumes that there the parameter that describes the treatment effect varies across trials. It is commonly assumed that it varies according to a normal distribution with unknown mean and standard deviation. Such an approach will give greater weight to studies with larger standard errors than the inverse-variance approach does.\nOne common method for random-effects meta-analysis is the DerSimonian and Laird method (DerSimonian 1986). Expressed as a regression model, this method corresponds to a regression model for the observed treatment effect estimates that has an intercept term and a random study effect on the intercept, i.e.: \\[\\text{E} \\hat{\\delta}_i = \\mu + \\nu_i\\] with \\(\\nu_i \\sim N(0, \\tau)\\) for some \\(\\tau&gt;0\\) and \\(\\hat{\\delta}_i \\sim N(\\text{E} \\hat{\\delta}_i, \\text{SE}(\\hat{\\delta}_i))\\) for \\(i=1, \\ldots, I\\) (again, conditioning on the observed value of the standard error). In brmsformula syntax, this model can be expressed as log_hr | se(log_hr_stderr) ~ 1 + (1|study).\nIn this model, inference is usually made for \\(\\mu\\), although arguably the effect \\(\\delta^* \\sim N(\\mu, \\tau)\\) in a new population drawn from the distribution of study populations may also be of interest.\n\n\n4.3.2 With data from each arm as an input\nWhen we have data from each arm as an input, we can use a fixed-effects approach with a regression model for the observed outcomes in each arm that has fixed study and treatment effects: \\[\\text{E} \\hat{\\theta}_j = \\sum_{i=1}^I \\alpha_i \\times 1\\{\\text{study}_j=i\\} + \\delta \\times 1\\{\\text{treatment}_j = 1\\}\\] with \\(\\hat{\\theta}_i \\sim N(\\text{E} \\hat{\\theta}_i, \\text{SE}(\\hat{\\theta}_i))\\). The expected control group outcome \\(\\alpha_i\\) for each study is a nuissance parameter and we aim to make inference about \\(\\delta\\). In brms formula syntax this model can be expressed as theta | se(se_theta) ~ 0 + study + treatment.\nIn a frequentist setting, there is only value in using this input data format, if we have more than two treatments, because different comparisons versus the same control group from the same study are correlated and this approach can automatically reflects this. The generalization of the regression equation above is straightforward.\nWhen we have data from each arm as an input, we can implement a random-effects approach via a regression model for the observed outcomes in each arm that has fixed study effects and random treatment effects: \\[\\text{E} \\hat{\\theta}_j = \\sum_{i=1}^I \\alpha_i \\times 1\\{\\text{study}_j=i\\} + \\left( \\mu + \\sum_{i=1}^I \\nu_i \\times 1\\{\\text{study}_j=i\\} \\right) \\times 1\\{\\text{treatment}_j = 1\\}\\] with \\(\\hat{\\theta}_i \\sim N(\\text{E} \\hat{\\theta}_i, \\text{SE}(\\hat{\\theta}_i))\\). In theory, the brms formula syntax for such a model should be something like theta | se(se_theta) ~ 0 + study + (treatment|study), but this does not seem to do quite what we want, so we implement something else below.\n\n\n4.3.3 What to do when a normal approximation is not appropriate?\nIn the previous subsection, we assumed that it is a good approximation to assume that least-squares-means for each treatment group or treatment differences have a normal sampling distribution. That is often the case, especially after suitable transformations such as the log-transformation for odds ratios, rate ratios, hazard ratios, event rates, odds, and hazard rates. However, making this assumption is not always appropriate. Examples when different approaches are preferable include:\n\nsparse binomial, time-to-event or count outcomes (e.g. arms in some studies have a low number of events or no events - this is one of the most common scenarios),\nthe published information does not lend itself easily towards being represented as an estimate with a standard error (e.g. data consisting of Kaplan-Meier curves for each treatment arm),\ncorrelations in the data need to be reflected appropriately (e.g. meta-analysis of multivariate data such as data from multiple timepoints, or joint meta-analysis of diagnostic sensitivity and specificity), or\nmeta-analysis of individual-patient data accounting for covariates.\n\nIn these situations, there are a number of ideas we can conveniently use in a regression setting.\n\nUsing a likelihood corresponding to the individual patient data likelihood when sufficient statistics are available. Examples include:\n\nWhen number of patients with an event \\(Y_{ij}\\) and the total number of patients \\(N_{ij}\\) are available for each arm \\(j\\) of study \\(i\\), we could assume that \\(Y_{ij} \\sim \\text{Bin}(\\pi_{ij}, N_{ij})\\) and work with a regression equation for \\(\\text{logit}(\\pi_{ij})\\). For this, brms offers the formula = cases | trials(patients) = regression terms syntax in combination with specifying family = binomial(link=\"logit\").\nWhen the number of patients with at least an event \\(Y_{ij}\\) and the total time until first event or censoring \\(t_{ij}\\) are available for exponentially distributed time-to-event data, we can use the the log-likelihood is - up to a constant - the same as when we assume \\(Y_{ij} \\sim \\text{Poisson}(\\lambda_{ij} t_{ij})\\) and work with Poisson regression with a \\(\\log t_{ij}\\) offset. For this, brms offers the formula = cases | rate(followuptime) ~ regression terms syntax in combination with specifying family = poisson(link=\"log\").\nWhen the number of total events experienced by all patients \\(Y_{ij}\\) and the total time at risk of events for all patients \\(t_{ij}\\) are available for Poisson distributed count data, we can use \\(Y_{ij} \\sim \\text{Poisson}(\\lambda_{ij} t_{ij})\\) and work with Poisson regression with a \\(\\log t_{ij}\\) offset. For this, brms offers the formula = cases | rate(followuptime) ~ regression terms syntax in combination with specifying family = poisson(link=\"log\"). However, note that this is rarely appropriate, because in practice count data are nearly always over-dispersed compared to the Poisson distribution. In contrast, assuming an exponential distribution for the time-to-first-event seems to occasionally describe data from real trials reasonably well.\n\nFor individual patient data, we would use a regression model that we would use to analyze each trial, but let all model terms other than the treatment effect differ between trials. Most typically, we would simply assume that there are completely different parameters for each trial. For regression coefficients, this is equivalent to having an interaction between them and the trial effect. For distributional parameters such as the residual standard deviation, we can use the specific syntax offered by brms to specify that they should differ by trial, e.g. for the residual standard deviation in a linear regression we can write formula = bf(outcome ~ regression terms, sigma ~ 0 + trial), family = gaussian() to specify this. For more details see the brms vignette on distributional models.\nUsing custom likelihood functions can also be an option.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#implementation",
    "href": "src/02ab_meta_analysis_trtdiff.html#implementation",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "4.4 Implementation",
    "text": "4.4 Implementation\n\n4.4.1 Meta-analysis with treatment effects as inputs\nWe will first implement a Bayesian version of the “traditional” meta-analysis approach, where we use the estimates of a treatment effect and their standard errors as inputs. The brms code below performs a fixed-effects meta-analysis with log-hazard ratios and their standard errors as inputs.\n\nbrmfit1 = brm(data=sglt2_hr,\n              formula = log_hr | se(log_hr_stderr) ~ 1,\n              prior = prior(class=\"Intercept\", student_t(3, 0, 2.5)),\n              control=list(adapt_delta=0.99),\n              refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nsummary(brmfit1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_hr | se(log_hr_stderr) ~ 1 \n   Data: sglt2_hr (Number of observations: 7) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.13      0.03    -0.18    -0.07 1.00     1152     1068\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis brms code performs a random-effects meta-analysis with log-hazard ratios and their standard errors as inputs.\n\nbrmfit2 = brm(data=sglt2_hr,\n              formula = log_hr | se(log_hr_stderr) ~ 1 + (1|studyno),\n              prior = prior(class=\"Intercept\", student_t(3, 0, 2.5)) +\n                prior(class=\"sd\", normal(0,1)),\n              control=list(adapt_delta=0.99),\n              refresh = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.9 seconds.\n\n\nAs we noted before, there’s at least two ways of performing posterior inference after fitting the model above: 1. inference about the mean effect of the SGLT-2 inhibitors across studies, or 2. inference about the predicted effect in a new study drawn from the same distribution of studies as the studies under analysis. This difference becomes very clear when working in a Bayesian regression setting using brms, but is somewhat obscured when we use standard meta-analysis software.\n\nsummary(brmfit2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_hr | se(log_hr_stderr) ~ 1 + (1 | studyno) \n   Data: sglt2_hr (Number of observations: 7) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~studyno (Number of levels: 7) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.07      0.06     0.00     0.20 1.00      932     1264\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.14      0.05    -0.23    -0.05 1.00     1598     1289\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\npredict(object=brmfit2, \n        newdata=tibble(studyno=\"New study\", log_hr_stderr=10e-19), \n        allow_new_levels=TRUE, sample_new_levels = \"gaussian\")\n\n       Estimate Est.Error       Q2.5      Q97.5\n[1,] -0.1376752 0.1027312 -0.3394645 0.05522499\n\n\n\n\n4.4.2 Meta-analysis with data from each arm as inputs\nThis brms code performs a fixed-effects meta-analysis with arm-based data as inputs. We now need to assume prior distributions for the placebo group log-hazard rate in each trial, for this we use an extremely wide Student-t prior with 3 degrees of freedom centered on \\(-3\\), because when exponentiated \\(e^-3 \\approx 0.05\\) corresponds to a plausible yearly event rate.\n\nbrmfit3 = brm(data=sglt2,\n              formula = cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment,\n              family = poisson(link=\"log\"),\n              prior = prior(class=\"b\", coef=\"treatmentSGLT2inhibitor\",\n                            student_t(3, 0, 2.5)) +\n                prior(class=\"b\",  coef=\"studyCANVAS\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCANVASMR\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCREDENCE\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyDECLARE–TIMI58\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyEMPAMREGOUTCOME\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studySCORED\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyVERTISCV\",  student_t(3, -3, 5)),\n              control=list(adapt_delta=0.99),\n              refresh = 0) \n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\n\nsummary(brmfit3)\n\n Family: poisson \n  Links: mu = log \nFormula: cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment \n   Data: sglt2 (Number of observations: 14) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nstudyCANVAS                -3.49      0.04    -3.57    -3.41 1.00     3031\nstudyCANVASMR              -3.44      0.05    -3.55    -3.34 1.00     3923\nstudyCREDENCE              -3.07      0.05    -3.16    -2.98 1.00     3794\nstudyDECLARE–TIMI58        -3.69      0.03    -3.75    -3.64 1.00     2813\nstudyEMPAMREGOUTCOME       -3.14      0.04    -3.22    -3.07 1.00     2654\nstudySCORED                -2.83      0.04    -2.90    -2.76 1.00     3842\nstudyVERTISCV              -3.15      0.04    -3.22    -3.08 1.00     3269\ntreatmentSGLT2inhibitor    -0.13      0.03    -0.18    -0.08 1.00     1672\n                        Tail_ESS\nstudyCANVAS                 2762\nstudyCANVASMR               2624\nstudyCREDENCE               2879\nstudyDECLARE–TIMI58         2698\nstudyEMPAMREGOUTCOME        2745\nstudySCORED                 2786\nstudyVERTISCV               2880\ntreatmentSGLT2inhibitor     2412\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nbrmfit3 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws()\n\n# A tibble: 1 × 10\n  variable      mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 hazard ratio 0.877  0.876 0.0242 0.0241 0.838 0.918  1.00    1672.    2412.\n\n\nThis brms code performs a random-effects meta-analysis with arm-based data as inputs.\n\nbrmfit4 = brm(data=sglt2 %&gt;% mutate(trtno = 1L*(treatment==\"SGLT2 inhibitor\")),\n              formula = cases | rate(py_to_evt_or_cens) ~ 0 + study + treatment + (0+trtno|study),\n              family = poisson(link=\"log\"),\n              prior = prior(class=\"b\", coef=\"treatmentSGLT2inhibitor\", student_t(3, 0, 2.5)) +\n                prior(class=\"b\",  coef=\"studyCANVAS\",   student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCANVASMR\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyCREDENCE\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyDECLARE–TIMI58\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyEMPAMREGOUTCOME\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studySCORED\",  student_t(3, -3, 5)) +\n                prior(class=\"b\",  coef=\"studyVERTISCV\",  student_t(3, -3, 5)) +\n                prior(class=\"sd\", normal(0,1)),\n              control=list(adapt_delta=0.99),\n              refresh = 0) \n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.2 seconds.\n\n\nAs before, we can easily obtain inference about the mean effect of the SGLT-2 inhibitors across studies or the predicted effect in a new study drawn from the same distribution of studies as the studies under analysis.\n\nbrmfit4 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws() %&gt;%\n  dplyr::select(variable, median, q5, q95) %&gt;%\n  knitr::kable(digits=3)\n\n\n\n\nvariable\nmedian\nq5\nq95\n\n\n\n\nhazard ratio\n0.869\n0.799\n0.932\n\n\n\n\n\n\nbrmfit4 %&gt;%\n  as_draws_df() %&gt;%\n  mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor + \n                                          rnorm(n=length(b_treatmentSGLT2inhibitor), \n                                                mean=0, sd=sd_study__trtno))) %&gt;%\n  dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n  summarise_draws() %&gt;%\n  dplyr::select(variable, median, q5, q95) %&gt;%\n  knitr::kable(digits=3)\n\n\n\n\nvariable\nmedian\nq5\nq95\n\n\n\n\nhazard ratio\n0.87\n0.73\n1.024",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#results",
    "href": "src/02ab_meta_analysis_trtdiff.html#results",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "4.5 Results",
    "text": "4.5 Results\nLet us combine all the results including also a standard frequentist analysis. As we can see, particularly when using log-hazard-ratios as our analysis inputs, the results of the different models are extremely similar. There seems to be substantial evidence that on average there is a benefit on MACE from SGLT-2 inhibitors in type 2 diabetes patients.\nThe uncertainty from random effects models is greater - especially when predicting for a new trial. When using arm-based inputs, the prior we put on the results for each arm appears to influence there results - perhaps we should make those rates exchangeable between trials to make a more data-informed decision on those rates?\n\nlibrary(meta)\n\nfrequentist_ma = metagen(data = sglt2_hr,\n                         sm = \"HR\",\n                         TE=log_hr,\n                         seTE=log_hr_stderr, \n                         studlab=trial)\n\nbind_rows(\n  brmfit1 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Fixed effects - log-hazard ratios\"),\n   brmfit2 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - log-hazard ratios;inference on mean\"),\n  brmfit2 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_Intercept +\n                                            rnorm(n=length(b_Intercept), \n                                                  mean=0, \n                                                  sd=sd_studyno__Intercept))) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - log-hazard ratios;inference for new trial\"),\n  brmfit3 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws()  %&gt;%\n    mutate(Method = \"Fixed effects - rates by arm\"),\n  brmfit4 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor)) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - rates by arm; inference on mean\"),\n  brmfit4 %&gt;%\n    as_draws_df() %&gt;%\n    mutate_variables(`hazard ratio` = exp(b_treatmentSGLT2inhibitor + \n                                            rnorm(n=length(b_treatmentSGLT2inhibitor), \n                                                  mean=0, sd=sd_study__trtno))) %&gt;%\n    dplyr::select(.chain, .iteration, .draw, `hazard ratio`) %&gt;%\n    summarise_draws() %&gt;%\n    mutate(Method = \"Random effects - rates by arm; inference for new trial\"),\n  tibble(Method = c(\"Fixed effects - frequentist\", \"Random effects - frequentist\"),\n         variable=\"hazard ratio\",\n         median=exp(c(frequentist_ma$TE.fixed, frequentist_ma$TE.random)), \n         q5=exp(c(frequentist_ma$lower.fixed, frequentist_ma$lower.random)), \n         q95=exp(c(frequentist_ma$upper.fixed, frequentist_ma$upper.random)))) %&gt;%\n  dplyr::select(Method, variable, median, q5, q95) %&gt;%\n  mutate(method_class = case_when(\n    Method==\"Fixed effects - frequentist\"~1L,\n    Method==\"Fixed effects - log-hazard ratios\" ~ 2L,\n    Method==\"Fixed effects - rates by arm\"~3L,\n    Method==\"Random effects - frequentist\"~4L,\n    Method==\"Random effects - log-hazard ratios;inference on mean\"~5L,\n    Method==\"Random effects - rates by arm; inference on mean\"~6L,\n    Method==\"Random effects - log-hazard ratios;inference for new trial\"~7L,\n    Method==\"Random effects - rates by arm; inference for new trial\"~8L,\n    TRUE~9L)) %&gt;%\n  arrange(desc(method_class)) %&gt;%\n  mutate(method_class = factor(method_class, levels=8L:1L, labels=gsub(\" - \", \"\\n\", Method))) %&gt;%\n  ggplot(aes(x=median, y=method_class, xmin=q5, xmax=q95)) +\n  geom_vline(xintercept=1, linetype=2) +\n  geom_point(shape=19) +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10() +\n  xlab(\"Hazard ratio for MACE (&lt;1 indicates\\nhazard rate reduction vs. placebo)\") +\n  theme_bw(base_size = 14)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02ab_meta_analysis_trtdiff.html#conclusion",
    "href": "src/02ab_meta_analysis_trtdiff.html#conclusion",
    "title": "4  Meta-analysis to estimate treatment effects",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nAs we saw, brms makes it extremely easy to fit various meta-analysis models. Looking at these from a regression perspective makes it easier to see their connections and makes the extension of meta-analysis to meta-regression seem like just one little logical step.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meta-analysis to estimate treatment effects</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html",
    "href": "src/02b_dose_finding.html",
    "title": "5  Dose finding",
    "section": "",
    "text": "5.1 Background",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#what-role-does-dose-response-modeling-play-in-drug-development",
    "href": "src/02b_dose_finding.html#what-role-does-dose-response-modeling-play-in-drug-development",
    "title": "5  Dose finding",
    "section": "5.2 What role does dose response modeling play in drug development?",
    "text": "5.2 What role does dose response modeling play in drug development?\nIn clinical drug development, one key question is what dose of a drug should be used to treat a disease. Ideally, we would want a dose that achieves (nearly) optimal efficacy, while at the same time minimizing side effects to optimize the benefit-risk balance. In many therapeutic areas the relationship between dose and efficacy is characterized in Phase IIb clinical trials that randomize patients to received either one of several doses of a drug or a matching placebo.\nIn such studies, it would be inefficient to try a lot of doses and just look at the performance of each dose in isolation. Instead, we know that biologically there should be some smooth function that describes the relationship between dose and efficacy. We can exploit this knowledge to make the analysis of such studies more efficient. There are a number of methods for analyzing such trials that exploit that the true expected difference to placebo will follow some smooth function of the dose. However, we typically do not know what smooth function best approximates the true underlying dose response function. Obvious candidate functions include monotone functions that eventually plateau, and functions that initially achieve a maximum and then decline.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#data-pathway-asthma-example",
    "href": "src/02b_dose_finding.html#data-pathway-asthma-example",
    "title": "5  Dose finding",
    "section": "5.3 Data: PATHWAY asthma example",
    "text": "5.3 Data: PATHWAY asthma example\nWe will use the PATHWAY trial as an example. This was a placebo-controlled randomized trial of three different tezepelumab dosing regimens compared with placebo in severe asthma. Two of the tezepelumab dosing regimens were given every 4 weeks. We will treat the third regimen of 280 mg every 2 weeks as if it had been 560 mg every 4 weeks. The primary endpoint of PATHWAY was the annualized rate of asthma exacerbations. The plot below shows the published estimates with 95% confidence intervals per dose. Many standard dose response modeling approaches do not require access to individual patient data - which in this case is not publically available -, but can be conducted given estimates and standard error for each dose.\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8979476)\n\n\n# This is the PATHWAY DRF data by group\npathway = tibble(dose = c(0, 70, 210, 280*2),\n                 group = c(\"placebo\", \"tezepelumab 70 mg q4w\",\n                           \"tezepelumab 210 mg q4w\", \"tezepelumab 280 mg q2w\"),\n                 est = log(c(0.67, 0.26, 0.19, 0.22)),\n                 stderr = c(0.10304, 0.17689, 0.22217, 0.19108))\n\n# Simple plot of the data\npathway %&gt;%\n  ggplot(aes(x=dose, y=est, label=str_wrap(group, 12), col=group,\n             ymin=est-stderr*qnorm(0.975), ymax=est+stderr*qnorm(0.975))) +\n  geom_errorbar(width=10) +\n  geom_point() +\n  scale_y_continuous(breaks=log(seq(0.1,1,0.1)), labels=seq(0.1,1,0.1)) +\n  ylab(\"Annualized rate (95% CI)\") +\n  xlab(\"Tezepelumab dose (mg/month)\") +\n  geom_text_repel(nudge_x=c(25, 30, 60, -30), segment.color = NA)\n\n\n\n\n\n\n\n\nOr to show this as rate ratios compared with the placebo group:\n\n# This is the PATHWAY DRF log-rate-ratios vs. placebo\npathway_deltas = tibble(dose= c(0, 70, 210, 280*2),\n                        group = c(\"reference level\",\n                                  \"tezepelumab 70 mg q4w vs. placebo\",\n                                  \"tezepelumab 210 mg q4w vs. placebo\", \n                                  \"tezepelumab 280 mg q2w vs. placebo\"),\n                        logRR= c(0,log(c(1-0.61, 1-0.71, 1-0.66))),\n                        .lower90 = log(c(NA_real_, 1-0.75, 1-0.82, 1-0.79)), # Paper reports 90% CIs\n                        .upper90 = log(c(NA_real_, 1-0.39, 1-0.53, 1-0.47)),\n                        stderr = (.upper90-.lower90)/2/qnorm(0.95),\n                        .lower = logRR-stderr*qnorm(0.975), # Get 95% CIs\n                        .upper = logRR+stderr*qnorm(0.975))\n# Plot log-rate ratios\npathway_deltas %&gt;%\n  ggplot(aes(x=dose, y=logRR, \n             label=str_wrap(group, 12), col=group,\n             ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0) +\n  geom_errorbar(width=10) +\n  #geom_errorbar(width=10, aes(ymin=.lower90, ymax=.upper90)) +\n  geom_point() +\n  scale_y_continuous(breaks=log(seq(0.1,1,0.1)), labels=seq(0.1,1,0.1)) +\n  scale_x_continuous(limits=c(0,600)) +\n  ylab(\"Exacerbation rate ratio (95% CI)\") +\n  xlab(\"Tezepelumab dose (mg/month)\") +\n  geom_text_repel(nudge_x=c(25, 30, 60, -30), nudge_y=c(-0.15, 0, 0, 0), \n                  segment.color = NA)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#model-description",
    "href": "src/02b_dose_finding.html#model-description",
    "title": "5  Dose finding",
    "section": "5.4 Model description",
    "text": "5.4 Model description\nTo conduct dose response analyses under model uncertainty a number of approaches such as the Multiple Comparison Procedures and Modeling (MCP-Mod), the generalized MCP-Mod approach and Gaussian process based approaches are popular. Within the generalized MCP-Mod framework model averaging is an attractive approach for dealing with model uncertainty that has performed well in simulation studies. Here, we take a Bayesian approach to model averaging in the MCP-Mod framework, which Gould (2018) called BMA-Mod (BMA stands for Bayesian model averaging).\nWhile more complex mechanistic pharmacokinetic/pharmacodynamic (PK/PD) models are often important, here, we discuss directly modelling the relationship between dose and some clinical outcome in patients.\nBMA-Mod, just as MCP-Mod, uses a set of candidate models that might describe dose response relationships. In the subsections that follow, we describe each model.\n\n5.4.1 The likelihood function into which we put the dose response model\nNo matter whether we use summary data consiting of estimates with standard errors with each treatment group, or whether we have individual patient data (IPD) for a continuous, binary, count, time-to-event or ordinal outcome, our regression model will be one of the dose response models we will describe in the subsequent sections.\nWhat will differ is the likelihood function, into which we insert the dose response function. In the PATHWAY example, we will use that the estimates for log-mean-event-rate for each dose level \\(i=1,\\ldots,I\\) are approximately indpendent with \\(\\hat{\\theta}_i \\sim N(\\theta_i, \\text{SE}(\\hat{\\theta}_i))\\) distributions. Thus, we can use a Gaussian likelihood with known standard deviation. In case of IPD, we would instead use suitable IPD-likelihoods. This only requires minimal changes in the code for the examples.\n\n\n5.4.2 The sigmoid-Emax and the Emax model\nA very popular dose response model is the sigmoid-Emax model. This model assumes that the outcome for each dose group can be described by a function of the form \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\text{E}_\\text{max} \\times \\frac{\\text{dose}^h}{\\text{dose}^h + \\text{ED}_{50}^h}.\\] This is a monotone curve that starts at no difference to placebo at dose 0 (the expected placebo response is \\(\\text{E}_0\\)) and plateaus at a maximum effect of size \\(\\text{E}_\\text{max} \\in (-\\infty, \\infty)\\). 50% of the maximum effect is reached at the dose \\(\\text{ED}_{50} \\in (0, \\infty)\\). The steepness of the curve is determined by the so-called Hill parameter \\(h \\in (0, \\infty)\\). For \\(h=1\\), we have the simple Emax model, for \\(h&gt;1\\) the dose response curve is steeper around \\(\\text{ED}_{50}\\) (and plateaus faster) than for a simple Emax model, while for \\(h&lt;1\\) the curve is less steep. Typical values of \\(h\\) are smaller than 5 and usually at most 3, so assigning a prior that puts most of the prior weight for \\(\\log h\\) between \\(-\\log 5\\) and \\(\\log 5\\) makes sense.\n\n\n\n\n\n\n\n\n\nThe Emax model is a simple model with some biological plausibility. It is the equation that results from assuming the drug concentrations at the site of drug action are proportional to the administered dose, that drug binds to and unbinds from a receptor at a rate proportional to the drug concentration, and that drugs effects are proportional to receptor occupancy. And, it assumes that all these relationships are exactly the same for all patients. All of these assumptions can be relaxed/avoided when individual patient data (and even better, if also the necessary data for PK/PD modeling) are available.\n\n\n5.4.3 The modified beta model\nAnother possible dose response model is the modified beta model. \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\text{E}_{\\text{max }2} \\times \\frac{(\\delta_1 + \\delta_2)^{\\delta_1 + \\delta_2}}{\\delta_1^{\\delta_1} + \\delta_2^{\\delta_2}} \\times (\\frac{\\text{dose}}{S})^{\\delta_1} \\times (1-\\frac{\\text{dose}}{S})^{\\delta_2}\\] This model is capable of describing a non-monotone dose response.\n\n\n5.4.4 Exponential model, and linear and quadratic functions of dose or log-dose\nThe exponential model \\[f(\\text{dose}; \\text{parameters}) = b_1 + b_2 (\\exp(b_3 \\times \\text{dose})-1)\\] with \\(b_3&lt;0\\) was also discussed by Gould.\nGould also considered linear and quadratic functions of dose or log-dose. I.e. \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta \\times \\text{dose},\\] \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta \\times \\log (\\text{dose}),\\] \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta_1 \\times \\text{dose} + \\beta_2 \\times \\text{dose}^2\\] or \\[f(\\text{dose}; \\text{parameters}) = \\text{E}_0 + \\beta_1 \\times \\log (\\text{dose}) + \\beta_2 \\times (\\log (\\text{dose}))^2.\\] These models are perhaps better suited towards modeling the dose-response relationship within the range of tested dose and would not necessarily be expected to extrapolate well beyond the data range. Nevertheless, they may be useful for providing a more flexible set of models.\n\n\n5.4.5 Bayesian Model averaging\nIf we have multiple models that are similarly plausible given our prior information and the data we observed, then picking a single model out of these ignores the model uncertainty. For this reason, model averaging is known to be a good approach for dealing with model uncertainty. It assigns greater weight to more plausible models and can - given enough evidence - give all weight to a single clearly best model (and also give near-zero weight to clearly inappropriate models). So, - given enough data - it effectively results in model selection, but as long as we only have limited information multiple models will contribute to our inference.\nGould proposes to use a weighted combination of the predictions for each dose level from each of the candidate models and suggests to base the posterior model weigths on predictive likelihood weights.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#implementation-and-results",
    "href": "src/02b_dose_finding.html#implementation-and-results",
    "title": "5  Dose finding",
    "section": "5.5 Implementation and results",
    "text": "5.5 Implementation and results\nWe will now fit the various dose response models introduced in the statistical model section using brms. We will start with the sigmoid-Emax model and look at the things brms let us do easily with a single fitted model. We will not repeat that level of exploration for other models, but you may wish to do so in practice.\n\n5.5.1 Fitting dose response models with brms: sigmoid-Emax model\nWe use the capabilities of brms for fitting non-linear models. For example, a sigmoid-Emax model can be fitted as follows:\n\n# Fitting a sigmoid Emax model\nbrmfit1 &lt;-  brm( bf( est | se(stderr) ~ E0 + Emax * dose^h/(dose^h + ED50^h),\n                    nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)),\n                    E0 ~ 1, logED50 ~ 1, logh ~ 1, Emax ~ 1, \n                    nl=TRUE, family=gaussian()),\n                data=pathway,\n                prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                          prior(normal(0,1), nlpar=\"logh\"),\n                          prior(normal(0,1), nlpar=\"Emax\"),\n                          prior(normal(4,2), nlpar=\"logED50\")),\n                control=list(adapt_delta=0.999),\n                seed=3624)\n\nNote that with individual patient data, we would not write est | se(stderr) ~, but would instead use outcome ~ and bf() + negbinomial() (and similarly, e.g. + gaussian() for continuous data). With the settings above, you might still experience a divergent transition or two and you may wish to increase adapt_delta to 0.9999 or 0.99999, or alternatively try to come up with a way to reparameterize the model.\n\n# Summarize model fit\nsummary(brmfit1)\n\nWarning: There were 1 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * dose^h/(dose^h + ED50^h) \n         h ~ exp(logh)\n         ED50 ~ exp(logED50)\n         E0 ~ 1\n         logED50 ~ 1\n         logh ~ 1\n         Emax ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept         -0.42      0.10    -0.61    -0.21 1.00     2050     2044\nlogED50_Intercept     2.73      1.29    -0.10     4.99 1.00     1251     1246\nlogh_Intercept        0.00      0.95    -1.82     1.85 1.00     1397     1605\nEmax_Intercept       -1.28      0.30    -2.05    -0.83 1.00     1502     1168\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo what does the model fit look like? We have two options for looking at this. The easiest and fastest uses the conditional_effects function from brms.\n\nplot(conditional_effects(brmfit1, \"dose\"), \n     points = TRUE)\n\n\n\n\n\n\n\n\nThe downside is that in a controlled trial we are primarily interested in the contrast with placebo and that we would like to see the confidence intervals around the point estimates for each dose. We can obtain that with a little bit more code:\n\n# We are going to predict the annualized rate for all doses from 0,1,2,...600.\n# We are setting stderr=0 to avoid predicting with sampling variability.\ndr_curve_data = tibble(dose=as.integer(seq(0,600)), stderr=0)\n\n# Now we get the predictions and turn this into a dataset with one row per dose per MCMC sample\ndr_curve_pred &lt;- tidybayes::add_predicted_draws(dr_curve_data, brmfit1) %&gt;%\n  ungroup() %&gt;%\n  transmute(dose, .sample = .draw, pred = .prediction)\n\n# Now we get the rate ratio by subtracting the annualized placebo rate for each MCMC sample  \ndr_curve_pred = dr_curve_pred %&gt;%\n  left_join(dr_curve_pred  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;%\n  dplyr::select(dose, .sample, logRR)\n\n# Now we get median prediction, 50 and 95% credible intervals \n# and plot those with the original data overlaid.\np1 = dr_curve_pred  %&gt;%\n  dplyr::select(-.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"red\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"red\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkred\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"SigMod Emax model fit\"), col=\"darkred\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(p1)\n\n\n\n\n\n\n\n\nWe can also plot the dose response curve for each MCMC sample:\n\n dr_curve_pred  %&gt;%\n  ggplot(aes(x=dose, y=logRR, group=factor(.sample))) +\n  geom_hline(yintercept=0)  +\n  geom_line(col=\"darkred\", alpha=0.1, size=0.25) +\n  geom_point(data=pathway_deltas %&gt;% mutate(.sample=NA_integer_)) +\n  geom_errorbar(data=pathway_deltas %&gt;% mutate(.sample=NA_integer_), \n                aes(ymin=.lower, ymax=.upper), width=10) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\n\n\n\n\n\n\n\nNote, that this also makes it really easy to determine a distribution for the lowest dose that achieves a 50% relative rate reduction:\n\ndr_curve_pred  %&gt;%\n  filter(logRR&lt;log(0.5) | (logRR&gt;=log(0.5) & dose==600)) %&gt;%\n  group_by(.sample) %&gt;%\n  summarize(dose = min(dose), logRR=max(logRR)) %&gt;%\n  mutate(`Has 50% RRR`=(logRR&lt;log(0.5))) %&gt;%\n  ggplot(aes(x=dose, y=\"Dose with\\n&gt;50% RRR\")) +\n  stat_halfeye() +\n  coord_cartesian(xlim=c(0,200)) +\n  xlab(\"Tezepelumab dose [mg/month]\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nAs the plot below shows the data resulted in a substantially different posterior distribution compared with the prior distribution for the logED50 parameter and the Emax parameter. These parameters are reasonably well-identified even with just 3 doses - although the ED50 would be better estimated, if doses with approximately 50% of the Emax (maximum relative rate reduction) had been included in the study. In contrast, the log-Hill-parameter is poorly identified, when there are very few doses (as in this study), which is why the posterior distribution is more or less the prior distribution for the logED50 parameter.\n\nbrmfit1 %&gt;%\n  spread_draws(b_logED50_Intercept, b_logh_Intercept, b_Emax_Intercept) %&gt;%\n  pivot_longer(cols=c(\"b_logED50_Intercept\", \"b_logh_Intercept\", \"b_Emax_Intercept\")) %&gt;%\n  ggplot(aes(x=value)) +\n  theme_bw(base_size=16) +\n  geom_density() +\n  geom_line(data=tibble(name=c(rep(\"b_logED50_Intercept\",151),\n                               rep(\"b_logh_Intercept\",71),\n                               rep(\"b_Emax_Intercept\",61)),\n                        xval = c(seq(-5,10,0.1),\n                                 seq(-3,4,0.1),\n                                 seq(-3,3,0.1)),\n                        density=\n                          case_when(str_detect(name,\"ED50\") ~ dnorm(xval,mean=4,sd=2),\n                                    str_detect(name,\"logh\") ~  dnorm(xval,mean=0,sd=1),\n                                    TRUE ~ dnorm(xval,mean=0,sd=1))),\n            aes(x=xval, y=density), col=\"red\") +\n  geom_text(data=tibble(name=\"b_Emax_Intercept\", density=0.6, xval=1),\n            aes(x=xval, y=density, label=\"prior\"), col=\"red\", size=8) +\n  geom_text(data=tibble(name=\"b_Emax_Intercept\", density=1, xval=0.5),\n            aes(x=xval, y=density, label=\"posterior\"), col=\"black\", size=8) +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n5.5.2 Fitting dose response models with brms: modified beta model\nWe now fit the modified beta model that can capture non-monotone dose response relationships.\n\n# Try a modified beta model\nbrmfit2 &lt;-  brm( bf( est | se(stderr) ~ E0 + Emax*(delta1+delta2)^(delta1+delta2)/(delta1^delta1*delta2^delta2)*(dose/850)^delta1*(1-dose/850)^delta2,\n                   E0 ~ 1, delta1 ~ 1, delta2 ~ 1, Emax ~ 1,\n                   nl=TRUE, \n                   family = gaussian()),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(lognormal(0,1), nlpar=\"delta1\", lb=0),\n                         prior(lognormal(0,1), nlpar=\"delta2\", lb=0)),\n               control=list(adapt_delta=0.999),\n               save_pars = save_pars(all = TRUE),\n               seed = 7304)\n\nLet us summarize the model fit:\n\nsummary(brmfit2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * (delta1 + delta2)^(delta1 + delta2)/(delta1^delta1 * delta2^delta2) * (dose/850)^delta1 * (1 - dose/850)^delta2 \n         E0 ~ 1\n         delta1 ~ 1\n         delta2 ~ 1\n         Emax ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept        -0.43      0.10    -0.63    -0.23 1.00     1535     2122\ndelta1_Intercept     0.45      0.22     0.13     0.98 1.00     1303     1812\ndelta2_Intercept     0.75      0.42     0.16     1.75 1.00     1304     1577\nEmax_Intercept      -1.30      0.21    -1.73    -0.90 1.00     1640     1769\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo what does the model fit look like? We could, again, just use plot(conditional_effects(brmfit2, \"dose\"), points = TRUE) or get a slightly more tailored plot.\n\n# Now we get the predictions and turn this into a dataset with one row per dose per MCMC sample\ndr_curve_pred2 &lt;- tidybayes::add_predicted_draws(dr_curve_data, brmfit2) %&gt;%\n  ungroup() %&gt;%\n  transmute(dose, .sample = .draw, pred = .prediction)\n\n# Now we get the rate ratio by subtracting the annualized placebo rate for each MCMC sample  \ndr_curve_pred2 = dr_curve_pred2 %&gt;%\n  left_join(dr_curve_pred2  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;%\n  dplyr::select(dose, .sample, logRR)\n\n# Now we get median prediction, 50 and 95% credible intervals \n# and plot those with the original data overlaid.\np2 = dr_curve_pred2  %&gt;%\n  dplyr::select(-.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"royalblue\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"royalblue\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkblue\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"modified-beta model fit\"), col=\"darkblue\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(p2)\n\n\n\n\n\n\n\n\n\n\n5.5.3 Perform Bayesian model averaging\nNow we have two fitted model. We can see both of them together below:\n\np1 + p2\n\n\n\n\n\n\n\n\nIn practice, we do not really know, whether one of these is the true data generating model. In fact, realistically both models are going to be at least somewhat misspecified, because we made no attempt to fully model the full complexity of the underlying biological system. So, really, the question is which of the two models provides the better approximation. At a glance both seem to fit the data somewhat decently, so it seems unlikely that we can completely rule one of the two models out. That is where Bayesian model averaging comes in.\nFirst, we need to compare how well each model fits. As it turns out, when you just have 4 data points (due to us using summary data), it is problematic to just use the “standard” brms::loo function to perform approximate leave-one-out cross-validation (LOO-CV) based on the posterior likelihood as implemented in the loo package, as can be seen by running\n\nloo(brmfit1)\n\nWarning: Found 1 observations with a pareto_k &gt; 0.7 in model 'brmfit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo      1.0 0.3\np_loo         1.3 0.5\nlooic        -2.1 0.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 0.9]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3     75.0%   852     \n   (0.7, 1]   (bad)      1     25.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 0      0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nand\n\nloo(brmfit2)\n\nWarning: Found 4 observations with a pareto_k &gt; 0.7 in model 'brmfit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo     -0.4 0.4\np_loo         2.7 0.7\nlooic         0.7 0.8\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 0.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     0      0.0%   &lt;NA&gt;    \n   (0.7, 1]   (bad)      2     50.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 2     50.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nInstead, we actually fit each model leaving each dose group out. We could probably do something more sophisticated by simulating consistent individual patient data (and then performing approximate LOO-CV) or parametric sampling from the normal distribution around the point estimates.\n\n(loo_exact_brmfit1 &lt;- kfold(brmfit1, folds = \"loo\"))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 3.2 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.7 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.7 seconds.\n\n\n\nBased on 4-fold cross-validation.\n\n           Estimate  SE\nelpd_kfold     -0.5 1.3\np_kfold         2.8 1.9\nkfoldic         1.0 2.7\n\n(loo_exact_brmfit2 &lt;- kfold(brmfit2, folds = \"loo\"))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.2 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.5 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.6 seconds.\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 2.0 seconds.\n\n\n\nBased on 4-fold cross-validation.\n\n           Estimate  SE\nelpd_kfold     -2.1 1.7\np_kfold         4.4 2.2\nkfoldic         4.2 3.4\n\n\nNow, we can compare the models via\n\nloo_compare(loo_exact_brmfit1, loo_exact_brmfit2)\n\n        elpd_diff se_diff\nbrmfit1  0.0       0.0   \nbrmfit2 -1.6       0.8   \n\n\n\n5.5.3.1 Approach 1 (manual without brms)\nLet us say that we a-priori assign a 75% probability to the SigEmax model and 25% to the modified beta model.\n\n# follows: \n# Gould, A. L. (2019). BMA‐Mod: A Bayesian model averaging strategy for determining dose‐response relationships in the presence of model uncertainty. Biometrical Journal, 61(5), 1141-1159.\n# https://dx.doi.org/10.1002/bimj.201700211\nprior_weights = c(0.75,0.25)\nposterior_weigths = c(prior_weights[1]*exp(loo_exact_brmfit1$estimates[\"elpd_kfold\", \"Estimate\"]), \n                      prior_weights[2]*exp(loo_exact_brmfit2$estimates[\"elpd_kfold\", \"Estimate\"]))\n(posterior_weigths = posterior_weigths/sum(posterior_weigths))\n\n[1] 0.93629621 0.06370379\n\n\nAs we can see, using leaving-one-dose-group out cross-validation, suggests that the SigEmax model should be given most of the weight. A-posteriori, we assign 93.6% weight to the SigEmax model and 6.4% weight to the modified beta model. However, if we had done approximate LOO-CV with individual patient data, the modified beta model might have been given more weight.\nWhen we average the predictions of the two models with these weights, we get the dose response curve below.\n\nbma_curve = dr_curve_pred %&gt;%\n  inner_join(dr_curve_pred2, by=c(\".sample\", \"dose\")) %&gt;%\n  mutate(logRR = posterior_weigths[1]*logRR.x + posterior_weigths[2]*logRR.y) %&gt;%\n  dplyr::select(-logRR.x, -logRR.y, -.sample) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95))\n\n\npbma = bma_curve %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"orange\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"orange\", alpha=0.5) +\n  geom_line(col=\"darkorange\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"Bayesian model averaging\"), col=\"darkorange\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose\")\n\nplot( ( (p1 + theme(axis.title.x=element_blank()) + ggtitle(\"Sigmoid Emax model\")) | \n          (p2 + theme(axis.title.y=element_blank(),\n                      axis.title.x=element_blank()) + \n             ggtitle(\"Modified beta model\"))) / \n        (pbma + ggtitle(\"Bayesian model averaging\") | plot_spacer()) )\n\n\n\n\n\n\n\n\n\n\n5.5.3.2 Approach 2: Using brms functions\nBefore, we needed to do a lot of manual work to get model-averaged predictions, but the below using brms::pp_average would be a lot simpler, if we could rely on the approximate LOO-CV.\n\n5.5.3.2.1 Approach 2a: Not appropriate here\n\nbma_brms_curve = dr_curve_data %&gt;%\n  dplyr::select(-stderr) %&gt;%\n  bind_cols(as_tibble(\n    t(pp_average(brmfit1, brmfit2, newdata=dr_curve_data, \n                 weights=\"loo\", # Not a good option in this case.\n                 summary=F)),\n    .name_repair=vctrs::vec_as_names(repair=\"unique\", quiet =T))) %&gt;%\n  pivot_longer(cols=starts_with(\"V\"), names_to=\".sample\", values_to=\"pred\") %&gt;%\n  mutate(.sample = as.integer(str_extract(.sample, \"[0-9]+\")))\n\n# Forming differences to placebo by merging with the placebo (dose=0) predictions for each sample\nbma_brms_curve = bma_brms_curve %&gt;%\n  left_join(bma_brms_curve  %&gt;% \n              filter(dose==0) %&gt;% \n              dplyr::select(-dose) %&gt;% \n              rename(placebo=pred), \n            by=\".sample\") %&gt;%\n  mutate(logRR = pred - placebo) %&gt;% # Calculate log rate ratio (difference on log scale vs. placebo)\n  dplyr::select(dose, .sample, logRR)\n\n\n\n5.5.3.2.2 Approach 2b: Using leave-one-dose-out CV\nBecause we want to avoid using approximate LOO-CV in this case, we first replace the internally calculated approximate LOO-CV results with our own\n\nbrmfit1$criteria$loo &lt;- loo_exact_brmfit1\nbrmfit2$criteria$loo &lt;- loo_exact_brmfit2\n(w_dose &lt;- model_weights(brmfit1, brmfit2, weights = \"loo\"))\n\n brmfit1  brmfit2 \n0.830486 0.169514 \n\n\nWe can then do the following:\n\nbmapreds1 &lt;- posterior_epred(brmfit1, newdata = dr_curve_data)\nbmapreds2 &lt;- posterior_epred(brmfit2, newdata = dr_curve_data)\npe_avg &lt;- bmapreds1 * w_dose[1] + bmapreds2 * w_dose[2]\nrownames(pe_avg) &lt;- 1:nrow(pe_avg)\n\n# log-rate predictions for each dose\npe_avg = pe_avg %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;%\n  bind_cols(dr_curve_data %&gt;% dplyr::select(dose)) %&gt;%\n  pivot_longer(cols=-dose, names_to = \".sample\", values_to=\"pred\") \n\n# form difference to placebo (log-rate-ratio) and plot\n pbma2 = pe_avg %&gt;%\n  left_join(y=pe_avg %&gt;% filter(dose==0) %&gt;% dplyr::select(-dose), \n            by=\".sample\") %&gt;%\n  mutate(logRR=pred.x-pred.y) %&gt;%\n  dplyr::select(-.sample, -pred.x, -pred.y) %&gt;%\n  group_by(dose) %&gt;%\n  median_qi(.width = c(0.5, 0.95)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x=dose, y=logRR, ymin=.lower, ymax=.upper)) +\n  geom_hline(yintercept=0)  +\n  geom_ribbon(data=.%&gt;% filter(.width==0.95), fill=\"green\", alpha=0.5) +\n  geom_ribbon(data=.%&gt;% filter(.width==0.5), fill=\"darkgreen\", alpha=0.5) +\n  geom_line(data=.%&gt;% filter(.width==0.5), col=\"darkgreen\") +\n  geom_point(data=pathway_deltas) +\n  geom_errorbar(data=pathway_deltas, width=10) +\n  geom_text(data=tibble(dose=300, logRR=log(0.6), .lower=NA_real_, .upper=NA_real_),\n            aes(label=\"BMA (LOO weights)\"), col=\"darkgreen\", size=4) +\n  scale_y_continuous(breaks=log(c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \n                     labels=paste(100*(1-c(0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1)), \"%\")) +\n  ylab(\"Relative rate reduction\") +\n  xlab(\"Total tezepelumab dose [mg/month]\")\n\nplot(pbma2)\n\n\n\n\n\n\n\n\n\n\n\n5.5.3.3 Why did we do this form of Bayesian model averaging?\nWhy are we doing it like this? Can we not just fit a single model like this (as an example where we average a sigmoid-Emax and a modified beta model)? \\[\n\\begin{aligned}\nf(\\text{dose}; \\text{parameters}) = & \\text{E}_0 + w \\times \\text{E}_{\\text{max }1} \\times \\frac{\\text{dose}^h}{\\text{dose}^h + \\text{ED}_{50}^h} \\\\\n  & + (1-w) \\times \\text{E}_{\\text{max }2} \\times \\frac{(\\delta_1 + \\delta_2)^{\\delta_1 + \\delta_2}}{\\delta_1^{\\delta_1} + \\delta_2^{\\delta_2}} \\times (\\frac{\\text{dose}}{S})^{\\delta_1} \\times (1-\\frac{\\text{dose}}{S})^{\\delta_2}\n\\end{aligned}\n\\] brms certainly lets us specify such a model (see below). But, part of the reason why we do not use it, is that the model parameters are really badly identified: you can still obtain a similarly good fit at each observed dose level by making one of the two models fit better for one dose (and worse for another dose), if the other model in turn is made to fit worse for that dose (and better for the other dose).\nIf you run the code below, you will see how much the sampler struggles to fit this model, if we try to fit it. That is the case, even if we make things easier and fix the model weights to 50:50.\n\n# The model below does not work so well\nbrmfit3 &lt;-  brm( bf( est | se(stderr) ~ E0 + 0.5 * Emax * dose^h/(dose^h + ED50^h) + 0.5*Emax2*(delta1+delta2)^(delta1+delta2)/(delta1^delta1*delta2^delta2)*(dose/850)^delta1*(1-dose/850)^delta2,\n                   E0 ~ 1, ED50 ~ 1, h ~ 1, Emax ~ 1,\n                   #modelwgt ~ 1,\n                   delta1 ~ 1, delta2 ~ 1, Emax2 ~ 1,\n                   nl=T),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(lognormal(0,1), nlpar=\"h\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(lognormal(4,2), nlpar=\"ED50\"),\n                         #prior(beta(4,1), nlpar=\"modelwgt\", lb=0, ub=1),\n                         prior(normal(0,1), nlpar=\"Emax2\"),\n                         prior(lognormal(0,1), nlpar=\"delta1\", lb=0),\n                         prior(lognormal(0,1), nlpar=\"delta2\", lb=0)),\n               control=list(adapt_delta=0.999))\n\nIf you run this code, it produces a lot of severe warnings (which you should not ignore) about divergent transitions after warmup and a very high largest R-hat. Additionally, “Bulk Effective Samples Size” and “Tail Effective Samples Size (ESS)” are too low, and a lot of transitions after warmup that exceeded the maximum treedepth.\n\n\n\n5.5.4 Going beyond the default MCP-Mod models\nSo far, a lot of what we did could also be done in a frequentist manner using the MCP-Mod approach. However, with brms it is easy to make our models more complex when necessary. For example, we assumed that a 280 mg q2w dose is equivalent to a 560 mg q4w dose. This is of course only the case in terms of the total amount of drug injected over the trial period. However, the two dose regimens differ in terms of their blood concentrations over time as shown in the plot below.\n\nsim_1st_order_pk = function(injected=c(1,rep(0,28*5)), dose=1, thalf=28*24, V=10){\n  k = log(2)/thalf  \n  concentration = rep(0, length(injected))\n  for (i in 1:length(injected)){\n    if (i&gt;1) {\n      concentration[i] = concentration[i-1] - k*concentration[i-1] + dose*injected[i]/V\n    } else {\n      concentration[i] = dose*injected[i]/V\n    }\n  }\n  return(concentration)\n}\n\nplot_data = tibble(regimen=\"q4wk\",\n                   dose=280,\n                   concentration = sim_1st_order_pk(rep(c(1,rep(0, 28*24-1)),12), dose=280)) %&gt;%\n  mutate(hour=1:n()) %&gt;%\n  bind_rows(tibble(regimen=\"q2wk\",\n       dose=280,\n       concentration = sim_1st_order_pk(rep(c(1,rep(0, 14*24-1)),24), dose=280)) %&gt;%\n         mutate(hour=1:n())) %&gt;%\n  bind_rows(bind_rows(tibble(regimen=\"q4wk\",\n       dose=560,\n       concentration = sim_1st_order_pk(rep(c(1,rep(0, 28*24-1)),12), dose=560)) %&gt;%\n         mutate(hour=1:n()))) %&gt;%\n  mutate(dose_regimen = paste0(dose, \" mg \", regimen)) \n\nplot_data %&gt;%\n  ggplot(aes(x=hour, y=concentration, col=dose_regimen)) +\n  geom_line() +\n  coord_cartesian(ylim=c(0,120)) +\n  scale_x_continuous(breaks=seq(0,12)*28*24, labels=seq(0,12)) +\n  geom_text(data=. %&gt;% filter( (hour==8025 & str_detect(dose_regimen, \"280 mg q2\")) |\n                               (hour==500 & str_detect(dose_regimen, \"560 mg q4\")) |\n                               (hour==5000 & str_detect(dose_regimen, \"280 mg q4\"))),\n            aes(label=dose_regimen), nudge_y=c(-20,25, 60), angle=c(0,-75,0)) +\n  xlab(\"Time in trial (months)\") +\n  ylab(\"Drug concentration [mg/L]\")\n\n\n\n\n\n\n\n\nThese differences in pharmacokinetic profiles likely lead to at least somewhat different efficacy outcomes. For example, if efficacy is more driven by the peak concentration achieved in each dosing interval, 280 mg q2w might be less effective than a 560 mg q4w regiment would have been. On the other hand, if efficacy is more determined by the minimum concentration maintained throughout time, then 280 mg q2w could be more effective than 560 mg q4w. If the main determinant of efficacy is the average drug concentration over time, and peaks and troughs do not matter (at least within the concentrations seen), then the two dosing regimens might be exactly identical.\nSo, we could update our previous sigmoid Emax model and add an extra term that describes how the 280 mg q2w dose might relate to q4w doses. Note that assuming a monotone concentration response, a 280 mg q2w regimen cannot be worse than a 280 mg q4w regimen - this is also illustrated by the plot showing that the concentrations of 280 mg q2w are (unsurprisingly) always above those of 280 mg q4w. This gives us a lower bound for the efficacy of 280 mg q2w. Similarly, its peak concentrations stay below 2.5 times the peak concentrations of 280 mg q2w, which gives us an upper bound. We will specify a log-normal prior with mean 0 and SD 0.67 for a regimen factor that indicates what q4w dose a q2w dose is equivalent to.\n\nbrmfit3 &lt;- brm( bf( est | se(stderr) ~ E0 + Emax * (dose*dosefactor)^h / ((dose*dosefactor)^h + ED50^h),\n                   nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)), nlf(dosefactor ~ exp((dose==560)*regimen)),\n                   E0 ~ 1, logED50 ~ 1, logh ~ 1, Emax ~ 1, regimen ~ 1, \n                   nl=T, family=gaussian()),\n               data=pathway,\n               prior = c(prior(normal(0,1), nlpar=\"E0\"),\n                         prior(normal(0,1), nlpar=\"logh\"),\n                         prior(normal(0,1), nlpar=\"Emax\"),\n                         prior(normal(4,2), nlpar=\"logED50\"),\n                         prior(normal(0,0.67), nlpar=\"regimen\")),\n               control=list(adapt_delta=0.999),\n               seed=2023,\n               save_pars = save_pars(all = TRUE))\n\nLet’s summarize the model results:\n\nsummary(brmfit3)\n\nWarning: There were 3 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: est | se(stderr) ~ E0 + Emax * (dose * dosefactor)^h/((dose * dosefactor)^h + ED50^h) \n         h ~ exp(logh)\n         ED50 ~ exp(logED50)\n         dosefactor ~ exp((dose == 560) * regimen)\n         E0 ~ 1\n         logED50 ~ 1\n         logh ~ 1\n         Emax ~ 1\n         regimen ~ 1\n   Data: pathway (Number of observations: 4) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nE0_Intercept         -0.42      0.10    -0.61    -0.22 1.00     2624     2529\nlogED50_Intercept     2.76      1.34    -0.33     5.09 1.00     1552     1553\nlogh_Intercept        0.00      0.98    -1.81     1.98 1.00     1388     1769\nEmax_Intercept       -1.30      0.33    -2.11    -0.84 1.00     1429     1348\nregimen_Intercept    -0.05      0.68    -1.36     1.25 1.00     2553     2452\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we can see, we can fit such a model, but the marginal posterior for the log-regimen factor is still a N(0, 0.67) distribution. Thus, the data do not inform this parameter very well - which should not surprise us. However, if we had information external to the PATHWAY trial, such as the elicited judgments of experts, we could incorporate it via this type of model.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#conclusion",
    "href": "src/02b_dose_finding.html#conclusion",
    "title": "5  Dose finding",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nAs we could see brms makes it surprisingly easy to fit non-linear dose response models and to perform model averaging. Taking a Bayesian approach to dose response modeling is attractive for a number of reasons: * using prior information e.g. on expected placebo outcomes, the maximum plausible treatment effect and typical dose response patterns, * (weakly-)informative priors avoid issues with maximum likelihood estimation such as infinitely steep sigmoid Emax models (common when the lowest tested dose has the best point estimate), * the convenience of obtaining predictions about things that are transformations of the model parameters (e.g. the dose with at least a certain effect), and * the straightforward way, in which we can extend the basic models to account for the specifics of our dose finding study.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02b_dose_finding.html#excercises",
    "href": "src/02b_dose_finding.html#excercises",
    "title": "5  Dose finding",
    "section": "5.7 Excercises",
    "text": "5.7 Excercises\nThe data below are from a non-clinical study published by Goodner and Horsfall in 1935 (Goodner, K. and Horsfall Jr, F.L., 1935. The protective action of type I antipneumococcus serum in mice: I. The quantitative aspects of the mouse protection test. The Journal of Experimental Medicine, 62(3), pp.359-374.). Mice were given injections of different amounts from a pneumococcus culture, which at the doses given would in the absence of treatment be invariably fatal, mixed together with different doses of type I antipenumococcus horse and rabbit sera. Over several days it was recorded whether each mouse survived or died. This is an example of a dose response curve that appears to have been accepted to be truly non-monotonic in the scientific literature.\n\nmice = tibble(\n  `Source of immune serum` = factor( c(rep(1L, 29*3), rep(2L, 32*3)),\n                                     levels=c(1L, 2L), \n                                     labels=c(\"rabbit\", \"horse\")),\n  `Amount of serum (cc.)` = rep(0.4*0.5^c(0L:5L, 0L:6L, 0L:7L, 0L:7L,\n                                          1L:5L, 0L:8L, 0L:8L, 0L:8L),\n                                each=3),\n  `Amount of culture (cc.)` = c(rep(0.4, 18), rep(0.2, 21), rep(0.1, 24), \n                                rep(0.05, 24), rep(0.4, 15), rep(0.2, 27), \n                                rep(0.1, 27), rep(0.05,27)),\n  Died = c(rep(1L, 18),\n            1L,rep(0L,5), 1L,1L,0L, 1L,0L,0L, 1L,1L,1L, 1L,0L,0L, 1L,1L,1L,\n            rep(0L, 14),1L, 0L,0L,0L, 1L,1L,0L, 1L,1L,1L,\n            rep(0L,24), \n            rep(1L, 15),\n            rep(1L, 8),0L, 1L,0L,0L, 1L,0L,0L, rep(1L,5),0L, 0L,rep(1L,5),\n            rep(1L,5), 0L,1L,rep(0L,9),rep(1L,4),0L, rep(1L,6),\n            rep(1L,5), rep(0L,12),1L, 0L,0L, rep(1L,7))) %&gt;%\n  mutate(Survived = 1L-Died)\n\nDisplayed in a fashion similar to the original article, the data look as in the figure below.\n\n\n\n\n\n\n\n\n\nAn alternative display showing the proportion of mice that survived versus the amount of serum given as displayed in the figure below.\n\n\n\n\n\n\n\n\n\nExcercises in increasing order of complexity:\n\nFit a sigmoid Emax model to the horse serum data of each individual mouse assuming a Bernoulli distribution after choosing reasonable weakly informative priors, but remember that 0 cc. of serum was stated to result in a death rate of near 100%. Perform model checking.\nFit a modified beta model to the horse serum data and also perform Bayesian model averaging. Does this result in a better fit?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dose finding</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html",
    "href": "src/02c_dose_escalation.html",
    "title": "6  Oncology dose escalation",
    "section": "",
    "text": "6.1 Background\nIn Oncology, a principal aim of a Phase-I trial is to study the relationship between dose and toxicity. Larger doses of the study drug are understood to increase the likelihood of severe toxic reactions known as dose-limiting toxicities (DLTs). The key estimand in such a trial is the maximum tolerated dose (MTD) or recommended dose (RD).\nThe class of trial designs that are typically employed are dose escalation designs. In order to protect patient safety, small cohorts of patients are enrolled sequentially, beginning with low dose levels of the study treatment, and monitored for DLTs. Once a dose level is found to be safe, the dose level may be escalated, and the subsequent cohort enrolled at a higher dose.\nWhereas traditional designs such as the 3+3 design are based on algorithmic rules governing the decision for the subsequent cohort based on the outcome at the current dose, Bayesian model-based designs have proven to provide greater flexibility and improved performance for estimating the MTD, while protecting patient safety.\nIn the model-based paradigm for dose escalation, one develops a model for the dose-toxicity relationship. As DLT data accumulates on-study, the model is used to adaptively guide dose escalation decisions. Bayesian approaches are well-suited for this setting, due in part to the limited amount of available data (Phase-I studies are generally small).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#data",
    "href": "src/02c_dose_escalation.html#data",
    "title": "6  Oncology dose escalation",
    "section": "6.2 Data",
    "text": "6.2 Data\nIn the simplest setting of a first-in-human (FIH) study of a new Oncology treatment, the relevant data are the doses, and associated counts of the number of patients evaluated and the number of patients with DLTs.\nFor dose levels indexed by \\(i=1,\\ldots, K\\), we observe:\n\\[ d_i = \\text{dose for the }i^\\text{th}\\text{ dose level} \\] \\[ n_i = \\text{number of evaluable patients treated at }d_i \\] \\[ Y_i = \\text{number of patients with a DLT at }d_i. \\]\nAs an example:\n\ndlt_data &lt;- dplyr::transmute(OncoBayes2::hist_SA, dose = drug_A, num_patients, num_toxicities)\nknitr::kable(dlt_data)\n\n\n\n\ndose\nnum_patients\nnum_toxicities\n\n\n\n\n1.0\n3\n0\n\n\n2.5\n4\n0\n\n\n5.0\n5\n0\n\n\n10.0\n4\n0\n\n\n25.0\n2\n2",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#model-description",
    "href": "src/02c_dose_escalation.html#model-description",
    "title": "6  Oncology dose escalation",
    "section": "6.3 Model description",
    "text": "6.3 Model description\nNeuenschwander, Branson, and Gsponer (2008) proposed the following simple Bayesian logistic regression model (BLRM).\n\\[ Y_i \\, | \\, \\pi(d_i) \\, \\sim \\, \\mathrm{Binom}(n_i, \\pi(d_i))\\, \\, \\text{for }i=1,\\ldots,K \\] \\[ \\pi(d_i) \\, | \\, \\alpha, \\beta = \\mathrm{logit}^{-1}\\Bigl( \\log\\alpha + \\beta\\log \\Bigl(\\frac{d_i}{d^*}\\Bigr) \\Bigr) \\] \\[ \\log\\alpha \\, \\sim \\, N(\\mathrm{logit}(p_0), s_\\alpha^2) \\] \\[ \\log\\beta \\, \\sim \\, N(m_\\beta, s_\\beta^2). \\]\nNote:\n\nA key modeling consideration is monotonicity of the dose-toxicity curve. The use of a lognormal prior for \\(\\beta\\) ensures the curve is strictly increasing \\((\\beta &gt; 0)\\).\nIn the second line, \\(d^*\\) is known as a reference dose, used to scale the doses \\(d_i\\).\nHyperparameters \\(p_0\\), \\(s_\\alpha\\), \\(m_\\beta\\), and \\(s_\\beta\\) may be chosen based on clinical understanding at trial outset. Typically, little information is available prior to the FIH study, so weakly informative priors are preferred.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#implementation",
    "href": "src/02c_dose_escalation.html#implementation",
    "title": "6  Oncology dose escalation",
    "section": "6.4 Implementation",
    "text": "6.4 Implementation\n\nlibrary(dplyr)\nlibrary(brms)\nlibrary(OncoBayes2)\nlibrary(bayesplot)\nlibrary(posterior)\nlibrary(knitr)\nlibrary(BOIN)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8794567)\n\nSuch a model is straightforward to implement in brms. Below, we use a nonlinear formula specification, in order to allow the prior for the intercept to be specified on the log scale.\n\n# Reference dose\ndref &lt;- 50\n\n# Standardize the covariate as in the model description\ndlt_data &lt;- mutate(dlt_data, std_dose = log(dose / dref))\n\n# Model formula in brms\nblrm_model &lt;- bf(num_toxicities | trials(num_patients) ~ logalpha + exp(logbeta) * std_dose, nl = TRUE) +\n  lf(logalpha ~ 1) +\n  lf(logbeta ~ 1)\n\n# Get the list of model parameters requiring prior specifications\nget_prior(blrm_model, data = dlt_data, family = \"binomial\")\n\n  prior class      coef group resp dpar    nlpar lb ub       source\n (flat)     b                           logalpha            default\n (flat)     b Intercept                 logalpha       (vectorized)\n (flat)     b                            logbeta            default\n (flat)     b Intercept                  logbeta       (vectorized)\n\n# Set the prior as described in the previous section\nblrm_prior &lt;- prior(normal(logit(0.33), 2), nlpar = \"logalpha\") +\n  prior(normal(0, 0.7), nlpar = \"logbeta\")\n\n# Compile and fit the model using brms\nblrm_fit &lt;- brm(blrm_model,\n                data = dlt_data,\n                family = \"binomial\",\n                prior = blrm_prior,\n                refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: binomial_logit_lpmf: Probability parameter[1] is -inf, but must be finite! (in '/tmp/RtmpDJVvwQ/model-27ba3ffbe19d.stan', line 42, column 4 to column 50)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#results",
    "href": "src/02c_dose_escalation.html#results",
    "title": "6  Oncology dose escalation",
    "section": "6.5 Results",
    "text": "6.5 Results\n\n6.5.1 Inference for model parameters\nIt is simple to get summary statistics and graphical displays of the posterior distributions for the model parameters.\n\n# Summary statistics for log(alpha) and log(beta)\nposterior_summary(blrm_fit)\n\n                       Estimate Est.Error       Q2.5     Q97.5\nb_logalpha_Intercept  0.6916997 1.3269729 -1.7873140  3.451116\nb_logbeta_Intercept   0.4820649 0.5326815 -0.7049998  1.367179\nlprior               -3.1659583 1.0529848 -6.0760212 -2.194609\nlp__                 -6.4175598 1.0432911 -9.3018430 -5.392544\n\n\n\n# Posterior density estimates for log(alpha) and log(beta)\nbrms::mcmc_plot(blrm_fit, type = \"dens\", facet_args = list(ncol = 1))\n\n\n\n\n\n\n\n\nHowever, these parameters are not themselves the primary target of inference for decision making. Rather the dose-DLT curve \\(\\pi(d)\\) itself is estimated, in order to guide decision making about future dose levels.\n\n\n6.5.2 Inference for DLT rates\nEstimation of \\(\\pi(d)\\) is similarly straightforward. For posterior summary statistics of \\(\\pi(d_i)\\) at the observed dose levels \\(i=1,\\ldots,K\\), one can use the poserior::summarize_draws function as follows:\n\n# Posterior draws for pi(d)\ndlt_data %&gt;%\n  bind_cols(\n    posterior::summarize_draws(\n      posterior_linpred(blrm_fit, transform = TRUE),\n      c(\"mean\", \"sd\", \"quantile2\")\n    ) %&gt;% select(-variable)\n  ) %&gt;%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndose\nnum_patients\nnum_toxicities\nstd_dose\nmean\nsd\nq5\nq95\n\n\n\n\n1.0\n3\n0\n-3.912\n0.011\n0.022\n0.000\n0.048\n\n\n2.5\n4\n0\n-2.996\n0.025\n0.036\n0.000\n0.095\n\n\n5.0\n5\n0\n-2.303\n0.054\n0.057\n0.002\n0.169\n\n\n10.0\n4\n0\n-1.609\n0.127\n0.097\n0.015\n0.322\n\n\n25.0\n2\n2\n-0.693\n0.381\n0.194\n0.101\n0.730\n\n\n\n\n\nNext we illustrate estimation of \\(\\pi(d)\\) on a fine grid of dose levels, in order to visualize the posterior for the continuous curve.\n\ndose_grid &lt;- seq(0, dref, length.out = 500)\n\n# Posterior draws for pi(d)\ndlt_rate_draws &lt;- posterior_linpred(blrm_fit,\n                                    newdata = tibble(dose = dose_grid,\n                                                     std_dose = log(dose / dref),\n                                                     num_patients = 1),\n                                    transform = TRUE)\n\n# Visualization using bayesplot\nbayesplot::ppc_ribbon(y = rep(0, 500), yrep = dlt_rate_draws, x = dose_grid) +\n  guides(fill = \"none\", color = \"none\") +\n  labs(x = \"Dose\", y = \"P(DLT)\")\n\n\n\n\n\n\n\n\n\n\n6.5.3 Predictive inference for future cohorts\nAnother key quantity for understanding risk of toxicity for patients is the predictive distribution for DLTs in a future, unobserved cohort:\n\\[ \\Pr(\\tilde Y = y \\, | \\, y_{obs}) = \\int \\Pr(\\tilde Y = y \\, | \\, \\theta) \\, d\\pi(\\theta|y_{obs}). \\]\nOne can easily obtain such information from brms. Suppose we wished to know the predictive distribution for the DLT count in new cohorts of size 4 at a set of candidate dose levels, \\(d=10, 25, 50\\).\n\ncandidate_doses &lt;- tibble(\n  dose = c(10, 25, 50),\n  std_dose = log(dose / dref),\n  num_patients = 4\n)\n\npredictive_draws &lt;- posterior_predict(blrm_fit, newdata = candidate_doses)\n\ncount_freq &lt;- function(x, breaks = 0:4){\n  x &lt;- factor(x, breaks)\n  setNames(\n    prop.table(table(x)),\n    paste0(\"P(\", breaks, \" DLTs)\")\n  )\n}\n\nkable(bind_cols(\n  candidate_doses,\n  summarize_draws(predictive_draws, count_freq) %&gt;% select(-variable)\n), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndose\nstd_dose\nnum_patients\nP(0 DLTs)\nP(1 DLTs)\nP(2 DLTs)\nP(3 DLTs)\nP(4 DLTs)\n\n\n\n\n10\n-1.609\n4\n0.631\n0.271\n0.082\n0.014\n0.002\n\n\n25\n-0.693\n4\n0.231\n0.296\n0.250\n0.161\n0.062\n\n\n50\n0.000\n4\n0.077\n0.159\n0.223\n0.270\n0.271\n\n\n\n\n\n\n\n6.5.4 Dose escalation decisions\nOne common framework for guiding dosing decisions using a model-based dose escalation framework is known as Escalation With Overdose Control (EWOC). Decisions are based on an MTD threshold above which constitutes excessive toxicity \\(\\pi_{over}\\) (typically \\(\\pi_{over}\\)), and a so-called feasibility bound \\(c\\):\n\\[ \\text{EWOC satisfied at }d \\iff \\Pr( \\pi(d) &gt; \\pi_{over}) \\leq c \\]\nFurthermore, we often define another cutoff \\(\\pi_{targ}\\), and summarize the posterior probabilities for three intervals,\n\\[ \\Pr(d\\text{ is an underdose}) = \\Pr( \\pi(d) &lt; \\pi_{targ} ) \\] \\[ \\Pr(d\\text{ is in the target range}) = \\Pr( \\pi_{targ} \\leq \\pi(d) &lt; \\pi_{over} ) \\] \\[ \\Pr(d\\text{ is an overdose}) = \\Pr( \\pi(d) &gt; \\pi_{over} ),\\]\nand evaluate EWOC by checking if the last quantity exceeds \\(c\\).\nFor the set of candidate doses, the EWOC criteria can be evaluated as follows:\n\npi_target &lt;- 0.16 # motivated by 3+3 design (~ 1/6)\npi_over &lt;- 0.33 # motivated by 3+3 design (~ 1/3)\nc &lt;- 0.25\n\nraw_draws &lt;- posterior_linpred(blrm_fit, newdata = candidate_doses, transform = TRUE)\ndraws &lt;- posterior::as_draws_matrix(raw_draws)\ninterval_probs &lt;- posterior::summarize_draws(\n  draws,\n  list(\n    function(x){\n      prop.table(table(cut(x, breaks = c(0, pi_target, pi_over, 1))))\n    }\n  )\n)\n\nbind_cols(\n  select(candidate_doses, dose),\n  select(mutate(interval_probs, ewoc_ok = `(0.33,1]` &lt;= c), -variable)\n) %&gt;%\n  kable(digits = 3)\n\n\n\n\ndose\n(0,0.16]\n(0.16,0.33]\n(0.33,1]\newoc_ok\n\n\n\n\n10\n0.701\n0.253\n0.046\nTRUE\n\n\n25\n0.130\n0.308\n0.562\nFALSE\n\n\n50\n0.032\n0.105\n0.863\nFALSE",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#conclusion",
    "href": "src/02c_dose_escalation.html#conclusion",
    "title": "6  Oncology dose escalation",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nbrms is easily capable of handling this simple logistic regression model, and producing all the posterior summaries necessary for guiding a dose escalation trial with it. There are, of course, many extensions and complications to this basic methodology that we encounter frequently. The OncoBayes2 package is specifically tailored to this class of regression models, and covers all the typical use-cases. However, the approach described above for brms can be extended as well, for example to handle dose escalation for drug combinations, rather than single agents. See [Advanced Topics section on combination dose escalation][Combination dose escalation].",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02c_dose_escalation.html#exercise",
    "href": "src/02c_dose_escalation.html#exercise",
    "title": "6  Oncology dose escalation",
    "section": "6.7 Exercise",
    "text": "6.7 Exercise\nAn alternative to designs based on the BLRM is the so-called Bayesian Optimal INterval (BOIN) design (see Yan et al, 2020). In this exercise we will explore how the estimation of the MTD differs between the BOIN approach vs the BLRM vs a flexible monotone-regression approach that can be easily implemented in brms.\nSuppose are planning a trial with the following provisional doses.\n\nprovisional_doses &lt;- tribble(\n  ~dose_id, ~dose, ~prob_dlt,\n         1,     1,      0.05,\n         2,     2,       0.1,\n         3,     4,       0.2,\n         4,     8,       0.3,\n         5,    16,       0.5\n)\n\nTo generate data for comparing the MTD estimation methods, we begin by simulating a trial acording to true DLT rates prob_DLT above, and according to BOIN decision rules for escalation and de-escalation. We starting with dose = 2, and enroll cohorts of size 4 until either 12 cohorts have been enrolled, or more than 12 patients have been enrolled at any one dose. Below is some code that simulates such a trial.\n\nlibrary(BOIN)\n\n# Function to simulate a BOIN trial\nsimulate_BOIN &lt;- function(starting_dose_id,\n                          provisional_doses,\n                          cohort_size,\n                          num_cohorts,\n                          num_patients_stop,\n                          target_dlt_rate = 0.25,\n                          low_dlt_rate = 0.6 * target_dlt_rate,\n                          high_dlt_rate = 1.4 * target_dlt_rate){\n  \n  BOIN_decisions &lt;- BOIN::get.boundary(\n    target = target_dlt_rate,\n    ncohort = num_cohorts,\n    cohortsize = cohort_size,\n    n.earlystop = num_patients_stop,\n    p.saf = low_dlt_rate,\n    p.tox = high_dlt_rate\n  )\n  \n  current_dose_id &lt;- starting_dose_id\n  \n  dlt_data &lt;- provisional_doses %&gt;%\n    mutate(\n      num_patients = 0,\n      num_toxicities = 0,\n      eliminated = FALSE\n    )\n  \n  cohort_history &lt;- dlt_data[0,]\n  \n  cohort_time &lt;- 1\n  while(cohort_time &lt;= num_cohorts){\n    \n    current_dose_data &lt;- dlt_data[current_dose_id,]\n    \n    true_dlt_rate &lt;- dlt_data$prob_dlt[current_dose_id]\n    num_toxicities &lt;- rbinom(1, cohort_size, true_dlt_rate)\n    \n    current_dose_data$num_patients &lt;- current_dose_data$num_patients + cohort_size\n    current_dose_data$num_toxicities &lt;- current_dose_data$num_toxicities + num_toxicities\n    \n    \n    new_data &lt;- mutate(current_dose_data,\n                       num_patients = cohort_size,\n                       num_toxicities = !! num_toxicities)\n    \n    cohort_history &lt;- rbind(\n      cohort_history,\n      new_data\n    )\n    \n    dlt_data[current_dose_id, ] &lt;- current_dose_data\n    \n    if(current_dose_data$num_patients &gt;= num_patients_stop) break\n    \n    boundaries &lt;- setNames(\n      BOIN_decisions$full_boundary_tab[-1, current_dose_data$num_patients],\n      NULL\n    )\n    \n    comparison &lt;- c(\n      escalate = current_dose_data$num_toxicities &lt;= boundaries[1],\n      stay = current_dose_data$num_toxicities &gt; boundaries[1] & current_dose_data$num_toxicities &lt; boundaries[2],\n      de_escalate = current_dose_data$num_toxicities &gt;= boundaries[2],\n      eliminate = current_dose_data$num_toxicities &gt;= boundaries[3]\n    )\n    \n    decision &lt;- names(comparison[max(which(comparison))])\n    \n    if(decision == \"stay\"){\n      next_dose_id &lt;- current_dose_id\n    } else if(decision == \"escalate\"){\n      next_dose_id &lt;- current_dose_id + 1\n    } else{\n      next_dose_id &lt;- current_dose_id - 1\n    }\n    \n    if(decision == \"eliminate\") dlt_data$eliminated[current_dose_id] &lt;- TRUE\n    \n    if(next_dose_id &gt; nrow(dlt_data)){\n      next_dose_id &lt;- nrow(dlt_data)\n    }\n    \n    if(dlt_data$eliminated[next_dose_id]){\n      next_dose_id &lt;- next_dose_id - 1\n    }\n    \n    if(next_dose_id &lt; 1) break\n    \n    current_dose_id &lt;- next_dose_id\n    cohort_time &lt;- cohort_time + 1\n    \n  }\n  \n  mtd &lt;- BOIN::select.mtd(\n    target = target_dlt_rate,\n    npts = dlt_data$num_patients,\n    ntox = dlt_data$num_toxicities,\n    p.tox = high_dlt_rate\n  )\n  \n  return(\n    brms:::nlist(\n      dlt_data,\n      cohort_history,\n      mtd\n    )\n  )\n}\n\n\n# simulate a trial\nwithr::with_seed(\n  -1423048045,\n  {\n    \n    sim &lt;- simulate_BOIN(\n      starting_dose_id = 2,\n      provisional_doses = provisional_doses,\n      cohort_size = 4,\n      num_cohorts = 12,\n      num_patients_stop = 13,\n      target_dlt_rate = 0.25\n    )\n\n  }\n)\n\nThe output of this call include:\n\nsim$dlt_data # simulated cohort-by-cohort data\nsim$mtd # isotonic regression to determine the MTD; p_est shows the result\n\nIn the BOIN design, the MTD is selected based on isotonic regression of beta-binomial posterior quantiles (see Yan et al, 2020).\nNow for the exercise:\n\nUse brms to fit a BLRM similar to these data with reference dose \\(d^* = 16\\) and the same prior as described in this section.\nIn brms, the mo() function is used to specify an ordinal covariate with a monotone relationship with the response. Fit a model in which the log odds of DLT (family = binomial(\"logit\")) is a monotone function of the dose_id variable defined in dlt_data (i.e. the covariate is an integer sequence indexing the provisional doses). How do these estimates compare to the estimates from the BLRM?\nCompare the estimates of DLT rate (e.g. posterior means and quantiles) by dose: for the models in #1 and #2, and as estimated by BOIN in sim$mtd$p_est. Does the BOIN MTD differ from the MTD that would result from taking the highest dose satisfying EWOC from models #1 and #2?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Oncology dose escalation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html",
    "href": "src/02e_multiple_imputation.html",
    "title": "7  Multiple imputation",
    "section": "",
    "text": "7.1 Background",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#background",
    "href": "src/02e_multiple_imputation.html#background",
    "title": "7  Multiple imputation",
    "section": "",
    "text": "7.1.1 Why multiple imputation?\nIn clinical trials, some data we would like to observed will inevitably be missing, or the data that we observe will not be of interest for our estimand (the quantity we really want to estimate with our clinical trial) due to some intercurrent event (https://database.ich.org/sites/default/files/E9-R1_Step4_Guideline_2019_1203.pdf). A common approach to deal with this situation is to perform some form of imputation for such values. We do this, because there is no realistic scenario under which an analysis of only the observed data (“complete case analysis”) targets a meaningful estimand in a valid way.\nWhen would a complete case analysis ever be appropriate? One scenario would be for a hypothetical “as-if-patients-had-been-able-to-complete-treatment” estimand, if we assume that patients stopping treatment or stopping trial participation happens completely at random and has nothing to do with patient characteristics and previous (efficacy and safety) outcomes for the patients. However, this is obviously rather implausible and even if it were the case, a pharmaceutical company would be hard pressed to convince the scientific community and regulatory authorities that it is so.\nAt least until about 2010, people would often impute a single value to replace any data that were missing or not of interest. One popular single value imputation approach was “last-observation-carried-forward” (LOCF), in which the last observed value of interest was used to as a single imputation for any future values. However, even if the imputed value is the best possible guess for the missing value, imputing a single value completely ignores the uncertainty about the unobserved value. This is where multiple imputation (MI) comes in. MI imputes not one single value, but multiple ones. This set of values represent a sample from a distribution that describes the uncertainty about the unobserved values.\n\n\n7.1.2 How do the inner workings of multiple imputation look like?\nIn general, an analysis using MI involves four steps (Rubin Donald B. “Multiple imputation for nonresponse in surveys”. Hoboken: Wiley; 2004.):\n\nA (Bayesian) imputation model is fit to the data and \\(M\\) samples are drawn from the posterior distribution of the model parameters given the data.\n\nThis imputation model could be a single model for all treatment groups without treatment group by covariate interactions, a model that has completely separate parameters for each treatment group, or a model that assumes that (some) parameters in different treatment groups are similar.\nIn a single model for all treatment groups, we can also allow for treatment by covariate interactions.\nGenerally, we need to ensure that imputation model is “congenial” to the analysis model. I.e. it needs to be able to capture all the complexity present in the analysis model. It will often be more complex than the analysis model, for example it might be a model for the longitudinal data of patients over time when the analysis model is only for the final study visit, or it might separately model on- and off-treatment records for patients.\n\nFor each of the \\(M\\) posterior samples \\(M=1,\\ldots,M\\) the missing data are simulated based on the sampled values from the posterior of the model parameters. What set of parameter values applies depends on the estimand of interest.\n\nUnder a treatment policy estimand implemented using a “jump-to-reference” (J2R) approach, we would impute missing values after treatment discontinuation in all treatment groups based on placebo group parameters.\nAlternatively, we could target a treatment policy estimand by imputing missing values after treatment discontinuation in each treatment group based on parameters that describe what happens in that treatment group after treatment discontinuation (based on observed off-treatment data).\nUnder a hypothetical “as-if-patients-had-been-able-to-complete-treatment” estimand, we would impute missing values for patients based on the parameters of the treatment group they were assigned to.\nWe will often use have patient specific latent random effect(s) that are used to reflect the correlation of observation from the same patient. In both of the scenarios described above, we would use the sampled values of the random effect(s) for each patients to simulate missing values for the patient. It usually computationally convenient to use latent normal random effects.\n\nEach of the \\(M\\) sets of partially directly observed and partially imputed data are analyzed separately. This is done using an analysis model that need not be identical to the imputation model.\nThe results of the \\(M\\) analyses are aggregated e.g. using Rubin’s rule (Rubin Donald B. “Multiple imputation for nonresponse in surveys”. Hoboken: Wiley; 2004.).\n\nThis all sounds pretty complicated, but it gets surprisingly easy with brms. Interestingly, brms let’s us specify some pretty complex imputation models for a wide range of situations that would otherwise require bespoke and error-prone code e.g. in Stan. We will illustrate that in the next sections.\n\n\n7.1.3 How many imputations to use?\nWhile older literature suggests as few as 3 to 5, or maybe 50 imputations, there is usually some further reduction in standard errors by using a larger number of observations such as 250. Additionally, it is desirable to make our results as independent of pure play of chance in our MCMC sampling (it is just akward when the interpretation of the results may change with the choice of a random number seed). Thus, in practice we would frequently use 1000 or even 2500 imputations, as long as the added runtime is not prohibitive.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#data",
    "href": "src/02e_multiple_imputation.html#data",
    "title": "7  Multiple imputation",
    "section": "7.2 Data",
    "text": "7.2 Data\nWe will need the following R packages in this Section:\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(ggrepel)\nlibrary(MASS)\nlibrary(emmeans)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(7894562)\n# we also disable normalization of the likelihood which accelerates Poisson models used\noptions(brms.normalize=FALSE)\n\nLet us consider some simulated data for a trial in chronic obstructive pulmonary disease (COPD). Patients are enrolled into this trial, if they had at least one COPD exacerbation requiring oral corticosteroids, antibiotics, emergency department visit or hospitalization, or leading to death. Patients are then randomized to receive a drug or placebo and followed for up to 1 year. However, some patients discontinue treatment before 1 year. Some proportion of these patients agrees to be followed until the end of the trial, while some proportion is lost to follow-up.\n\npatients &lt;- 2000 # Total number of patients randomized\nmultiple &lt;- 10 # Multiple of patients randomized to be simulated before applying inclusion criteria\nminmum_for_inclusion &lt;- 1 # Mininum number of events in previous year for inclusion in trial\nmean_rate &lt;- 1 # Mean event rate in simulated population before applying inclusion criteria\nkappa &lt;- 1.5 # Dispersion parameter of simulated population\nstudy_length &lt;- 1 # Length of simulated study\ndiscontinuation_rate &lt;- 0.2 # Exponential rate at which patients discontinue treatment\nloss_proportion &lt;- 0.5 # Proportion of discontinued patients that is lost to follow-up\nnum_imputations &lt;- 500 # number of imputations to use\nset.seed(67588)\n\n# Simulate true event rates for patient population that is screened\npatient_rates &lt;- rgamma(n=patients*multiple, shape=1/kappa, rate = 1/kappa)*mean_rate\n# Draw random number of events in previous year before trial\nprevious_year &lt;- rpois(n=patients*multiple, patient_rates)\n# Apply inclusion criteria\npatient_rates &lt;- patient_rates[previous_year&gt;=minmum_for_inclusion][1:patients]\nprevious_year &lt;- previous_year[previous_year&gt;=minmum_for_inclusion][1:patients]\n\n# Simulate data for patients that met inclusion criteria\ncount_data &lt;- tibble(patient=1:patients, \n                    BASE = previous_year,\n                    treatment = ifelse(patient&lt;=patients/2, 0L, 1L),\n                    discontinuation_time = rexp(n=patients, rate=discontinuation_rate),\n                    lost_to_followup = rbinom(n=patients,size=1, prob=loss_proportion),\n                    patient_follow_up = min(study_length, discontinuation_time),\n                    #drop_out = (drop_out_time&lt;study_length),\n                    patient_rate = patient_rates[patient]) %&gt;%\n  left_join(expand_grid(patient=1:patients,\n                        period=factor(1L:3L, levels=1L:3L, \n                                      labels=c(\"On-treatment\", \n                                               \"Post-treatment\", \n                                               \"Lost-to-follow-up\"))), \n            by = \"patient\", multiple = \"all\") %&gt;%\n  mutate(follow_up = case_when(\n    period==\"On-treatment\" ~ pmin(discontinuation_time, study_length),\n    period==\"Post-treatment\" ~ (1-lost_to_followup) * pmax(study_length-discontinuation_time, 0), \n    period==\"Lost-to-follow-up\" ~ lost_to_followup * pmax(study_length-discontinuation_time, 0),\n    TRUE ~ 1.0),\n    events = ifelse(period==\"Lost-to-follow-up\", NA_integer_,\n                    rpois(n=patients*3, \n                          lambda=follow_up*patient_rate*0.5^(treatment*(period==\"On-treatment\")))),\n    patient = factor(patient, levels=1:patients),\n    treatment = factor(treatment, levels=0L:1L),\n    logBASE = log(BASE))\n\ncount_data  %&gt;% \n  group_by(treatment, period) %&gt;% \n  summarize(Events=sum(events), \n            `Patient-years`=sum(follow_up), \n            `Event rate (/p-y)` = Events/`Patient-years`,\n            `Total events prev. year` = sum(BASE),\n            `Rate in previous year` = mean(BASE),\n            .groups=\"drop\") %&gt;%\n  knitr::kable(digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nperiod\nEvents\nPatient-years\nEvent rate (/p-y)\nTotal events prev. year\nRate in previous year\n\n\n\n\n0\nOn-treatment\n1505\n903.36\n1.67\n2182\n2.18\n\n\n0\nPost-treatment\n78\n48.18\n1.62\n2182\n2.18\n\n\n0\nLost-to-follow-up\nNA\n48.47\nNA\n2182\n2.18\n\n\n1\nOn-treatment\n753\n898.05\n0.84\n2162\n2.16\n\n\n1\nPost-treatment\n72\n50.63\n1.42\n2162\n2.16\n\n\n1\nLost-to-follow-up\nNA\n51.32\nNA\n2162\n2.16",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#model-description",
    "href": "src/02e_multiple_imputation.html#model-description",
    "title": "7  Multiple imputation",
    "section": "7.3 Model description",
    "text": "7.3 Model description\n\n7.3.1 Negative binomial regression as a Poisson random effects model\nOur example uses overdispersed count data that follows a negative binomial distribution. We will use the parameterization of the negative binomial distribution, which can be derived as a Poisson random effects model with independent and identically distributed (i.i.d.) random effects. These random effects follow a gamma distribution with shape parameter \\(1/\\kappa\\) and a rate parameter \\(1/\\kappa\\) \\[\\begin{equation*}\nU_i \\sim \\text{Gamma}(1/\\kappa, 1/\\kappa),\n\\end{equation*}\\] where \\(\\kappa&gt;0\\). The distribution of the \\(Y_i\\) for patient \\(i=1,\\ldots,N\\) conditional on the random patient effect \\(U_i=u_i\\) and on the observed follow-up time \\(T_i=t_i\\) is \\[\\begin{equation}\\label{eq:poissondist}\nY_i|(U_i=u_i \\text{ and } T_i=t_i) \\sim \\text{Poisson}(\\mu u_i t_i).\n\\end{equation}\\] Note that as \\(\\kappa\\) approaches zero the negative binomial distribution becomes increasingly similar to a Poisson distribution. The glm.nb in the MASS R package instead uses a parametrization in terms of \\(\\theta := 1/\\kappa\\).\nThus, a negative binomial regression model is a Poisson random effects model with a log-link function: \\[\\begin{equation}\\label{eq:Poisson_re_model}\n\\log E(Y_i|U_i=u_i \\text{ and } T_i=t_i) = \\log \\mu + \\log u_i + \\log t_i\n\\end{equation}\\] or more generally: \\[\\begin{equation*}\n\\log E(Y_i|U_i=u_i \\text{ and } T_i=t_i) = \\boldsymbol{x_i}\\boldsymbol{\\beta} + \\log u_i + \\log t_i.\n\\end{equation*}\\] \\(\\boldsymbol{x_i}\\) is the covariate vector for subject \\(i\\) and \\(\\log t_i\\) is an offset variable — i.e. a covariate with coefficient 1. In a clinical trial setting \\(\\boldsymbol{\\beta}\\) will usually consist of the coefficient for an intercept \\(\\beta_0\\), coefficients \\(\\beta_k\\) for each test treatment group \\(k=1, \\ldots, K\\) — these are the log-rate-ratios for each test group compared to the control group — and the coefficients for other covariates. Other covariates might be e.g.@ the logarithm of number of events in some preceding time period.\n\n\n7.3.2 Extension to multiple observed time periods per patient\nWe can extent this model to having multiple records per patient with potentially different follow-up and covariates in each time period, with all observations from the same patient being linked through/sharing the same random effect \\(u_i\\). E.g. for observation \\(j\\) of patient \\(i\\), we can use \\[\\begin{equation*}\n\\log E(Y_{ij}|U_i=u_i \\text{ and } T_{ij}=t_{ij}) = \\boldsymbol{x_{ij}}\\boldsymbol{\\beta} + \\log u_i + \\log t_{ij},\n\\end{equation*}\\]\nInstead of gamma-distributed random effects \\(\\log U_i\\), we will use normally distributed random effects \\(\\nu_i \\sim N(0, \\tau)\\) in our imputation model. This is computationally an easier model to fit for brms. Additionally, we can approximate the distribution of a log-transformed gamma random variable quite well with a normal distribution.\n\n\n7.3.3 An imputation framework for missing count data\nWe will look at different estimands for count data that have all been used in practice:\n\nHypothetical estimand (“as if all patients had stayed on treatment until the end of the trial”)\n\nApproach #1: Maximum likelihood estimation using a negative binomial regression model for the on-treatment data (discarding any observed off-treatment data)\nApproach #2: Explicit imputation of off-treatment or lost-to-follow-up time periods based on on-treatment data.\nThese two approaches should give almost identical results.\nIn the second approach we fit a Bayesian imputation model on the on-treatment data. Then we perform prediction with this model (=multiple imputation) setting the treatment covariate to the treatment assigned as randomization for the records that need imputation. We will impute hypothetical on-treatment data for both time periods that were unobserved, but also for any observed post-treatment time periods.\n\nTreatment policy estimand (reflecting that after treatment discontinuations patients are off treatment):\n\nApproach #1: Imputation from observed off-treatment data\nApproach #2: Imputation based on the control (“reference”) group event rate (“jump-to-reference” or “J2R”)\nIn the first approach, we fit a Bayesian imputation model on the on-treatment and observed off-treatment data. We add a “type of time period” (on- or off-treatment) covariate, as well as its interaction with the assigned treatment group. This allows us to predict off-treatment records for all treatment groups based on the observed off-treatment data by setting the type of time period to “off-treatment” for the periods that we did not observe data for.\nIn the second approach, we fit a Bayesian imputation model on the on-treatment data and impute the unobserved off-treatment data based on the reference group distribution (i.e. we set the treatment covariate to the reference group before predicting the unobserved data for these time periods). We do not impute the observed off-treatment data and include it in the data for each patient before the analysis of each imputed dataset.\nArguably the first approach is the more logical, because it directly uses the available evidence on what happens after discontinuing treatment. However, there are some difficulties. Firstly, there is often very little observed post-treatment data so that there will be a lot of uncertainty about the post-discontinuation event rate. Secondly, it is very challenging to model to what extent the event rate may change the more time has passed post-treatment-discontinuation (e.g. is there a residual treatment effect that needs to “wash-out” over time?) and in practice this is usually ignored.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#implementation",
    "href": "src/02e_multiple_imputation.html#implementation",
    "title": "7  Multiple imputation",
    "section": "7.4 Implementation",
    "text": "7.4 Implementation\n\n7.4.1 Imputation from observed off-treatment data\nFirst, let us conduct an imputation model based on the observed off-treatment data, in which we specify that event rates can be different on- and off-treatment for each treatment group. Using this model, we perform predictions for the missing post-treatment values (patients lost-to-follow-up).\nWe use family=poisson() here, because a Poisson model with a random patient effect on the intercept is approximately equivalent to a negative binomial regression model and this is a way of reflecting the correlation of multiple observation periods for a patient that is consistent with that. However, we could also make the imputation model more complex e.g. by allowing counts to be overdispersed within patient by using family=negbinomial(), or allowing for some interactions such as treatment:logBASE or period:logBASE.\n\n# Fit imputation model\nbrmfit1 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + period + treatment:period + logBASE,\n               data=count_data %&gt;% \n                   filter(period %in% c(\"On-treatment\", \"Post-treatment\") & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 11.2 seconds.\nChain 2 finished in 10.5 seconds.\nChain 3 finished in 13.8 seconds.\nChain 4 finished in 13.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 49.2 seconds.\n\n# Predictions for missing (lost-to-follow-up) post-treatment values\n# Output format is a matrix with each column being a patient record and each row being one imputation\nimputations1 &lt;- predict(brmfit1, \n                        newdata=count_data %&gt;% \n                            filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n                            mutate(period=\"Post-treatment\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\n# Combine predictions with existing data frame of lost-to-follow-up records\n# (results in one row per imputation per patient), then bind together with \n# non-missing values (on-treatment and observed off-treatment values), for\n# which we have one record per patient and `.imputation` will be missing.\nimputed1 &lt;- count_data %&gt;% \n    filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations1[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n    dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period %in% c(\"On-treatment\", \"Post-treatment\") & \n                     follow_up&gt;0))\n\n\n\n7.4.2 Imputation based on the control group event rate (“jump-to-reference”)\nNow, we do an imputation based on control group data (i.e. assuming that after treatment discontinuation patients are off treatment and we use either the observed off-treatment data or impute missing data based on the control group event rate).\nFor this the code looks very similar to what we did in the previous subsection. We just slightly change our imputation model, and now predict based on setting the period covariate to \"On-treatment\" and the treatment covariate to \"0\" (=control group).\n\nbrmfit2 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + logBASE,\n               data=count_data %&gt;% \n                   filter(period==\"On-treatment\" & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 10.1 seconds.\nChain 2 finished in 10.1 seconds.\nChain 3 finished in 12.9 seconds.\nChain 4 finished in 12.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.4 seconds.\nTotal execution time: 45.9 seconds.\n\nimputations2 &lt;- predict(brmfit2, \n                        newdata=count_data %&gt;% \n                            filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n                        mutate(period=\"On-treatment\",\n                               treatment=\"0\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\nimputed2 &lt;- count_data %&gt;% \n    filter(period %in% \"Lost-to-follow-up\" & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations2[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n  dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period %in% c(\"On-treatment\", \"Post-treatment\") & \n                     follow_up&gt;0))\n\n\n\n7.4.3 Imputation under a hypothetical estimand\nFinally, we also do an imputation based on on-treatment data of the same treatment group. I.e. we do an analysis under a hypothetical estimand “as if all patients had finished treatment”. As before, we just change our imputation model and the covariates for the data to be imputed accordingly.\n\nbrmfit3 &lt;- brm(formula = events | rate(follow_up) ~ 1 + (1|patient) + treatment + logBASE,\n               data=count_data %&gt;% \n                   filter(period==\"On-treatment\" & follow_up&gt;0),\n               family=poisson(),\n               refresh = 0, silent = 0)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 12.5 seconds.\nChain 2 finished in 9.7 seconds.\nChain 3 finished in 10.3 seconds.\nChain 4 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.3 seconds.\nTotal execution time: 45.8 seconds.\n\nimputations3 &lt;- predict(brmfit3, \n                        newdata=count_data %&gt;% \n                            filter(period %in% c(\"Lost-to-follow-up\", \"Post-treatment\") & follow_up&gt;0) %&gt;%\n                         mutate(period=\"On-treatment\"),\n                        summary = F,\n                        ndraws = num_imputations) \n\nimputed3 &lt;- count_data %&gt;% \n    filter(period %in% c(\"Lost-to-follow-up\", \"Post-treatment\") & follow_up&gt;0) %&gt;%\n    mutate(row_number=1:n(),\n           imputed_values = map(row_number, \n                                function(x) tibble(events=imputations3[,x]) %&gt;% \n                                            mutate(.imputation=1:n()))) %&gt;%\n    dplyr::select(-events) %&gt;%\n    unnest(imputed_values) %&gt;%\n    bind_rows(count_data %&gt;% \n              filter(period==\"On-treatment\" &  follow_up&gt;0))\n\n\n\n7.4.4 Combining the imputations and a quick look at them\nWe combine the imputations from the different approaches and have a look at the mean rates under the different imputation approaches. As you can see, assuming a continued treatment effect results in a lower imputed mean event rate than assuming a cessation of the treatment effect (or basing the imputation on observed off-treatment data).\n\n# How different are the numbers?\nimputed &lt;- bind_rows(\n  imputed1 %&gt;%\n    mutate(Method=\"Retrieved drop-out imputation\"),\n  imputed2 %&gt;%\n    mutate(Method=\"Jump-to-reference (J2R)\"),\n  imputed3 %&gt;%\n    mutate(Method=\"Hypothetical (MAR)\")\n) \n\nimputed %&gt;%\n  group_by(Method, treatment, period, patient) %&gt;%\n  summarize(event_rate=mean(events/follow_up), .groups=\"drop\") %&gt;%\n  group_by(treatment, period, Method) %&gt;%\n  summarize(event_rate=mean(event_rate), .groups=\"drop\")\n\n# A tibble: 18 × 4\n   treatment period            Method                        event_rate\n   &lt;fct&gt;     &lt;fct&gt;             &lt;chr&gt;                              &lt;dbl&gt;\n 1 0         On-treatment      Hypothetical (MAR)                 1.61 \n 2 0         On-treatment      Jump-to-reference (J2R)            1.61 \n 3 0         On-treatment      Retrieved drop-out imputation      1.61 \n 4 0         Post-treatment    Hypothetical (MAR)                 1.68 \n 5 0         Post-treatment    Jump-to-reference (J2R)            1.48 \n 6 0         Post-treatment    Retrieved drop-out imputation      1.48 \n 7 0         Lost-to-follow-up Hypothetical (MAR)                 1.56 \n 8 0         Lost-to-follow-up Jump-to-reference (J2R)            1.55 \n 9 0         Lost-to-follow-up Retrieved drop-out imputation      1.54 \n10 1         On-treatment      Hypothetical (MAR)                 0.830\n11 1         On-treatment      Jump-to-reference (J2R)            0.830\n12 1         On-treatment      Retrieved drop-out imputation      0.830\n13 1         Post-treatment    Hypothetical (MAR)                 0.859\n14 1         Post-treatment    Jump-to-reference (J2R)            1.49 \n15 1         Post-treatment    Retrieved drop-out imputation      1.49 \n16 1         Lost-to-follow-up Hypothetical (MAR)                 0.899\n17 1         Lost-to-follow-up Jump-to-reference (J2R)            1.75 \n18 1         Lost-to-follow-up Retrieved drop-out imputation      1.49 \n\n\n\n\n7.4.5 Negative binomial regression for each imputed dataset\nWe can now conduct an analysis of each imputed dataset and look at how much the results vary between them. To do that, we need to fit a negative binomial regression to each imputed dataset. Negative binomial regression is implemented in all popular statistical software, but there are some particular challenges. Skip the next sub-section, if you are not interested in these particular points regarding negtive binomial regression using R.\n\n7.4.5.1 Negative binomial regression in R (optional subsection)\nFor example, the SAS/STATsoftware provides the GENMOD procedure and for the Poisson random effects version the COUNTREG procedure. Similarly, R has the glm.nb function in the MASS package. \nThese implementations differ in their parameterization. The GENMOD procedure estimates \\(\\kappa\\) and reverts to Poisson regression on the boundary of the parameter space when \\(\\hat{\\kappa}=0\\). The glm.nb function parameterizes the model in terms of \\(\\theta:=1/\\kappa\\) and its algorithm will not converge to a finite estimate \\(\\hat{\\theta}\\) whenever the maximum likelihood estimator for \\(\\kappa\\) is \\(\\hat{\\kappa}=0\\). However, after a sufficient number of iterations \\(\\hat{\\theta}\\) will be large enough that the estimates of the regression coefficients will be those we would obtain by Poisson regression — alternatively, we can use the results from Poisson regression in this case. This is implemented in the code below.\nInstead of the preferable observed Fisher information used in the GENMOD procedure, the glm.nb function uses the expected Fisher information for the standard errors of the regression coefficients — while the standard error for \\(\\hat{\\theta}\\) is based on the observed rather than the expected Fisher information — as of version 7.3-47 of the MASS package. Especially for small samples sizes this may result in too small standard errors. Thus, they should — especially for confirmatory clinical trials — be corrected, which is easily done using code provded by Bartlett.  \n\nglm.nb.cov &lt;- function(mod) {\n  # Basis of code for this function: \n  # https://stats.stackexchange.com/questions/221648/negative-binomial-regression-in-r-allowing-for-correlation-between-dispersion\n  #given a model fitted by glm.nb in MASS, this function returns a variance covariance matrix for the\n  #regression coefficients and dispersion parameter, without assuming independence between these\n  #note that the model must have been fitted with x=TRUE argument so that design matrix is available\n  #formulae based on p23-p24 of http://pointer.esalq.usp.br/departamentos/lce/arquivos/aulas/2011/LCE5868/OverdispersionBook.pdf\n  #and http://www.math.mcgill.ca/~dstephens/523/Papers/Lawless-1987-CJS.pdf\n  \n  k &lt;- mod$theta\n  p &lt;- dim(vcov(mod))[1]\n  \n  #construct observed information matrix\n  obsInfo &lt;- array(0, dim=c(p+1, p+1))\n  \n  #first calculate top left part for regression coefficients\n  for (i in 1:p) {\n    for (j in 1:p) {\n      obsInfo[i,j] &lt;- sum( (1+mod$y/mod$theta)*mod$fitted.values*mod$x[,i]*mod$x[,j] / (1+mod$fitted.values/mod$theta)^2  )\n    }\n  }\n  \n  #information for dispersion parameter\n  obsInfo[(p+1),(p+1)] &lt;- -sum(trigamma(mod$theta+mod$y) - trigamma(mod$theta) -\n                                 1/(mod$fitted.values+mod$theta) + (mod$theta+mod$y)/(mod$theta+mod$fitted.values)^2 - \n                                 1/(mod$fitted.values+mod$theta) + 1/mod$theta)\n  \n  #covariance between regression coefficients and dispersion\n  for (i in 1:p) {\n    obsInfo[(p+1),i] &lt;- -sum(((mod$y-mod$fitted.values) * mod$fitted.values / ( (mod$theta+mod$fitted.values)^2 )) * mod$x[,i] )\n    obsInfo[i,(p+1)] &lt;- obsInfo[(p+1),i]\n  }\n  \n  #return variance covariance matrix\n  solve(obsInfo)\n}\n\nfitmodel &lt;- function(data) {\n  # Function to fit NB model - returns result of Poisson model,\n  # if estimate of 1/dispersion parameter appears to go towards infinity.\n  # Additionally, standard errors are calculated using the observed\n  # information matrix by calling the previously defined function for that.\n  poifit &lt;- glm(\n    data = data,\n    formula = events ~ treatment + logBASE + offset(logfollowup),\n    control = glm.control(maxit = 2500),\n    family = poisson()\n  )\n  # glm.nb may fail to converge with a message such as\n  # \"Warning: step size truncated due to divergence\"\n  # \"Error in glm.fitter(X[, \"(Intercept)\", drop = FALSE], Y, w, offset = offset,: NA/NaN/Inf in 'x'\"\n  # We need to make sure we catch that issue. In that case we set nbfit = list(theta=10001), which\n  # results in Poisson regression results being used (see check on nbfit$theta later).\n  nbfit &lt;- tryCatch(\n      glm.nb(\n        data = data,\n        formula = aval ~ events ~ treatment + logBASE + offset(logfollowup),\n        start = poifit$coefficients,\n        init.theta = 1.25,\n        control = glm.control(maxit = 30),\n        x = TRUE\n      ), warning = function(err)\n        list(theta = 99999),\n      error = function(err)\n        list(theta = 99999)\n    )\n  \n  emm_options(msg.nesting = FALSE,\n              msg.interaction = FALSE)\n  if (nbfit$theta &gt; 10000) {\n    results &lt;- data.frame(\n      treatment = sort(unique(data$treatment)),\n      logrr = c(0, summary(poifit)$coefficients[2, 1]),\n      logrrSE = c(NA, summary(poifit)$coefficients[2, 2]),\n      as.matrix(data.frame(summary(\n        emmeans(\n          poifit,\n          ~ factor(treatment),\n          weights = \"proportional\",\n          at = list(logfollowup = 0)\n        )\n      ))[, c(\"emmean\", \"SE\")])\n    )\n  } else {\n    results &lt;- data.frame(\n      trt = sort(unique(data$treatment)),\n      logrr = c(0, summary(nbfit)$coefficients[2:2, 1]),\n      logrrSE = c(NA, sqrt(diag(\n        glm.nb.cov(nbfit)\n      ))[2:2]),\n      as.matrix(data.frame(summary(\n        emmeans(\n          nbfit,\n          ~ factor(treatment),\n          weights = \"proportional\",\n          at = list(logfollowup = 0),\n          vcov = glm.nb.cov(nbfit)[1:length(nbfit$coefficients), 1:length(nbfit$coefficients)]\n        )\n      ))[, c(\"emmean\", \"SE\")])\n    )\n    \n  }\n  \n  return(results)\n}\n\n\n\n7.4.5.2 Fitting the negative binomial regression to the imputed data\nActually fitting negative binomial regression on each imputed datasets is now easy with the function above:\n\nresults &lt;- tibble(.imputation = 1:max(imputed$.imputation, na.rm = T)) %&gt;%\n  mutate(results = parallel::mclapply(.imputation , function(x)\n    bind_rows(\n      imputed %&gt;%\n        filter(\n          Method == \"Hypothetical (MAR)\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Hypothetical (MAR)\"),\n      imputed %&gt;%\n        filter(\n          Method == \"Retrieved drop-out imputation\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Retrieved drop-out imputation\"),\n      imputed %&gt;%\n        filter(\n          Method == \"Jump-to-reference (J2R)\" &\n            .imputation %in% c(x, NA_integer_)\n        ) %&gt;%\n        group_by(patient, treatment, BASE, logBASE, .imputation) %&gt;%\n        summarize(\n          logfollowup = log(sum(follow_up)),\n          events = sum(events),\n          .groups = \"drop\"\n        ) %&gt;%\n        fitmodel(data = .) %&gt;%\n        as_tibble() %&gt;%\n        filter(treatment == \"1\") %&gt;%\n        mutate(Method = \"Jump-to-reference (J2R)\")\n    ))) %&gt;%\n  unnest(results)\n\nresults %&gt;%\n  ggplot(aes(x=logrr, xmin=logrr-qnorm(0.975)*logrrSE,\n             xmax=logrr+qnorm(0.975)*logrrSE, y=.imputation, )) +\n  theme_bw(base_size=16) +\n  geom_vline(xintercept=0, linetype=2) +\n  geom_point() +\n  geom_errorbarh() +\n  facet_wrap(~Method) +\n  scale_x_continuous(limits = c(-0.9, 0)) +\n  ylab(\"Imputation number\") +\n  xlab(\"log-rate ratio (drug versus placebo, &lt;0 favors drug)\")\n\n\n\n\n\n\n\n\n\n\n\n7.4.6 Combining the results of the analysis of each dataset using Rubin’s rule\nFinally, we use Rubin’s rule to combine results across the different imputed datasets for each imputation method.\n\nrubins_rule &lt;- function(estimates, SEs) {\n  nmi &lt;- length(estimates)\n  mi_estimate &lt;- mean(estimates)\n  mi_SE &lt;- sqrt( sum(SEs^2)/nmi + (1+1/nmi) * sum((mi_estimate-estimates)^2) / (nmi-1) )\n  mi_Z &lt;- mi_estimate/mi_SE\n  mi_p &lt;- 2*min(pnorm(mi_Z), 1-pnorm(mi_Z))\n  return( tibble(logrr=mi_estimate, logrrSE=mi_SE, p=mi_p) )\n}\n\nfinal_results &lt;- results %&gt;%\n  group_by(Method) %&gt;%\n  do(rubins_rule(.$logrr, .$logrrSE)) %&gt;%\n  ungroup() %&gt;%\n  bind_rows(\n    count_data %&gt;%\n      filter(period==\"On-treatment\" & follow_up&gt;0) %&gt;%\n      mutate(logfollowup=log(follow_up)) %&gt;%\n      fitmodel(data=.) %&gt;% \n      as_tibble() %&gt;%\n      filter(treatment==\"1\") %&gt;% \n      mutate(Method=\"Hypothetical\\n(NegBin for on-treatment data)\",\n             p=2*min(pnorm(logrr/logrrSE), 1-pnorm(logrr/logrrSE)) ) %&gt;%\n      dplyr::select(-treatment, -emmean, -SE)\n  ) %&gt;%\n  mutate(upper = logrr + qnorm(0.975)*logrrSE,\n         lower = logrr - qnorm(0.975)*logrrSE)\n\n\n\n7.4.7 What if we want to do a Bayesian analysis after MI?\nIf we have multiple imputated datasets and wish to perform Bayesian inference, we would recommend the approach of described in the Bayesian Data Analysis book by Gelman et al. (Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A. and Rubin, D.B., 2014. Bayesian data analysis (3rd ed.). CRC press. p.452) that uses the (equally weighted) mixture distribution of the posterior distributions as the combined posterior distribution. I.e. we use all the (or a random subset) of the MCMC samples from each analysis as samples from the combined posterior distribution. This has been reported to perform well with a sufficient number of imputations such as 100 (Zhou, X. and Reiter, J.P., 2010. A note on Bayesian inference after multiple imputation. The American Statistician, 64(2), pp.159-163.).\nWe could of course write some code to fit separate Bayesian models for each imputed dataset and then combine the posterior samples, but we do not have to, because brms can take care of it for us. E.g. the code below would run 1 MCMC chain for each of 20 imputed datasets (without specifying any prior distributions, which we would in practice of course also do).\n\nlibrary(future)\nplan(multisession)\nfit_multiple1 &lt;- brm_multiple(\n    events | rate(follow_up) ~ treatment + logBASE,\n    data = map(1L:20L, \n               function(x) imputed1 %&gt;% \n                 filter(is.na(.imputation) | .imputation==x) %&gt;%\n                 group_by(patient, logBASE, treatment) %&gt;%\n                 summarize(events=sum(events), follow_up=sum(follow_up))),\n    family = negbinomial(),\n    chains = 1, seed = 6234)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#results",
    "href": "src/02e_multiple_imputation.html#results",
    "title": "7  Multiple imputation",
    "section": "7.5 Results",
    "text": "7.5 Results\nAs we can see the results from imputing explicitly “as if all patients had finished treatment” matches the results from a negative binomial regression of on-treatment data rather closely. Similarly, the two approaches for treatment policy estimands (jump-to-reference or imputation based on retrieved drop-outs) give quite similar results, but with higher variability when basing imputations on the small number of subjects with post-treatment-discontinuation data.\n\nfinal_results %&gt;%\n  ggplot(aes(x=exp(logrr), y=Method, xmin=exp(lower), xmax=exp(upper))) +\n  geom_vline(xintercept=1, linetype=1) +\n  geom_point() +\n  geom_errorbarh(height=0.1) +\n  scale_x_log10(limits=c(0.25, 1.0)) +\n  xlab(\"Rate ratio (drug vs. placebo)\\n(values &lt;1.0 favor drug over placebo)\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#conclusion",
    "href": "src/02e_multiple_imputation.html#conclusion",
    "title": "7  Multiple imputation",
    "section": "7.6 Conclusion",
    "text": "7.6 Conclusion\nImputation for overdispersed count data is not covered by popular alternatives for missing data imputation such as the PROC MI procedure in SAS or the Amelia package in R. We are able to quickyly implement something sensible in brms. For this purpose, brms enabled us to fit imputation models and do imputation based on MAR (hypothetical estimand), J2R (treatment policy estimand) or retrieved drop-out data (treatment policy estimand).\nWe can also fit multivariate imputation models (see the corresponding brms vignette) across different types of outcomes that could be missing and correlated. For that we would use syntax like bf1 &lt;- bf(severity | subset(is_obs_severity) ~ (1|p|gr(USUBJID, by=treatment))) + cumulative(), bf2 &lt;- bf(CHG1 | subset(is_obs_CHG1) ~ treatment + (1|p|gr(USUBJID, by=treatment))) + gaussian() and bf9 &lt;- bf(country | subset(is_obs_country) ~ (1|p|gr(USUBJID, by=treatment))) + categorical() in combination with formula = bf1 + bf2 + bf3 + set_rescor(FALSE) in our call to the brm function. However, such models can be more challenging to fit and we aim to add examples on this in the future.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02e_multiple_imputation.html#excercises",
    "href": "src/02e_multiple_imputation.html#excercises",
    "title": "7  Multiple imputation",
    "section": "7.7 Excercises",
    "text": "7.7 Excercises\n\n7.7.1 Excercise 1: Food allergy\nThis is example is about a simulated randomized controlled trial of a new treatment compared with placebo for peanut allergy. The primary endpoint is the result of a double-blind, placebo controlled food challenge (DBPCFC), during which patients take increasing amounts of peanut flour (1, 3, 10, 30, 100, 300, 600 and 1000 mg). Tolerating a higher amount of peaanut flour is a better outcome and the proportion of patients tolerating at least 600 mg at week 52 will be analyzed in the primary analysis. DBPCFC is performed at baseline, at week 12, 26 and 52 (1-year). The code below creates a tibble with simulated data from a hypothetical study.\n\nset.seed(1234)\n\npatients = 200L\namounts = c(1, 3, 10, 30, 100, 300, 600, 1000)\npatient_re = rnorm(patients, mean=1, sd=0.5)\n\nsimulated = expand_grid(patient=1L:patients, visit=c(0L,12L,26L,52L)) %&gt;%\n  mutate(Treatment = 1L*(patient&gt;patients/2)) %&gt;%\n  arrange(patient, visit) %&gt;%\n  group_by(patient) %&gt;%\n  mutate(\n    discontinued = ( cumsum( (visit&gt;=12L)*rbinom(n=n(), size = 1, prob=0.2) ) &gt;=1),\n    lost_to_followup = ( cumsum( discontinued * rbinom(n=n(), size=1, prob=0.3) ) &gt;= 1) ,\n    latent = exp(rnorm(n=n(), sd=1.0) + 4.0*(visit&gt;=12L) + ((visit&gt;=12L)*2.0+(visit&gt;=26)*2.0+(visit&gt;=52)*1.0)*Treatment*(discontinued==F) + patient_re[patient]),\n    aval = case_when(latent&lt;1 ~ 1L,\n                     latent&lt;3 ~ 2L,\n                     latent&lt;10 ~ 3L,\n                     latent&lt;30 ~ 4L,\n                     latent&lt;100 ~ 5L,\n                     latent&lt;300 ~ 6L,\n                     latent&lt;600 ~ 7L,\n                     latent&lt;1000 ~ 8L,\n                     TRUE ~ 9L))  %&gt;%  \n  mutate(aval = ifelse(lost_to_followup, NA_integer_, aval)) %&gt;% \n  ungroup() %&gt;%\n  dplyr::select(-latent)\n\nmodel_data = simulated %&gt;%\n  filter(visit&gt;0L) %&gt;%\n  left_join(simulated %&gt;% \n              filter(visit==0L) %&gt;% \n              mutate(base=aval) %&gt;% \n              dplyr::select(patient, base), \n            by=\"patient\") %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\")),\n         aval = ordered(aval, levels=1L:9L),\n         visit = factor(visit, levels=c(12L, 26L, 52L)))\n\nSummarizing the data by treatment group and visit as a bar plot, it appears like the new drug may have a large effect on the outcome of the DBPCFC.\n\nsimulated %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\")),\n         `Tolerated amount\\nof peanut flour` = ordered(aval, levels=c(NA_integer_, 1L:9L),\n                        labels=c( \"&lt; 1 mg\", paste0(c(1, 3, 10, 30, 100, 300, 600, 1000), \" mg\"))),\n         visit = ordered(visit, levels=c(0L,12L,26L,52L),\n                         labels = paste0(\"Visit \", c(0,12,26,52)))) %&gt;%\n  ggplot(aes(x=Treatment, fill=`Tolerated amount\\nof peanut flour`)) +\n  geom_bar(col=\"darkgrey\") +\n  scale_fill_brewer(palette=\"PuBu\", na.value=\"black\") +\n  facet_wrap(~visit, nrow = 1, ncol=4) +\n  theme(legend.position=\"bottom\") +\n  ylab(\"Patients\") +\n  theme(legend.text=element_text(size=rel(0.5)), \n        legend.title = element_text(size=rel(0.5)),\n        legend.key.size = unit(0.2, \"cm\"))\n\n\n\n\n\n\n\n\nOr, as an alternative display as a spaghetti plot (note the handy seed option in position_jitter() to keep the line and point geoms aligned).\n\nsimulated %&gt;%\n  filter(!is.na(aval)) %&gt;%\n  mutate(Treatment = factor(Treatment, levels=0L:1L, labels=c(\"Placebo\", \"Drug\"))) %&gt;%\n  ggplot(aes(x=visit, y=aval, group=patient, color=Treatment)) +\n  geom_line(alpha=0.2, position = position_jitter(width = 0.2, height=0.1, seed = 123)) +\n  geom_point(alpha=0.2, position = position_jitter(width = 0.2, height=0.1, seed = 123)) +\n  scale_y_continuous(breaks=1:9, labels=c( \"&lt; 1 mg\", paste0(c(1, 3, 10, 30, 100, 300, 600, 1000), \" mg\"))) +\n  scale_x_continuous(breaks=c(0,12,26,52)) +\n  facet_wrap(~Treatment) +\n  xlab(\"Time since randomization (weeks)\") +\n  ylab(\"Tolerated amount of peanut flour\")\n\n\n\n\n\n\n\n\nExcercises in increasing order of complexity (* marks advanced questions):\n\nFit a simple logistic regression (i.e. glm(formula = response ~ Treatment, family=binomial(link=\"logit\"))) that uses a composite estimand treating discontinuation of treatment as equivalent to the worst response category (not tolerating 1 mg of peanut flour, i.e. model_data %&gt;% filter(visit == 52L) %&gt;% mutate(aval = ifelse(is.na(aval), 1L, aval), response = 1L*(aval &gt;= 8L))). Does this support the alternative hypothesis that the new drug is better than placebo? What if we change the null hypothesis to the difference in response rate being less than 10 percentage points higher than the placebo response rate (and the alternative hypothesis to it being 10 or more percentage points better than placebo)?\nCompare the results to imputing the ordinal outcome under a treatment policy estimand either based on the observed post-treatment data for patients that discontinued treatemnt. For this, you could use family=cumulative() and formula = aval ~ 1 + (1|patient) + visit*Treatment*discontinued*mo(base). Note that mo(base) implies a monotonic effect of the baseline DBPCFC outcome being higher without imposing a linear relationship. Allowing for all of the interactions we are specifying and the adjustment for baseline are examples of making an imputation model more complex than our analysis model. Perform, say, 100 imputations, perform the analysis for each imputation and combine estimates of the log-odds-ratio.\n\nIn this case, do we appear to gain much for the purposes of showing that the drug works from the more complex imputation approach?\nDo you think the answer to this would change, if half of the discontinuation occurred due to the COVID-19 pandemic and we imputed post-treatment data for these patients under a hypothetical estimand (assuming patients would continue to take treatment to the end of the study)?\n\nHow would you target the treatment policy estimand using the jump-to-reference approach? What would your imputation model look like and for which records would you perform the imputation?\n* Would a linear model for log-tolerated amount of peanut flour fit the data better than the ordinal regression model we used to fit the data? For this, treat not tolerating 1 mg as being right-censored below 1 mg (brms allows you to use the aval | cens(censored) ~ regression formula syntax, where the variable censored would contain the values 'none' for uncensored observations and 'right' for right-censored observations).\n* Perform similar analyses as above (1-3), but use an ordinal regression model e.g. via MASS::polr or rms::orm.\n\nFurther reading: Several blogposts by Frank Harrell cover proportional odds models (e.g. 1, 2, 3 and 4).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple imputation</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html",
    "href": "src/02g_longitudinal.html",
    "title": "8  Longitudinal data",
    "section": "",
    "text": "8.1 Background\nMany clinical trials assess efficacy and other endpoints at numerous timepoints post-baseline. For example, consider the case where a continuous response endpoint is assessed every 2 weeks post baseline (Week 2, Week 4, …, Week 12). Although clinical interest may focus on the response rate at a particular visit (e.g. at Week 12), efficacy data at other visits is of course still of interest, either in its own right, to build understanding of the full time vs. efficacy profile, or as a means to increase statistical power for estimation at Week 12.\nThere are multiple approaches for proceeding with estimation of the Week 12 treatment effect. Adopting a cross-sectional approach , one could ignoring all other post-baseline assessments except Week 12. If certain patients missed the Week 12 assessment, this missing data would need to be handled using an appropriate pre-defined strategy (e.g. for response rate endpoints, imputing missing outcomes with nonresponder status, carrying the last observation forward, multiple imputation approaches, or even dropping missing data altogether). Such approaches may lose signifcant power or incur bias if there is substantial missing data.\nAlternatively longitudinal models incorporate the full post-baseline set of assessments to facilitate modeling of efficacy at all visits. In some cases, these may substantially increase statistical power.\nThe brms package contains a deep suite of modelling tools for longitudinal data, including many options for modelling the mean trajectory across visits/time, and for modelling autocorrelated errors within patients across time. In this section we illustrate some potential uses of these tools in a clinical trial setting.\nlibrary(dplyr)\nlibrary(brms)\nlibrary(emmeans)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(here)\n# instruct brms to use cmdstanr as backend and cache all Stan binaries\noptions(brms.backend=\"cmdstanr\", cmdstanr_write_stan_file_dir=here(\"_brms-cache\"))\n# create cache directory if not yet available\ndir.create(here(\"_brms-cache\"), FALSE)\nset.seed(8904568)\ncontrol_args &lt;- list(adapt_delta = 0.95)\nadpasi &lt;- readr::read_csv(here::here(\"data\", \"longitudinal.csv\"), show_col_types = FALSE) %&gt;%\n  dplyr::filter(TRT01P %in% c(\"PBO\", \"TRT\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\npasi_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#data",
    "href": "src/02g_longitudinal.html#data",
    "title": "8  Longitudinal data",
    "section": "8.2 Data",
    "text": "8.2 Data\nThe example involves simulated results of a hypothetical Phase-II study of an experimental treatment for Psoriasis. Synthetic data are generated using the mmrm package. We consider a subset of the study involving 100 patients, 50 of whom were randomized to receive placebo, and 50 of whom received treatment.\nEfficacy was assessed using the Psoriasis Area and Severity Index (PASI), a numerical score which measures the severity and extent of psoriasis. This is assessed at baseline, and again at 7 post-baseline timepoints.\n\n\n\n\n\n\n\n\n\nTwo endpoints of interest based on the PASI score are (1) PASI change from baseline, and (2) the binary endpoint PASI 75, which defines a responder as any patient with at least a 75% change from baseline in PASI.\n\n\n\n\n\n\n\n\n\nThe data has been transformed to follow a typical CDISC Analysis Data Model (ADaM) format.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#models",
    "href": "src/02g_longitudinal.html#models",
    "title": "8  Longitudinal data",
    "section": "8.3 Models",
    "text": "8.3 Models\nThere are a few key ingredients to a longitudinal model for the PASI score outcomes.\n\nA mean model which describes the expected value of the response over time, across treatments, and across values of any other relevant covariates.\nA correlation model which describes the correlation structure of the error terms.\n\nbrms offers many modelling options for each component.\n\n8.3.1 Mean models\n\n8.3.1.1 Cell-means model\nThe most common approach for modeling the mean in clinical trial practice is to adopt a specification which allows the mean to vary freely across visits, without any parametric specification of the trajectory of the mean over time. Here we will call this the “cell-means” model, where levels of the treatment group and visit comprise the cells.\nIn this approach, we include all treatment-by-visit interactions. In brms, the formula specification would be:\n\n\nCHG ~ BASE + TRT01P * AVISIT\n\n\n\n\n8.3.1.2 Linear-in-time model\nA far stronger assumption would be to assume the mean response is linear in time (i.e. the number of weeks post baseline for visit). In this case, the formula specification would be\n\n\nCHG ~ BASE + TRT01P * AVISITN\n\n\nNote the key difference is the use of AVISITN (a numeric variable indicating the number of weeks post baseline) rather than AVISIT (a factor variable for the visit id).\nSuch a model should be considered only for exploratory modelling purposes, and not for confirmatory analyses, the main reason being that the linearity assumption cannot be assessed at the trial design stage before having collected the data.\nEven if the linearity assumption appeared reasonable over the time range explored in the trial, it should not be used to extrapolate outside the observed time period.\n\n\n8.3.1.3 Quadratic-in-time model\nIn the previous section, we saw there was a hint of curvature in the sample mean response over time:\n\n\n\n\n\n\n\n\n\nHence one might consider a mean model that assumes the mean response is quadratic in time rather than linear:\n\n\nCHG ~ BASE + TRT01P * AVISITN + TRT01P * AVISITN^2\n\n\nThis model should similarly not be used to extrapolate outside the week 1-12 window.\n\n\n8.3.1.4 Gaussian process prior\nAn interesting nonparametric alternative to the just-discussed specifications is based on a Gaussian process prior for the mean across visits within treatments. As in the case of the cell-means model, this model does not assume any parametric shape for the mean response over time. It assumes only that the mean response over time follows a continuous curve which is assigned a Gaussian process (GP) prior.\nWhile the GP itself is a continuous-time process, the joint distribution of a realization at any discrete set of timepoints is multivariate normal.\nIn brms, the formula specification would be as follows:\n\n\nCHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P)\n\n\nThe implication of this model is that there is a correlation between the mean response at any collection of visits, and the correlation between any pair of visits decays as the time between them increases, according to an exponential covariance function. See ?gp for additional details on the brms implementation.\n\n\n\n8.3.2 Correlation models\nThe repeated measurement of an endpoint over time on the same patient induces autocorrelation between the within-patient measurements. To illustrate, consider if we ignored the correlations and fit a model with uncorrelated errors. Below are scatterplots and correlation coefficients for the residuals of such a fitted model. The correlations are quite strong, especially between visits that are closer in time.\n\npasi_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\")\nlm_fit &lt;- lm(CHG ~ BASE + TRT01P * AVISIT, data = pasi_data)\nres &lt;- residuals(lm_fit)\npasi_data %&gt;%\n  mutate(res = res) %&gt;%\n  select(SUBJID, AVISIT, res) %&gt;%\n  pivot_wider(id_cols = SUBJID, names_from = AVISIT, values_from = res) %&gt;%\n  select(-SUBJID) %&gt;%\n  GGally::ggpairs()\n\n\n\n\n\n\n\n\nFailing to model these within-subject correlations will result in a loss of statistical power (wider confidence intervals, less powerful tests). brms offers several options for modelling within-subject correlations.\n\n8.3.2.1 Subject-level random effects\nOne way to achieve within-subject correlation is to use a model that includes a subject-level random effect. When the resulting mean response function is averaged over the distribution of the random effect terms (“integrating them out”), a uniform correlation is induced between all measurements on the same patient. The magnitude of the correlation is determined by the relative size of the random-effect variance and the error variance. (See exercise 1.)\nIn brms, a subject-level random intercept can be easily added in the formula specification. For example:\n\n\nCHG ~ BASE + TRT01P * AVISIT + (1 | SUBJID)\n\n\n\n\n8.3.2.2 Autoregressive correlation structures\nA very common model for serial correlation is based on an autoregressive process. Under such a process, the error term \\(\\varepsilon_t\\) at a timepoint \\(t\\) is explicitly dependent on some collection of preceeding error terms. Under a first-order autoregressive process, for example, it is dependent only on one of its predecessors: \\[ \\varepsilon_t = \\alpha\\varepsilon_{t-1} + Z_t,\\] for \\(t\\geq 2\\), where \\(Z_t\\) are iid \\(\\mathrm N(0, \\sigma^2)\\). The process is initialized with \\(\\varepsilon_1 \\sim \\mathrm N(0, \\sigma^2 / (1 - \\alpha^2))\\).\nThe resulting covariance matrix of a collection \\((\\varepsilon_1,\\ldots,\\varepsilon_T)\\) has a simple form; the reader is referred to the SAS paper “Guidelines for Selecting the Covariance Structure in Mixed Model Analysis” by Chuck Kincaid for detail on the AR structure (c.f. page 2), and other covariance structures.\nThe autocor argument of brm() and brmsformula() is used to set an autoregressive correlation strucutre. For order-1 autoregressive. Below is a choice of AR(1) autocorrelation that we might consider in the psoriasis example:\n\n\n~ar(time = AVISIT, gr = SUBJID, p = 1)\n\n\nThis choice implies that autocorrelation exists across visits within subjects.\n\n\n8.3.2.3 Compound symmetry\nAnother choice offered by brms is that of compound symmetry. The reader is again referred to the SAS paper linked above and ?cosy for more information.\n\n\n~cosy(time = AVISIT, gr = SUBJID)\n\n\n\n\n8.3.2.4 Other choices\nbrms offers several other choices for autocorrelation models. See ?'autocor-terms' for more a listing.\nWe note that currently, unstructured correlation models (a standard choice for MMRM specifications in clinical trial protocols) re not supported by brms.\nHowever, fixed correlation structures are, and one could consider plugging in for this an unstructured correlation estimate from a frequentist MMRM. For example,\n\n# fit a MMRM using gls\ngls_fit &lt;- nlme::gls(CHG ~ BASE + TRT01P * AVISIT,\n                     data = pasi_data,\n                     correlation = nlme::corSymm(form = ~ 1 | SUBJID))\n\n# estimated correlation matrices by subject\nSig_subj &lt;- nlme::corMatrix(gls_fit$modelStruct[[1]])\nSig_subj[1]\n\n$`1`\n            [,1]        [,2]      [,3]      [,4]        [,5]        [,6]\n[1,]  1.00000000 0.896714971 0.5883171 0.2669088 -0.03659905 -0.05666876\n[2,]  0.89671497 1.000000000 0.6313654 0.3841036  0.03162521  0.03550198\n[3,]  0.58831712 0.631365431 1.0000000 0.4509002  0.37604603  0.21075194\n[4,]  0.26690877 0.384103637 0.4509002 1.0000000  0.63152510  0.47401182\n[5,] -0.03659905 0.031625213 0.3760460 0.6315251  1.00000000  0.77205541\n[6,] -0.05666876 0.035501982 0.2107519 0.4740118  0.77205541  1.00000000\n[7,] -0.08775131 0.008196384 0.2465146 0.5921762  0.79671624  0.74110361\n             [,7]\n[1,] -0.087751306\n[2,]  0.008196384\n[3,]  0.246514616\n[4,]  0.592176181\n[5,]  0.796716238\n[6,]  0.741103610\n[7,]  1.000000000\n\n\nA block-diagonal matrix containing these estimated within-subject correlations could be plugged in as the M argument of the fcor() function in brms.\n\n\n\n8.3.3 Cross-sectional approaches\nFor modeling a continuous endpoint such as PASI, the cross-sectional analogue of the longitudinal models we’ve discussed is the ANCOVA model, in which a linear regression model (conditioning as before on the baseline PASI covariate) is fit to the Week-12 cross section of data.\n\n# analysis data includes only the Week 12 cross section\nancova_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL, AVISITN == 12)\n\n# formula specification for ANCOVA\nancova_formula &lt;- bf(\n  CHG ~ BASE + TRT01P,\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nancova_prior &lt;- get_prior(\n  ancova_formula,\n  data = ancova_data\n)\n\nancova_fit &lt;- brm(\n  ancova_formula,\n  prior = ancova_prior,\n    data = ancova_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nIn the next section, this model is also explored for the purposes of comparing to the longitudinal approaches",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#results",
    "href": "src/02g_longitudinal.html#results",
    "title": "8  Longitudinal data",
    "section": "8.4 Results",
    "text": "8.4 Results\nA typical estimand for longitudinal or ANCOVA involve Least Squares means (LS means) or estimated marginal means (EMM). Inference for EMMs is extremely convenient with brms due to its integration with the emmeans R package. The EMM can be roughly understood as the average, stratified by treatment group and visit, of the mean prediction across subjects in the trial.\nWe begin by briefly illustrating how emmeans can be used with a brmsfit object to estimate LS means and their contrasts.\n\nanalysis_data &lt;- filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL)\n\nbrms_formula &lt;- bf(\n  CHG ~ BASE + TRT01P * AVISIT,\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nprior &lt;- get_prior(\n  brms_formula,\n  data = analysis_data\n)\n\nfit &lt;- brm(\n  brms_formula,\n  prior = prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 12.3 seconds.\nChain 2 finished in 12.1 seconds.\nChain 3 finished in 12.6 seconds.\nChain 4 finished in 11.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 49.0 seconds.\n\nfit\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CHG ~ BASE + TRT01P * AVISIT \n         autocor ~ cosy(time = AVISIT, gr = SUBJID)\n   Data: analysis_data (Number of observations: 684) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncosy     0.41      0.04     0.33     0.51 1.00     3163     2356\n\nRegression Coefficients:\n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                 12.95      1.87     9.29    16.56 1.00     2673\nBASE                      -0.70      0.08    -0.85    -0.54 1.00     3934\nTRT01PTRT                 -3.56      1.59    -6.62    -0.45 1.00     1884\nAVISITWeek2               -0.16      1.16    -2.47     2.11 1.00     2034\nAVISITWeek4               -2.49      1.23    -4.87     0.02 1.00     2158\nAVISITWeek6               -0.87      1.31    -3.46     1.70 1.00     2142\nAVISITWeek8                1.10      1.34    -1.55     3.77 1.00     2250\nAVISITWeek10              -1.22      1.32    -3.78     1.43 1.00     2184\nAVISITWeek12              -3.66      1.17    -6.01    -1.41 1.00     2064\nTRT01PTRT:AVISITWeek2     -4.13      1.75    -7.59    -0.71 1.00     1942\nTRT01PTRT:AVISITWeek4     -6.15      1.79    -9.73    -2.59 1.00     2173\nTRT01PTRT:AVISITWeek6     -9.30      1.83   -12.89    -5.67 1.00     1916\nTRT01PTRT:AVISITWeek8    -12.69      1.85   -16.38    -9.16 1.00     1981\nTRT01PTRT:AVISITWeek10   -10.77      1.89   -14.41    -7.06 1.00     2080\nTRT01PTRT:AVISITWeek12    -7.38      1.73   -10.82    -4.09 1.00     1890\n                       Tail_ESS\nIntercept                  2488\nBASE                       3210\nTRT01PTRT                  2654\nAVISITWeek2                2992\nAVISITWeek4                2789\nAVISITWeek6                2597\nAVISITWeek8                2734\nAVISITWeek10               2506\nAVISITWeek12               2704\nTRT01PTRT:AVISITWeek2      2370\nTRT01PTRT:AVISITWeek4      2821\nTRT01PTRT:AVISITWeek6      2290\nTRT01PTRT:AVISITWeek8      2465\nTRT01PTRT:AVISITWeek10     2343\nTRT01PTRT:AVISITWeek12     2542\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.21      0.32     7.65     8.89 1.00     3278     2296\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# estimate the EMMs\nemm &lt;- emmeans(fit, c(\"TRT01P\", \"AVISIT\"), nesting = list())\nemm\n\n TRT01P AVISIT   emmean lower.HPD upper.HPD\n PBO    Week 1   -1.214     -3.38    0.8924\n TRT    Week 1   -4.782     -6.98   -2.3953\n PBO    Week 2   -1.378     -3.45    0.7517\n TRT    Week 2   -9.065    -11.30   -6.7377\n PBO    Week 4   -3.718     -6.02   -1.4343\n TRT    Week 4  -13.426    -15.69  -11.3379\n PBO    Week 6   -2.099     -4.40    0.3942\n TRT    Week 6  -14.976    -17.23  -12.6760\n PBO    Week 8   -0.144     -2.63    2.2711\n TRT    Week 8  -16.363    -18.77  -14.1551\n PBO    Week 10  -2.449     -4.94    0.0976\n TRT    Week 10 -16.780    -19.11  -14.3734\n PBO    Week 12  -4.874     -7.17   -2.7912\n TRT    Week 12 -15.795    -18.10  -13.5977\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n# estimate EMM contrasts\nemm_contrasts &lt;- contrast(emm, method = \"revpairwise\", by = \"AVISIT\")\nemm_contrasts\n\nAVISIT = Week 1:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -3.58     -6.53    -0.391\n\nAVISIT = Week 2:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -7.67    -10.81    -4.698\n\nAVISIT = Week 4:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO    -9.70    -12.87    -6.692\n\nAVISIT = Week 6:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -12.88    -16.12    -9.505\n\nAVISIT = Week 8:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -16.24    -19.47   -12.831\n\nAVISIT = Week 10:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -14.33    -17.57   -10.757\n\nAVISIT = Week 12:\n contrast  estimate lower.HPD upper.HPD\n TRT - PBO   -10.93    -14.22    -8.029\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n8.4.1 Fitting several models\nIn order to understand the impact of possible choices for the mean model and correlation model, we fit a series of models in loop.\nCode to prepare a list of models to be fit:\n\n\nCode\nsetup_analyses &lt;- function(adpasi){\n  \n  formulas &lt;- tribble(\n    ~endpoint,   ~formula_name,   ~paramcd,     ~family,                                             ~formula, ~correl, ~longitudinal,\n       \"PASI\",    \"cell-means\", \"PASITSCO\",  gaussian(),                         CHG ~ BASE + TRT01P * AVISIT,    TRUE,          TRUE,\n       \"PASI\",        \"linear\", \"PASITSCO\",  gaussian(),                        CHG ~ BASE + TRT01P * AVISITN,    TRUE,          TRUE,\n       \"PASI\",     \"quadratic\", \"PASITSCO\",  gaussian(), CHG ~ BASE + TRT01P * AVISITN + TRT01P * AVISITN ^ 2,    TRUE,          TRUE,\n       \"PASI\",            \"gp\", \"PASITSCO\",  gaussian(),       CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),    TRUE,          TRUE,\n       \"PASI\",        \"raneff\", \"PASITSCO\",  gaussian(),          CHG ~ BASE + TRT01P * AVISIT + (1 | SUBJID),   FALSE,          TRUE,\n       \"PASI\", \"cross-section\", \"PASITSCO\",  gaussian(),                                  CHG ~ BASE + TRT01P,   FALSE,         FALSE\n  )\n  \n  datasets &lt;- tribble(\n      ~paramcd, ~longitudinal, ~missing_approach,                                               ~analysis_data,\n    \"PASITSCO\",          TRUE,            \"drop\",                filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL),\n    \"PASITSCO\",         FALSE,            \"drop\", filter(adpasi, PARAMCD == \"PASITSCO\", DROPFL, AVISITN == 12)\n  )\n  \n  autocor &lt;- tribble(\n    ~correl, ~autocor_name, ~autocor,\n       TRUE,         \"AR1\", ~ ar(time = AVISIT, gr = SUBJID, p = 1),\n       TRUE,        \"COSY\", ~ cosy(time = AVISIT, gr = SUBJID)\n  )\n  \n  analyses &lt;- formulas %&gt;%\n    full_join(datasets, c(\"paramcd\", \"longitudinal\"), multiple = \"all\") %&gt;%\n    full_join(autocor, c(\"correl\"), multiple = \"all\") %&gt;%\n    replace_na(list(autocor_name = \"none\"))\n  \n  analyses\n  \n}\n\n\nCode to fit the models in a loop using clustermq:\n\n\nCode\nhere::i_am(\"src/longitudinal/fit_models.R\")\nlibrary(dplyr)\nlibrary(brms)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(here)\nlibrary(emmeans)\nlibrary(clustermq)\nlibrary(purrr)\n\nsource(here(\"src\", \"longitudinal\", \"setup_analyses.R\"))\n\n# iter &lt;- Sys.getenv(\"BRMS_ITER\")\n# chains &lt;- Sys.getenv(\"BRMS_CHAINS\")\n\nadpasi &lt;- readr::read_csv(here(\"data\", \"longitudinal.csv\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\nanalyses &lt;- setup_analyses(adpasi)\nanalyses &lt;- filter(analyses, endpoint == \"PASI\") %&gt;%\n  mutate(id = 1:n())\n\nfit_model &lt;- function(id, analysis_data, formula, formula_name,\n                      family, correl, autocor, autocor_name,\n                      save_individuals = FALSE){\n  \n  \n  # set up the formula ---------------------------------------------------------\n  \n  if(correl){\n    autocor &lt;- autocor\n  } else{\n    autocor &lt;- NULL\n  }\n    \n  brms_formula &lt;- bf(\n    formula,\n    autocor = autocor,\n    family = family,\n    center = FALSE,\n    nl = FALSE\n  )\n  \n  # fit the model --------------------------------------------------------------\n  \n  prior &lt;- get_prior(\n    brms_formula,\n    data = analysis_data\n  )\n  \n  cat(\"\\n*** Fitting model\", id, \": \", formula_name, \", \", autocor_name, \"***\\n\")\n  \n  fit &lt;- brm(\n    brms_formula,\n    prior = prior,\n    cores = 4,\n    backend = \"rstan\",\n    data = analysis_data\n  )\n  \n  if(save_individuals) saveRDS(fit, here::here(\"reports\", paste0(\"longitudinal_fit_\", id, \".rds\")))\n  \n  # estimate marginal means ----------------------------------------------------\n  \n  cat(\"\\n*** Estimating marginal means\", id, \": \", formula_name, \", \", autocor_name, \"***\\n\")\n  \n  base &lt;- analysis_data %&gt;%\n    select(SUBJID, BASE) %&gt;%\n    distinct()\n  \n  visits &lt;- analysis_data %&gt;%\n    select(AVISIT, AVISITN) %&gt;%\n    distinct()\n  \n  treatments &lt;- analysis_data %&gt;%\n    select(TRT01P) %&gt;%\n    distinct()\n  \n  emm &lt;- bind_rows(lapply(\n    split(visits, 1:nrow(visits)),\n    function(visit){\n      \n      nd &lt;- visit %&gt;%\n        crossing(treatments) %&gt;%\n        crossing(base) %&gt;%\n        split(.$TRT01P)\n      \n      lp &lt;- map(nd, ~ posterior_linpred(fit, transform = TRUE, newdata = .))\n      \n      marginal_mean &lt;- lapply(lp, rowMeans) %&gt;%\n        as_tibble() %&gt;%\n        mutate(diff = .[[2]] - .[[1]]) %&gt;%\n        setNames(c(names(nd),\n                   paste(rev(names(nd)), collapse = \" - \")))\n      \n      bind_cols(\n        visit,\n        summarise_draws(as_draws_df(marginal_mean))\n      ) %&gt;%\n        mutate(\n          TRT01P = factor(variable, levels(analysis_data$TRT01P)),\n          contrast = case_when(\n            is.na(TRT01P) ~ variable,\n            TRUE ~ NA_character_\n          )\n        )\n      \n    }\n  ))\n  \n  if(save_individuals) saveRDS(emm, here::here(\"reports\", paste0(\"longitudinal_emm\", id, \".rds\")))\n  \n  return(emm)\n  \n}\n\nemm &lt;- clustermq::Q_rows(\n  select(analyses, id, analysis_data, formula, formula_name, family, correl, autocor, autocor_name),\n  fit_model,\n  const = list(save_individuals = FALSE),\n  n_jobs = nrow(analyses),\n  pkgs = c(\"brms\", \"emmeans\", \"tidyr\", \"dplyr\", \"purrr\", \"posterior\"),\n  template = list(\n    walltime = 120,\n    job_name = \"longitudinal_child\",\n    log_file = here(\"reports\", \"longitudinal_child_%I.log\"),\n    memory = 3000,\n    cores = 4\n  ),\n  job_size = 1\n)\n\nsaveRDS(emm, file = here(\"reports\", \"longitudinal_fits.rds\"))\nsaveRDS(analyses, file = here(\"reports\", \"longitudinal_analyses.rds\"))\n\nanalyses$emm &lt;- emm\n\n\n\n\n8.4.2 PASI change from baseline: EMMs by visit\n\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\n\n8.4.3 PASI change from baseline: EMM contrasts with placebo by visit\n\ncontrast_results &lt;- analyses %&gt;%\n  select(formula_name, autocor_name, emm) %&gt;%\n  unnest(emm) %&gt;%\n  dplyr::filter(!is.na(contrast)) %&gt;%\n  mutate(formula_name = factor(formula_name, unique(formula_name)),\n         autocor_name = factor(autocor_name, unique(autocor_name)),\n         contrast = factor(contrast, unique(contrast))) %&gt;%\n  rename(\n    mean_model = formula_name,\n    corr_model = autocor_name\n  )\n\nblank &lt;- mutate(slice(group_by(contrast_results, mean_model, corr_model), 1), estimate = 0)\n\nggplot(\n  data = contrast_results,\n  mapping = aes(x = AVISIT, group = mean_model, color = mean_model,\n                y = median, ymin = q5, ymax = q95)\n) +\n  geom_pointrange(position = position_dodge(0.5)) +\n  geom_path(position = position_dodge(0.5)) +\n  labs(x = \"Visit\", y = \"PASI change from baseline\\nTreatment - Control\\nDifference in Estimated Marginal Means\") +\n  geom_blank(data = blank) +\n  facet_grid(. ~ corr_model, labeller = label_both) +\n  scale_x_discrete(labels = levels(pasi_data$AVISIT)) +\n  scale_color_discrete(\"Mean model\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n8.4.4 Estimates of Week 12 efficacy\n\ncontrast_results %&gt;%\n  dplyr::filter(AVISITN == 12,\n         !mean_model %in% c(\"linear\", \"quadratic\"),\n         corr_model %in% c(\"COSY\", \"none\")) %&gt;%\n  mutate(method = paste(\"mean model:\", mean_model, \"\\ncorrelation model:\", corr_model)) %&gt;%\n  ggplot(aes(x = method, y = median, ymin = q5, ymax = q95)) +\n  geom_pointrange(position = position_dodge(0.6)) +\n  labs(x = \"Modeling approach\",\n       y = \"PASI change from baseline\\nTreatment - Control\\nDifference in Estimated Marginal Means\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n8.4.5 Assessing model fit\nIn this section we illustrate the use of side-by-side posterior predictive checks to compare the fit of two models: one with a GP prior for the mean structure and one with a linear-in-time model for the mean, each with compound-symmetric autocorration models.\nThe checks are done across timepoint (# of weeks since baseline), by treatment group and quartile of the distribution of baseline PASI.\n\n# Gaussian process model -------------------------------------\ngp_formula &lt;- bf(\n  CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\ngp_prior &lt;- get_prior(\n  gp_formula,\n  data = analysis_data\n)\n\ngp_fit &lt;- brm(\n  gp_formula,\n  prior = gp_prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 36.0 seconds.\nChain 2 finished in 39.8 seconds.\nChain 3 finished in 36.8 seconds.\nChain 4 finished in 39.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 38.1 seconds.\nTotal execution time: 153.0 seconds.\n\n\nWarning: 10 of 4000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n# Linear mean model -------------------------------------\nlm_formula &lt;- bf(\n  CHG ~ BASE + TRT01P * AVISITN,\n  autocor = ~ cosy(time = AVISIT, gr = SUBJID),\n  family = gaussian(),\n  center = FALSE,\n  nl = FALSE\n)\n\nlm_prior &lt;- get_prior(\n  lm_formula,\n  data = analysis_data\n)\n\nlm_fit &lt;- brm(\n  lm_formula,\n  prior = lm_prior,\n    data = analysis_data,\n    seed = 46474576,\n    control = control_args,\n    refresh = 0\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 7.0 seconds.\nChain 2 finished in 7.7 seconds.\nChain 3 finished in 7.0 seconds.\nChain 4 finished in 7.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 7.2 seconds.\nTotal execution time: 29.4 seconds.\n\nbase_quartiles &lt;- pasi_data %&gt;%\n  select(SUBJID, BASE) %&gt;%\n  distinct() %&gt;%\n  pull(BASE) %&gt;%\n  quantile(c(0, 0.25, 0.5, 0.75, 1))\n\nquartile_center &lt;- (base_quartiles[1:4] + base_quartiles[2:5]) / 2\n\nfull_newdata &lt;- pasi_data %&gt;%\n  select(SUBJID, BASE) %&gt;%\n  distinct() %&gt;%\n  mutate(base_catn = as.numeric(cut(BASE, base_quartiles)),\n         base_cat = paste(\"Baseline PASI quartile\", base_catn),\n         BASE = quartile_center[base_catn])\n\npp_checks &lt;- lapply(\n  split(full_newdata, full_newdata$base_catn),\n  function(nd){\n    \n    p1 &lt;- brms::pp_check(gp_fit, type = \"ribbon_grouped\", group = \"TRT01P\", x = \"AVISITN\",\n                         newdata = inner_join(nd, select(pasi_data, -BASE), \"SUBJID\", multiple = \"all\"),\n                         y_draw = \"points\") +\n      labs(x = \"Weeks post baseline\",\n           y = \"Week 12 PASI change from baseline\",\n           title = paste(\"Quartile\", unique(nd$base_catn), \"of Baseline PASI\")) +\n      theme(legend.position = \"bottom\") +\n      ylim(-50, 30)\n    \n    p2 &lt;- brms::pp_check(lm_fit, type = \"ribbon_grouped\", group = \"TRT01P\", x = \"AVISITN\",\n                         newdata = inner_join(nd, select(pasi_data, -BASE), \"SUBJID\", multiple = \"all\"),\n                         y_draw = \"points\") +\n      labs(x = \"Weeks post baseline\",\n           y = \"Week 12 PASI change from baseline\",\n           title = paste(\"Quartile\", unique(nd$base_catn), \"of Baseline PASI\")) +\n      theme(legend.position = \"bottom\") +\n      ylim(-50, 30)\n    \n    list(\n      gp = p1,\n      lm = p2\n    )\n    \n  }\n)\n\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\n\n\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\nUsing all posterior draws for ppc type 'ribbon_grouped' by default.\n\n\nNow we visualize the posterior predictive distribution against the observed data, first for the Gaussian process model:\n\n\nCode\npp_checks[[1]]$gp / pp_checks[[2]]$gp / pp_checks[[3]]$gp / pp_checks[[4]]$gp\n\n\n\n\n\n\n\n\n\nand next for the linear-in-time model:\n\n\nCode\npp_checks[[1]]$lm / pp_checks[[2]]$lm / pp_checks[[3]]$lm / pp_checks[[4]]$lm\n\n\n\n\n\n\n\n\n\nAnother useful visualization for assessing model involves approximate leave-one-out (LOO) cross-validation techniques.\nOne can numerically compare the approximate expected log predictive density (ELPD) for holdout observations using loo_compare.\n\nloo_gp &lt;- loo(gp_fit)\nloo_lm &lt;- loo(lm_fit)\nloo_compare(loo_gp, loo_lm)\n\n       elpd_diff se_diff\ngp_fit   0.0       0.0  \nlm_fit -20.8       5.2  \n\n\nThis suggests the Gaussian-process based model has a superior model fit.\nTo visualize the predictive densities versus observed values, pp_check() with type \"loo_pit\". In these visualizations, the observed outcomes are compared with their respective leave-one-out predictive distributions using the probability integral transformation (PIT). In an ideal model fit, the resulting PIT-transformed values would be uniformly distributed: i.e. the blue points and dashed lines would align with the distribution function of a Uniform(0,1) variable (solid line).\n\ngp_loo &lt;- pp_check(gp_fit, type = \"loo_pit\") + geom_abline(intercept = 0, slope = 1) + ggtitle(\"LOO PIT: GP model\")\nlm_loo &lt;- pp_check(lm_fit, type = \"loo_pit\") + geom_abline(intercept = 0, slope = 1) + ggtitle(\"LOO PIT: linear-in-time model\")\ngp_loo + lm_loo",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02g_longitudinal.html#exercises",
    "href": "src/02g_longitudinal.html#exercises",
    "title": "8  Longitudinal data",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises\n\nUsing brms, fit a cross-sectional (non longitudinal) logistic regression model to the binary PASI 75 endpoint at Week 12. The only covariate term should be the treatment effect. Use the following analysis data (which uses only the Week 12 outcomes, and uses nonresponder imputation for any missing outcomes at Week 12). Use the emmeans package and function to estimate the marginal mean Week 12 response rates by treatment. Use emmeans::contrast to estimate the treatment-vs-control contrasts in response rates.\n\n\nadpasi &lt;- readr::read_csv(here(\"data\", \"longitudinal.csv\"), show_col_types = FALSE) %&gt;%\n  dplyr::filter(TRT01P %in% c(\"PBO\", \"TRT\")) %&gt;%\n  mutate(AVISIT = factor(AVISIT, paste(\"Week\", c(1, 2 * (1:6)))),\n         TRT01P = factor(TRT01P, c(\"PBO\", \"TRT\")))\n\nanalysis_data1 &lt;- filter(adpasi, PARAMCD == \"PSRS75\", NRFL, AVISIT == \"Week 12\")\n\n\n\nCode\n# solution\nfit1 &lt;- brm(AVAL ~ TRT01P, family = bernoulli(), data = analysis_data1,\n            silent = 2, refresh = 0)\nemm1 &lt;- emmeans(fit1, c(\"TRT01P\"), transform = \"response\")\nemm1\ncontrast(emm1, method = \"revpairwise\")\n\n\n\nFit a longitudinal model to the binary PASI 75 endpoint. Use a Gaussian process prior (stratified by treatment arm) for the mean across weeks and an AR(1) process to model autocorrelation in the residuals. Use the following analysis data (in which any missing assessments are not imputed). Use the emmeans package and function to estimate the marginal mean response rates by treatment and visit. Use emmeans::contrast to estimate the treatment-vs-control contrasts in response rates by visit. How does the inference for Week 12 response rate difference compare to the cross-sectional model fit from 1?\n\n\nanalysis_data2 &lt;- filter(adpasi, PARAMCD == \"PSRS75\", !MISSFL)\n\n\n\nCode\n# solution\nfit2 &lt;- brm(\n  bf(\n    AVAL ~ TRT01P + gp(AVISITN, by = TRT01P),\n    autocor = ~ cosy(time = AVISIT, gr = SUBJID)\n  ),\n  data = analysis_data2,\n  silent = 2,\n  refresh = 0\n)\nemm2 &lt;- emmeans(fit2, c(\"TRT01P\", \"AVISITN\"), cov.keep = c(\"TRT01P\", \"AVISITN\"), nesting = list(\"AVISTN\" = \"AVISIT\"), transform = \"response\")\nemm2\ncontrast(emm2, method = \"revpairwise\", by = \"AVISITN\")\n\n\n\nUse Leave-One-Out cross validation to compare the model fits to the observed Week 12 data. (Hint: use loo(fit, newdata = newdata) with the below choice of newdata, then use loo_compare). Which model has better predictive power?\n\n\nnewdata &lt;- filter(adpasi, PARAMCD == \"PSRS75\", !MISSFL, AVISIT == \"Week 12\")\n\n\n\nCode\n# solution\nloo1 &lt;- loo(fit1, newdata = newdata)\nloo2 &lt;- loo(fit2, newdata = newdata)\nloo_compare(loo1, loo2)\n\n\n\n(Advanced) In this exercise, we will use a model fit to the continuous PASI change from baseline to do inference on the binary 75 response rate. First, fit a GP-based model to the PASI endpoint using the following code:\n\n\npasi_fit &lt;- brm(\n  bf(\n    CHG ~ BASE + TRT01P + gp(AVISITN, by = TRT01P),\n    autocor = ~ cosy(time = AVISIT, gr = SUBJID)\n  ),\n  data = pasi_data,\n  family = gaussian()\n)\n\nNext, create two sets of covariate values that includes “counterfactuals” for each patient as if they received both treatment and control. The following code is convenient:\n\nx_treatment &lt;- mutate(filter(pasi_data, AVISIT == \"Week 12\"),\n                      TRT01P = factor(\"TRT\", levels(pasi_data$TRT01P)))\nx_control &lt;- mutate(filter(pasi_data, AVISIT == \"Week 12\"),\n                    TRT01P = factor(\"PBO\", levels(pasi_data$TRT01P)))\n\nNext, for each of x_treatment and x_control:\n\nuse posterior_predict using the newdata argument to sample from the posterior predictive distribution for PASI change from baseline at each level x_treatment and x_control, respectively. The result of posterior_predict whose columns are posterior predictions of PASI change from baseline for individual patients.\nDivide these predictions by the baseline PASI for the respective patients to obtain predictive draws for PASI % change from baseline.\nConvert the result to binary indicators of PASI % change from baseline being below \\(-75\\%\\).\nFor each MCMC iteration, compute the percentage of responders to obtain a vector of posterior draws for the marginal mean response rates for x_treatment and x_control, respectively.\nHow do the median and credible intervals for the marginal mean response rates compare to the results of emmeans from exercises 1 and 2?\nGraph the posterior density for the difference in marginal mean response rates for treatment minus control.\n\n\n\nCode\n# solution\n\n# predicted change from baseline\npasi_chg_treatment &lt;- posterior_predict(pasi_fit, newdata = x_treatment)\npasi_chg_control &lt;- posterior_predict(pasi_fit, newdata = x_control)\n\n# predicted % change from baseline\npasi_pchg_treatment &lt;- sweep(pasi_chg_treatment, 2, x_treatment$BASE, '/')\npasi_pchg_control &lt;- sweep(pasi_chg_control, 2, x_control$BASE, '/')\n\n# predicted PASI 75 response\npasi_rr_treatment &lt;- pasi_pchg_treatment &lt; -0.75\npasi_rr_control &lt;- pasi_pchg_control &lt; -0.75\n\n# marginal PASI 75 response rates\nmarginal_rr_treatment &lt;- rowMeans(pasi_rr_treatment)\nmarginal_rr_control &lt;- rowMeans(pasi_rr_control)\n\n# How do the median and credible intervals compare to emm1 and emm2?\napply(cbind(trt = marginal_rr_treatment,\n            ctrl = marginal_rr_control), 2, median)\ncoda::HPDinterval(coda::as.mcmc(cbind(trt = marginal_rr_treatment,\n                                      ctrl = marginal_rr_control)))\nemm1\nemm2\n\n# visualize posterior distribution for marginal mean response rates by treatment\nqplot(\n  x = cbind(marginal_rr_control, marginal_rr_treatment),\n  group = cbind(rep(\"control\", length(marginal_rr_treatment)),\n                rep(\"treatment\", length(marginal_rr_treatment))),\n  color = cbind(rep(\"control\", length(marginal_rr_treatment)),\n                rep(\"treatment\", length(marginal_rr_treatment))),\n  geom = \"density\"\n) + scale_color_discrete(NULL) + theme(legend.position = \"bottom\") +\n  labs(x = \"Marginal mean response rate\",\n       y = \"Posterior density\")\n\n# visualize posterior distribution for difference in response rates\nqplot(\n  x = marginal_rr_treatment - marginal_rr_control,\n  geom = \"density\"\n) + \n  labs(x = \"Difference in marginal mean response rate\",\n       y = \"Posterior density\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Longitudinal data</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html",
    "href": "src/02h_mmrm.html",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "",
    "text": "9.1 Background\nIn randomized controlled clinical trials efficacy variables are often measured at multiple points of time. This may be at visits to the trial site for assessments that require a patient to be in the investigator’s office, but could also be at a patient’s home (e.g. for a daily quality of life questionnaire). Multiple assessments over time are useful for a wide variety of reasons. Firstly, we get information about how the difference between treatment groups develops over time. I.e. about both the onset of action, as well as whether treatment effects disappear or decline at some point (e.g. after the discontinuation of treatment either by some patients during the treatment period or by all patients during a planned post-treatment follow-up period). Secondly, seeing consistent data over time provides additional evidence for the presence of an effect of an intervention. Thirdly, pre-treatment (baseline) assessment(s) of a variable are often used as a covariate in analyses, because this tends to reduce unexplained variability leading to smaller standard errors. Finally, data from post-baseline visits can help us deal with missing data or data that are not relevant for our estimand of interest.\nSometimes, we would be primarily interested in the treatment difference at one particular visit (e.g. the final visit at the end of the planned treatment period) or we might want to combine (e.g. average) treatment effects across multiple visits.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#data",
    "href": "src/02h_mmrm.html#data",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.2 Data",
    "text": "9.2 Data\nWe will simulate data from a hypothetical parallel group RCT that tests three doses of a drug (10, 20 or 40 mg once daily) compared with a placebo (0 mg once daily). The endpoint of interest is continuous and assessed at a baseline visit, as well as at 4 post-baseline visits (week 2, 4, 8 and 12). 200 patients are randomly assigned to each treatment group (approximately 50 patients per arm).\nWe simulate data with the following covariance matrix:\n\n# Correlation matrix between visits (baseline + 4 post-baseline visits)\ncorr_matrix &lt;- diag(5)\nrho &lt;- c(0.6, 0.48, 0.4, 0.375)\ncorr_matrix[1,2:5] &lt;- rho[1:4]\ncorr_matrix[2,3:5] &lt;- rho[1:3]\ncorr_matrix[3,4:5] &lt;- rho[1:2]\ncorr_matrix[4,5:5] &lt;- rho[1:1]\ncorr_matrix[lower.tri(corr_matrix)] &lt;- t(corr_matrix)[lower.tri(corr_matrix)]\n \n# Standard deviations by visit (baseline + 4 post-baseline visits)\nsds &lt;- sqrt(c(0.75, 0.8, 0.85, 0.95, 1.1))\n\ncov_matrix &lt;- diag(sds) %*% corr_matrix %*% diag(sds)\nprint(cov_matrix, digits=3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.750 0.465 0.383 0.338 0.341\n[2,] 0.465 0.800 0.495 0.418 0.375\n[3,] 0.383 0.495 0.850 0.539 0.464\n[4,] 0.338 0.418 0.539 0.950 0.613\n[5,] 0.341 0.375 0.464 0.613 1.100\n\n\nIn our simulation some patients stop treatment before the end of the trial and actually drop out of the study. We no longer follow them, because we are interested in a hypothetical estimand as if they had stayed on drug, which means we are no longer interested in values after treatment discontinuation.\n\n\nShow the code\n# Simulate from multivariate normal for control group \n# (before adding treatment effect later)\n# We simulate 1000 patients and then apply inclusion criteria and keep the\n# first 200 that meet them.\nN &lt;- 1000\nNf &lt;- 200\nif(use_small_N) {\n    Nf &lt;- 50\n}\neffect_course &lt;- function(dose, time, ed50=8, et50=3) {\n    0.9 * dose /(dose + ed50) * time^3 /(time^3 + et50^3)\n}\nzero &lt;- setNames(rep(0, 5), c(\"BASE\", paste0(\"visit\", 1:4)))\nsimulated_data &lt;- rmvnorm(n = N,\n                          mean = zero,\n                          sigma = cov_matrix) %&gt;%\n    # turn into tibble\n    as_tibble() %&gt;%\n    # Apply inclusion criteria and keep first Nf patients\n    filter(BASE&gt;0) %&gt;%\n    filter(row_number()&lt;=Nf) %&gt;%\n    # Assign subject ID, treatment group and create missing data\n    mutate(USUBJID = row_number(),\n           TRT01P = dqsample(x=c(0L, 10L, 20L, 40L), size=Nf, replace=T),\n           # Simulate dropouts\n           dropp2 = plogis(visit1-2),\n           dropp3 = plogis(visit2-2),\n           dropp4 = plogis(visit3-2),\n           dropv = case_when(runif(n=n())&lt;dropp2 ~ 2L,\n                             runif(n=n())&lt;dropp3 ~ 3L,\n                             runif(n=n())&lt;dropp4 ~ 4L,\n                             TRUE ~ 5L),\n           visit2 = ifelse(dropv&lt;=2L, NA_real_, visit2),\n           visit3 = ifelse(dropv&lt;=3L, NA_real_, visit3),\n           visit4 = ifelse(dropv&lt;=3L, NA_real_, visit4)) %&gt;%\n    dplyr::select(-dropp2, -dropp3, -dropp4, -dropv) %&gt;%\n    # Turn data into long-format\n    pivot_longer(cols=starts_with(\"visit\"), names_to = \"AVISIT\", values_to=\"AVAL\") %&gt;%\n    mutate(\n        # Assign visit days\n        ADY = case_when(AVISIT==\"visit1\" ~ 2L*7L,\n                        AVISIT==\"visit2\" ~ 4L*7L,\n                        AVISIT==\"visit3\" ~ 8L*7L,\n                        AVISIT==\"visit4\" ~ 12L*7L),\n        # Turn to factor with defined order of visits\n        AVISIT = factor(AVISIT, paste0(\"visit\", 1:4)),\n        # Assume rising treatment effect over time (half there by week 3) with an \n        # Emax dose response (ED50 = 5 mg)\n        AVAL = AVAL + effect_course(ADY/7, TRT01P),\n        # Change from baseline = value - baseline\n        CHG = AVAL - BASE,\n        TRT01P=factor(TRT01P)) %&gt;%\n    relocate(USUBJID, TRT01P, AVISIT, ADY, AVAL, CHG, BASE) %&gt;%\n    # Discard missing data\n    filter(!is.na(AVAL))\n\n\nThe first 10 rows of the simulated data set are:\n\n\n\n\n\n\n\n\n\n\nUSUBJID\nTRT01P\nAVISIT\nADY\nAVAL\nCHG\nBASE\n\n\n\n\n1\n1\n10\nvisit1\n14\n1.767\n0.885\n0.882\n\n\n2\n2\n40\nvisit1\n14\n0.744\n0.211\n0.532\n\n\n3\n2\n40\nvisit2\n28\n0.602\n0.070\n0.532\n\n\n4\n3\n0\nvisit1\n14\n1.178\n0.402\n0.776\n\n\n5\n3\n0\nvisit2\n28\n0.875\n0.099\n0.776\n\n\n6\n3\n0\nvisit3\n56\n−0.814\n−1.590\n0.776\n\n\n7\n3\n0\nvisit4\n84\n0.813\n0.037\n0.776\n\n\n8\n4\n10\nvisit1\n14\n0.836\n0.074\n0.761\n\n\n9\n4\n10\nvisit2\n28\n1.913\n1.152\n0.761\n\n\n10\n5\n40\nvisit1\n14\n2.757\n0.951\n1.806\n\n\n11..614\n\n\n\n\n\n\n\n\n\n615\n200\n40\nvisit4\n84\n1.462\n0.808\n0.654\n\n\n\n\n\n\n\n\nThe assumed true change from baseline relationship in this simulation is:\n\n\n\n\n\n\n\n\n\nThe entire simulated data set with simulated residual noise in a graphical representation looks like:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#model-description",
    "href": "src/02h_mmrm.html#model-description",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.3 Model description",
    "text": "9.3 Model description\n\n9.3.1 The Mixed Model for Repeated Measures (MMRM)\nThe Mixed Model for Repeated Measures (MMRM) is a very popular model for continuous endpoints assessed at multiple visits (or their change from a pre-treatment baseline value). Part of its popularity stems from the fact that it is a flexible model for an outcome measured at different visits that accounts for within patient correlation and can handle missing data without imputation (if you are interested in the right estimand). In particular, it was a major milestone for treating missing data and drop-outs more appropriately than via last-observation-carried-forward (LOCF), which lead to it being recommended by some as a default analysis approach for a hypothetical estimand. Another contributing factor to MMRM’s success in the pharmaceutical industry is that it can be fit using restricted maximum likelihood (REML) with standard software.\nA widely used default analysis is to have the following (fixed effects) model terms\n\nvisit as a factor,\ntreatment as a factor,\ntreatment by visit interaction,\nbaseline (pre-treatment) value of the continuous endpoint as a continuous covariate, and\nvisit by baseline value interaction.\n\nThese terms allow for a flexible time course in the control group, as well as different treatment effects at different visits on top of that. Additionally, adjustment for baseline values usually reduces standard errors in clinical trials. By allowing for a visit by baseline interaction we allow the relationship between the values at each visit and the baseline to differ. This reflects that the longer ago a baseline value was measured, the less it will typically be correlated with future measurements.\nWhere a MMRM differs from a linear model is that the residuals from different visits of the same patient are assumed to be correlated. We do not usually wish to restrict this correlation structure to be of a particular form (such as first-order autoregressive AR(1)), but to allow it to be of an “unstructured” form. This is preferable over simpler structures like first-order autoregressive AR(1), for example. For example, AR(1) assumes a constant correlation between any adjacent visits while raising this correlation to a power for more distant observations. This simple correlation structure may inappropriately model these distant observations for a single patient as almost fully independent resulting in inappropriately small standard errors.\n\n\n9.3.2 Formal specification of MMRM\nFormally, let us assume that there are \\(V\\) visits. We usually assume that the \\(V\\)-dimensional response \\(\\boldsymbol{Y}_i\\) for patient \\(i\\) satisfies \\[\\boldsymbol{Y}_i = \\boldsymbol{X}_i \\boldsymbol{\\beta} + \\boldsymbol{Z}_i \\boldsymbol{b}_i + \\boldsymbol{\\epsilon}_i,\\] where \\(\\boldsymbol{\\beta}\\) is a vector of regression coefficients for fixed effects, the \\(\\boldsymbol{b}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{D})\\) are random effects with the vector \\(\\boldsymbol{Z}_i\\) indicating which one applies for which component (each corresponding to a visit) of the response, and the \\(\\boldsymbol{\\epsilon}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\) are residual errors. Then, \\[\\boldsymbol{Y}_i \\sim \\text{MVN}(\\boldsymbol{X}_i \\boldsymbol{\\beta}, \\boldsymbol{V}_i),\\] where \\[\\boldsymbol{V}_i = \\boldsymbol{Z}_i \\boldsymbol{D} \\boldsymbol{Z}_i^T + \\boldsymbol{\\Sigma}.\\]\nWith the goal to avoid unnnecessary assumptions, the marginal covariance matrix \\(\\boldsymbol{V}_i\\) is usually kept “unstructured”. I.e. we usually want to avoid imposing any particular structure on the covariance matrix. However, an unstructured covariance matrix requires to inform potentially many parameters which grow exponentially fast in the number of visits \\(V\\). Modeling the covariance matrix as a structured correlation matrix and by visit residual error standard deviations may facilitate stabilizing fitting procedures.\n\n\n\n\n\n9.3.3 What estimand does MMRM address?\nMMRM can address a hypothetical estimand about a continuous variable at one or more visits “as if all patients had remained on treatment to the end of the planned treatment period”. This is done by applying MMRM to on-treatment data only and making the assumption that stopping treatment or leaving the study occurs at random conditional on the preceding observations and the covariates being included in the model (“missing at random” or MAR).\nOne key attractive feature is that MMRM estimates this estimand without us needing to directly impute any data. I.e. there is no need for multiple imputation and fitting the model to each imputation, instead we only need to fit MMRM to the on-treatment data under analysis once. However, note that if there are no post-baseline observations for a patient the model will exclude the patient from the analysis, which is a valid thing to do under the MAR assumption.\nNevertheless, it produces identical results as if we had created a large number of multiple imputations, ran the model and then combined the results.\nIf we wish to target different estimands or make different assumptions than implied by the MMRM, we would need to impute prior to fitting a MMRM. If we perform multiple imputation, fitting a MMRM becomes unnecessary, because when all patients have (non-missing) data at all visits that should enter the analysis (i.e. is directly assessing our estimand of interest), then fitting a MMRM (with both treatment group and all baseline covariates have an interaction with visit) is equivalent to separately fitting a linear regression model for each visit.\n\n\n9.3.4 Is MMRM only good for one variable across visits?\nThere is nothing that requires the separate observations being modeled by a MMRM to be measurements of the same variable across time. You can just as appropriately apply MMRM to multiple outcomes measured at the same time, or across multiple occasions. This can be helpful when missingness or data becoming irrelevant to the estimand of interest depends on multiple variables.\nAn example would be a diabetes study, where both fasting plasma glucose (FPG) and glycated hemoglobin (HbA1c) are measured at each visit, and rescue medication is initiated when either of the two is above some threshold. In order to estimate the hypothetical estimand for the HbA1c difference “as if no rescue medication had been taken”, you can jointly model FPG and HbA1c.\n\n\n9.3.5 Should we use the baseline measurement as a covariate or an extra observation?\nFor some patients all post-baseline assessments will be missing, additionally some patients may not have a baseline assessment. In this situation (unless we somehow impute these data), the MMRM model we described cannot include such a patient in the analysis.\nOne potential idea for dealing with this situation is to use the baseline assessment not as a covariate, but as an extra observation. I.e. there is an additional baseline record for each patient, but the baseline term, as well as the baseline by visit interaction terms are removed from the model.\nThis is a reasonable approach, if the residuals across visits are jointly multivariate normal, which we are already assuming for the post-baseline visits. However, this can be a problematic assumption to make when inclusion criteria are applied at baseline that lead to the residuals for the baseline assessment not following a normal distribution. Such an inclusion criterion could be on the variable under analysis itself, or it could be on a variable that is sufficiently strongly correlated with the analysis variable to deform its distribution.\n\n\n\n\n\n\n\n\n\n\n9.3.6 How is MMRM different from a mixed effects model with a random subject effect on the intercept?\nA mixed (random) effects model with a random subject effect on the intercept effectively assumes that the residuals for all visits are equally strongly correlated (“compound symmetric covariance structure”, see next subsection) and also that the standard deviation is the same across visits (although brms would let us avoid this by using distributional regression as discussed later).\n\n\n9.3.7 What covariance structure to use?\nThere are several options for the covariance structures we can use\n\nThe standard deviation will usually go up over time in RCTs. This occurs in part because the trial population was originally forced to be somewhat homogenous at baseline by the trial inclusion criteria. The further we move away from that point in time the more heterogenous outcomes will become across patients. Thus, we at least want to allow the variability to differ between visits.\nData from visits that are closer in time to each other tend to be more highly correlated. Thus, a compound symmetric covariance structure that makes all visits equally correlated will not be realistic, but may sometimes be a reasonable approximation.\nThe correlation between visits of a patient usually never drops to zero even if visits are months or years apart. Generally, assuming a correlation structure that substantially underestimates the correlation between some visits potentially does the most harm. In particular, an AR(1) correlation structure should in practice be avoided as it leads to exponentially fast diminishing correlations between visits.\nIt is most common to use MMRM with a completely unstructured covariance structure across visits and we often even allow it to differ by treatment group. This avoids making (potentially unnecessary) assumptions. It also ensures the equivalence between MMRM and a by-visit-analysis in the absence of missing data.\n\nAs an illustration of what covariance matrices look like in practice, here is the estimated covariance structure reported by Holzhauer et al. 2015 for the fasting plasma glucose (FPG) values from one treatment group of a diabetes trial.\n\n\n\n\n\n\n\n\n\nExample of a covariance matrix for fasting plasma glucose in diabetic patients\n\n\nVisit\nWeek 0\nWeek 4\nWeek 12\nWeek 16\nWeek 24\nWeek 32\nWeek 40\nWeek 52\n\n\n\n\nWeek 0\n7.68\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 4\n4.00\n6.60\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 12\n3.55\n4.51\n6.52\nNA\nNA\nNA\nNA\nNA\n\n\nWeek 16\n3.03\n3.83\n5.02\n6.90\nNA\nNA\nNA\nNA\n\n\nWeek 24\n3.32\n4.00\n4.29\n4.42\n6.20\nNA\nNA\nNA\n\n\nWeek 32\n3.06\n3.22\n3.72\n4.23\n4.45\n5.73\nNA\nNA\n\n\nWeek 40\n3.60\n3.74\n4.66\n5.10\n4.80\n5.30\n7.80\nNA\n\n\nWeek 52\n3.60\n3.44\n4.48\n4.53\n4.67\n5.16\n6.26\n8.51\n\n\n\n\n\n\n\n\nIn terms of longer term data, Holzhauer 2014 reported that in a RCT in pre-diabetic patients the correlation matrix for FPG could be well described by the numbers in the table below and that “during the first 3 years of the study, the variance appeared to be relatively constant around 0.68, but there was some indication of an increasing variability in years 3–6.” Unlike the numbers in diabetic patients, these numbers come from a model adjusting for the baseline FPG assessment.\nTo look at another endpoint, let us look at the covariance matrix for glycated hemoglobin A1c (HbA1c) trial in diabetes patients used in Holzhauer et al. 2015. Many patterns seem similar to FPG, but the correlation of adjacent visits is higher, which makes sense, because HbA1c is less subject to day-to-day variations than FPG.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#implementation",
    "href": "src/02h_mmrm.html#implementation",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.4 Implementation",
    "text": "9.4 Implementation\n\n9.4.1 Reference implementations using SAS and lme4\nWithin a frequentist framework, this model is usually estimated using Restricted Maximum Likelihood Estimation (REML). One popular way of fitting such a model is using SAS code similar to the following\n\nPROC MIXED DATA=simulated_data;\n  CLASS TRT01P AVISIT USUBJID;\n  MODEL CHG ~ TRT01P AVISIT BASE TRT01P*AVISIT AVISIT*BASE \n    / SOLUTION DDFM=KR ALPHA = 0.05;\n  REPEATED AVISIT / TYPE=UN SUBJECT = USUBJID R Rcorr GROUP=TRT01P;\n  LSMEANS TRT01P*AVISIT / DIFFS PDIFF CL OM E;\nRUN;\n\nNote that GROUP=TRT01P in the REPEATED statement specifies different covariance structures for each treatment groups. Importantly, DDFM=KR refers to using the method of Kenward and Roger for figuring out what degrees of freedom to use for inference with Student-t-distributions (or in other words to figure out how many independent observations the correlated observations across all our subjects are worth).\nAdditionally the OM option to the LSMEANS statement requests the by-treatment-group least squares means for each visit to be calculated for predicted population margins for the observed covariate distribution in the analysis dataset. On this occasion, it does not make a difference, because there are no categorical covariates in the model.\nIn R we can obtain a very similar frequentist fit for this model using the lme4 package using the following R code as explained in a R/PHARMA presentation on implementing MMRM in R by Daniel Sabanés Bové.\n\nlmer(data=simulated_data,\n     CHG ~ TRT01P + AVISIT + BASE + AVISIT*TRT01P + AVISIT*BASE + (0 + AVISIT | USUBJID),\n     control = lmerControl(check.nobs.vs.nRE = \"ignore\", optimizer=\"nlminbwrap\")) %&gt;%\n  summary()\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : unable to evaluate scaled gradient\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge: degenerate\nHessian with 1 negative eigenvalues\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: CHG ~ TRT01P + AVISIT + BASE + AVISIT * TRT01P + AVISIT * BASE +      (0 + AVISIT | USUBJID)\n   Data: simulated_data\nControl: lmerControl(check.nobs.vs.nRE = \"ignore\", optimizer = \"nlminbwrap\")\n\nREML criterion at convergence: 1357.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.10240 -0.45913  0.00105  0.48795  2.03059 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr          \n USUBJID  AVISITvisit1 0.2659   0.5156                 \n          AVISITvisit2 0.3765   0.6136   0.68          \n          AVISITvisit3 0.4937   0.7027   0.34 0.58     \n          AVISITvisit4 0.3802   0.6166   0.26 0.51 0.77\n Residual              0.2273   0.4768                 \nNumber of obs: 615, groups:  USUBJID, 200\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           -0.08426    0.13706  -0.615\nTRT01P10               0.24492    0.14386   1.703\nTRT01P20               0.29786    0.14819   2.010\nTRT01P40               0.51736    0.14829   3.489\nAVISITvisit2          -0.07721    0.16774  -0.460\nAVISITvisit3           0.07750    0.23229   0.334\nAVISITvisit4          -0.14634    0.22524  -0.650\nBASE                  -0.36436    0.09946  -3.664\nTRT01P10:AVISITvisit2  0.27258    0.18060   1.509\nTRT01P20:AVISITvisit2  0.22188    0.18280   1.214\nTRT01P40:AVISITvisit2 -0.17275    0.19101  -0.904\nTRT01P10:AVISITvisit3  0.05105    0.24370   0.209\nTRT01P20:AVISITvisit3  0.14378    0.24234   0.593\nTRT01P40:AVISITvisit3 -0.06732    0.25076  -0.268\nTRT01P10:AVISITvisit4  0.41930    0.23619   1.775\nTRT01P20:AVISITvisit4  0.28910    0.23543   1.228\nTRT01P40:AVISITvisit4  0.12510    0.24291   0.515\nAVISITvisit2:BASE      0.02082    0.12732   0.164\nAVISITvisit3:BASE     -0.07496    0.17874  -0.419\nAVISITvisit4:BASE      0.06482    0.17263   0.375\n\n\n\nCorrelation matrix not shown by default, as p = 20 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\noptimizer (nlminbwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n\n\nNote that the choice of the particular optimizer via control = lmerControl(optimizer=\"nlminbwrap\") was chosen based on trying different ones to resolve convergence warnings. This process, as well as other tricks (like scaling and centering covariates) are conveniently wrapped in the dedicated mmrm R package. For this reason the mmrm package is preferable and its usage is demonstrated below:\n\nmmrm_fit &lt;- mmrm(\n  formula = CHG ~ TRT01P + AVISIT + BASE + AVISIT:TRT01P + AVISIT:BASE + us(AVISIT | USUBJID),\n  data = simulated_data %&gt;%\n    mutate(USUBJID=factor(USUBJID)))\n# reml=TRUE default option used (not specifically requested)\n# e.g. cs(AVISIT | USUBJID) would request compound-symmetric covariance\n#   structure instead of unstructed (\"us\")\n# us(AVISIT | TRT01P / USUBJID) would request separate covariance matrices\n#   for each treatment gorup\n\nWhile we can of course summarize the model fit and regression coefficients via summary(mmrm_fit), this is often not the main output we are interested in. Instead, we often want for each visit the treatment differences or least-squares means by treatment group. For a MMRM this involves linear combinations of regression coefficients, potentially weighted according to the distribution of (categorical baseline covariates). One very convenient way of obtaining these inferences is with the emmeans R package, which we can use with many different types of models including brms models. To obtain LS-means by visit for each treatment group, we use the emmeans function:\n\nemm1 &lt;- emmeans(mmrm_fit, ~ TRT01P | AVISIT, \n                lmer.df=\"kenward-roger\", weights = \"proportional\")\nprint(emm1)\n\nAVISIT = visit1:\n TRT01P   emmean     SE  df lower.CL upper.CL\n 0      -0.33105 0.1107 195 -0.54933  -0.1128\n 10     -0.08614 0.0922 195 -0.26805   0.0958\n 20     -0.03320 0.0983 195 -0.22716   0.1608\n 40      0.18631 0.0994 195 -0.00981   0.3824\n\nAVISIT = visit2:\n TRT01P   emmean     SE  df lower.CL upper.CL\n 0      -0.39416 0.1334 163 -0.65763  -0.1307\n 10      0.12334 0.1107 163 -0.09525   0.3419\n 20      0.12558 0.1137 160 -0.09896   0.3501\n 40     -0.04956 0.1267 166 -0.29969   0.2006\n\nAVISIT = visit3:\n TRT01P   emmean     SE  df lower.CL upper.CL\n 0      -0.30432 0.1673 125 -0.63539   0.0267\n 10     -0.00835 0.1413 125 -0.28799   0.2713\n 20      0.13731 0.1392 123 -0.13831   0.4129\n 40      0.14572 0.1546 124 -0.16034   0.4518\n\nAVISIT = visit4:\n TRT01P   emmean     SE  df lower.CL upper.CL\n 0      -0.43349 0.1547 124 -0.73964  -0.1273\n 10      0.23072 0.1307 124 -0.02803   0.4895\n 20      0.15346 0.1286 123 -0.10106   0.4080\n 40      0.20896 0.1429 123 -0.07393   0.4918\n\nConfidence level used: 0.95 \n\n\nTo get the implied contrasts for each dose compared with the control we use the contrast function`:\n\ncontrasts1 &lt;- contrast(emm1, adjust=\"none\", method=\"trt.vs.ctrl\", ref=1)\nprint(contrasts1)\n\nAVISIT = visit1:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0    0.245 0.144 195   1.703  0.0903\n TRT01P20 - TRT01P0    0.298 0.148 195   2.010  0.0458\n TRT01P40 - TRT01P0    0.517 0.148 195   3.489  0.0006\n\nAVISIT = visit2:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0    0.517 0.173 163   2.984  0.0033\n TRT01P20 - TRT01P0    0.520 0.175 162   2.965  0.0035\n TRT01P40 - TRT01P0    0.345 0.184 165   1.875  0.0626\n\nAVISIT = visit3:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0    0.296 0.221 125   1.337  0.1836\n TRT01P20 - TRT01P0    0.442 0.218 124   2.022  0.0453\n TRT01P40 - TRT01P0    0.450 0.228 124   1.977  0.0503\n\nAVISIT = visit4:\n contrast           estimate    SE  df t.ratio p.value\n TRT01P10 - TRT01P0    0.664 0.205 124   3.243  0.0015\n TRT01P20 - TRT01P0    0.587 0.202 123   2.908  0.0043\n TRT01P40 - TRT01P0    0.642 0.210 124   3.053  0.0028\n\n\nThe pairwise comparisons showing nominal p-values only (due to adjust=\"none\"), but we could request e.g. adjust=\"bonferroni\" instead. When we specified method=\"trt.vs.ctrl\", we told the contrast function to use the first group as the control group by specifying its index via ref=1. Alternatively, we could have asked for all possible pairwise contrasts via contrast(..., method=\"pairwise\") (or pairs(...)).\nSometimes, we may want to average the treatment effects across weeks 8 and 12. E.g. if we are very sure that (almost) the full treatment effect will have set in by week 8. When doing so, it is useful to fit a model that estimates separate treatment effects for both visits and then to average them. In contrast to averaging outcomes across visits for individual patients first, this approach more coherently deals with missing data, intercurrent events and differing effects of covariates. emmeans also allows us to do that, we just need to specify our contrasts slightly more manually. Note that in this case we need to know the ordering of estimates in the output of the emmeans() function in order to specify the contrasts:\n\nemm1a &lt;- emmeans(mmrm_fit, ~ TRT01P:AVISIT, weights=\"proportional\")\ncontrast(emm1a, \n         method=list(\"40 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0,0.5, -0.5,0,0,0.5),\n                     \"20 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0.5,0, -0.5,0,0.5,0),\n                     \"10 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0.5,0,0, -0.5,0.5,0,0)),\n         adjust=\"none\")\n\n contrast      estimate    SE  df t.ratio p.value\n 40 vs Placebo    0.546 0.189 125   2.891  0.0045\n 20 vs Placebo    0.514 0.181 125   2.838  0.0053\n 10 vs Placebo    0.480 0.184 126   2.615  0.0100\n\n\nWith the release of brms 2.19.0 in March 2023 it became possible to fit equivalent models easily based on a Bayesian implementation of a MMRM with an unstructured covariance matrix as shown below.\n\n\n9.4.2 brms implementation\nThe code below shows how we specify a MMRM model in a very similar way to SAS and the lme4 approach.\n\n# Setup forward difference contrasts for changes between visits\ncontrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_model1 &lt;- bf(CHG ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT,\n                  autocor = ~unstr(time=AVISIT, gr=USUBJID),\n                  sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P)\n\n# If we manually specify priors by providing out own Stan code via the stanvars \n# option, we may want to use `center=FALSE` in `bf()`, otherwise not.\n\n# Explicitly specifying quite weak priors (given the variability in the data).\n# These priors are set considering that unity is assumed to be the\n# sampling standard deviation and that the outcome data is scaled such that\n# unity is a considerable change.\nmmrm_prior1 &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=b) +\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) + # 90% CrI spans 1/10 - 10\n    prior(normal(0,  log(2.0)/1.64), class=b, dpar=sigma) +         # 90% CrI spans  1/2 - 2x\n    prior(lkj_corr_cholesky(1), class=\"Lcortime\")\n\nbrmfit1 &lt;- brm(\n  formula=mmrm_model1,\n  prior= mmrm_prior1,\n  data = simulated_data,\n  seed=234235,\n  control = control_args,\n  refresh = 0\n)\n\nbrmfit1\n\nNote that instead of writing (0 + AVISIT | USUBJID) like in the lme4 code, the brms syntax resembles the mmrm package syntax. We specify an unstructured correlation structure over time for groups of observations (here with the same unique patient identifier USUBJID) using autocor=~unstr(time=AVISIT, gr=USUBJID).\n\nNote that using (0 + AVISIT | USUBJID) as in the lme4 turns out to not work well with brms. While conceptually this implies the correct correlation structure via an unstructured random effect, the model becomes overparametrized due to the residual error and random effect standard deviations becoming non-identifiable. Trying to avoid this e.g. by setting the uncorrelated residual standard deviation to a fixed value that is small relative to the total variability (e.g. prior(constant(0.1), sigma)) still results in very poor sampling of the posterior distribution (high Rhat values and very low effective sample size for the MCMC samples). The approach implemented by the unstr syntax uses instead a marginal approach avoiding the non-identifability of the conditional approach.\n\nThe model formula above specifies that the correlation structure is the same across all treatment groups. This is not an assumption we have to make and SAS provides the GROUP=TRT01P option in the REPEATED statement to avoid it. At the time of writing this chapter, brms, lme4 and the mmrm package do not have an option for allowing separate unstructed covariance matrices for the different treatment groups. However, brms offers more flexibility than the other two packages regarding the variation at different visits and treatment groups through “distributional regression” (i.e. we can specify a regression model for parameters of the distribution other than the location, in this case sigma).\n\nIf we just wanted a compound symmetric correlation matrix, we would just use (1 | USUBJID), because a simple random effect on the intercept induces an equal correlation between all visits.\n\nBy writing sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P we specify that we want heterogeneous standard deviations over time that may differ by treatment group (note that brms models the standard deviation rather than the variance), which we would typically assume. If we instead wanted to assume the same variance at each visit, we would simply omit this option. Note that while we did not specify it explicitly, we are using a \\(\\log\\)-link function for sigma so that a regression coefficient of \\(0.69\\) approximately corresponds to a doubling of the standard deviation. The user can explicitly define the link using the family argument of bf and set family=gaussian(link_sigma=\"log\") (or use other links if desired). Thus, if we wanted to set prior distributions for the treatment and the visit-by-treatment interaction terms, even a normal(0, 1) prior already allows for considerable a-priori variation in sigma between treatment groups and visits. A useful way for specifying a prior on a log scale is by defining the standard deviation of a normal distribution to be equal to \\(\\log(s)/\\Phi^{-1}(1/2+c/2)\\), which implies that the central credible interval \\(c\\) spans the range \\(1/s - s\\), i.e. \\(\\log(2)/1.64\\) corresponds to the central 90% credible interval ranging from 1/2x to 2x around the mean.    \nAs for the mmrm package, we can also easily obtain the equivalent of least-squares means with highest-posterior density (HPD) credible intervals for each treatment group by visit using emmeans.\n\nemm2 &lt;- emmeans(brmfit1, ~ TRT01P | AVISIT, weights=\"proportional\")\nemm2\n\nAVISIT = visit1:\n TRT01P  emmean lower.HPD upper.HPD\n 0      -0.3276  -0.52371   -0.1297\n 10     -0.0865  -0.25745    0.1001\n 20     -0.0346  -0.24943    0.1941\n 40      0.1835  -0.00788    0.3848\n\nAVISIT = visit2:\n TRT01P  emmean lower.HPD upper.HPD\n 0      -0.3790  -0.60531   -0.1305\n 10      0.1225  -0.12952    0.3609\n 20      0.1141  -0.10232    0.3464\n 40     -0.0563  -0.29188    0.1869\n\nAVISIT = visit3:\n TRT01P  emmean lower.HPD upper.HPD\n 0      -0.3126  -0.61408   -0.0215\n 10     -0.0136  -0.27038    0.2590\n 20      0.1300  -0.16042    0.3976\n 40      0.1431  -0.22516    0.4726\n\nAVISIT = visit4:\n TRT01P  emmean lower.HPD upper.HPD\n 0      -0.4260  -0.70728   -0.1735\n 10      0.2264  -0.08267    0.5232\n 20      0.1391  -0.10978    0.3750\n 40      0.1850  -0.11664    0.4572\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\nWe can also get the pairwise treatment comparisons with HPD credible intervals in this manner.\n\ncontrast(emm2, adjust=\"none\", method=\"trt.vs.ctrl\")\n\nAVISIT = visit1:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0    0.239  -0.02810     0.497\n TRT01P20 - TRT01P0    0.289  -0.01436     0.576\n TRT01P40 - TRT01P0    0.513   0.24406     0.795\n\nAVISIT = visit2:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0    0.499   0.17061     0.837\n TRT01P20 - TRT01P0    0.490   0.17114     0.810\n TRT01P40 - TRT01P0    0.325   0.00619     0.667\n\nAVISIT = visit3:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0    0.298  -0.08583     0.703\n TRT01P20 - TRT01P0    0.438   0.04671     0.867\n TRT01P40 - TRT01P0    0.449   0.04310     0.931\n\nAVISIT = visit4:\n contrast           estimate lower.HPD upper.HPD\n TRT01P10 - TRT01P0    0.654   0.25744     1.061\n TRT01P20 - TRT01P0    0.561   0.22743     0.942\n TRT01P40 - TRT01P0    0.609   0.21653     0.998\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n\n\nHPD and quantile based credible intervals may differ whenever the posterior is heavily skewed or not unimodal. The quantile based credible intervals are numerically simpler in their definition and can be estimated more robustly estimated via MCMC. Furthermore, the quantiles of a distribution are transformation invariant, whereas HPD intervals are not transformation invariant (this of interest for the case of generalized models with non-linear link functions). emmeans can report summaries (using the frequentist=TRUE option) based on a normal approximation resembling the frequentist estimate plus standard error approach.\nWhile the default HPD intervals reported from emmeans are a useful summary of the positerior, the respective quantile summaries can also be extracted by first converting with the as.mcmc conversion function the emmeans results into the respective posterior MCMC sample. This sample one can then conveniently summarized by using utilities from the posterior package:\n\n\nmc.emm2 &lt;- as.mcmc(emm2)\nsummarise_draws(mc.emm2, default_summary_measures()) \n\n# A tibble: 16 × 7\n   variable                   mean  median     sd    mad      q5     q95\n   &lt;chr&gt;                     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 TRT01P 0 AVISIT visit1  -0.326  -0.328  0.102  0.104  -0.492  -0.157 \n 2 TRT01P 10 AVISIT visit1 -0.0865 -0.0865 0.0918 0.0899 -0.239   0.0624\n 3 TRT01P 20 AVISIT visit1 -0.0340 -0.0346 0.112  0.110  -0.221   0.154 \n 4 TRT01P 40 AVISIT visit1  0.184   0.184  0.100  0.0979  0.0199  0.348 \n 5 TRT01P 0 AVISIT visit2  -0.379  -0.379  0.120  0.118  -0.577  -0.181 \n 6 TRT01P 10 AVISIT visit2  0.120   0.122  0.127  0.125  -0.0884  0.325 \n 7 TRT01P 20 AVISIT visit2  0.114   0.114  0.114  0.112  -0.0706  0.302 \n 8 TRT01P 40 AVISIT visit2 -0.0570 -0.0563 0.123  0.121  -0.254   0.147 \n 9 TRT01P 0 AVISIT visit3  -0.312  -0.313  0.153  0.156  -0.557  -0.0651\n10 TRT01P 10 AVISIT visit3 -0.0103 -0.0136 0.137  0.134  -0.231   0.218 \n11 TRT01P 20 AVISIT visit3  0.130   0.130  0.145  0.143  -0.105   0.369 \n12 TRT01P 40 AVISIT visit3  0.143   0.143  0.175  0.171  -0.142   0.429 \n13 TRT01P 0 AVISIT visit4  -0.424  -0.426  0.135  0.131  -0.644  -0.198 \n14 TRT01P 10 AVISIT visit4  0.225   0.226  0.153  0.150  -0.0307  0.469 \n15 TRT01P 20 AVISIT visit4  0.141   0.139  0.123  0.121  -0.0598  0.346 \n16 TRT01P 40 AVISIT visit4  0.185   0.185  0.147  0.142  -0.0538  0.430 \n\nmc.contrast.emm2 &lt;- as.mcmc(contrast(emm2, adjust=\"none\", method=\"trt.vs.ctrl\"))\nsummarise_draws(mc.contrast.emm2, default_summary_measures())\n\n# A tibble: 12 × 7\n   variable                                   mean median    sd   mad      q5   q95\n   &lt;chr&gt;                                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 contrast TRT01P10 - TRT01P0 AVISIT visit1 0.240  0.239 0.136 0.136  0.0106 0.459\n 2 contrast TRT01P20 - TRT01P0 AVISIT visit1 0.292  0.289 0.149 0.146  0.0408 0.533\n 3 contrast TRT01P40 - TRT01P0 AVISIT visit1 0.510  0.513 0.142 0.143  0.275  0.740\n 4 contrast TRT01P10 - TRT01P0 AVISIT visit2 0.499  0.499 0.173 0.167  0.220  0.786\n 5 contrast TRT01P20 - TRT01P0 AVISIT visit2 0.493  0.490 0.163 0.163  0.234  0.759\n 6 contrast TRT01P40 - TRT01P0 AVISIT visit2 0.322  0.325 0.170 0.170  0.0345 0.601\n 7 contrast TRT01P10 - TRT01P0 AVISIT visit3 0.301  0.298 0.202 0.202 -0.0205 0.635\n 8 contrast TRT01P20 - TRT01P0 AVISIT visit3 0.442  0.438 0.211 0.215  0.0986 0.784\n 9 contrast TRT01P40 - TRT01P0 AVISIT visit3 0.454  0.449 0.226 0.218  0.0885 0.836\n10 contrast TRT01P10 - TRT01P0 AVISIT visit4 0.649  0.654 0.206 0.207  0.306  0.979\n11 contrast TRT01P20 - TRT01P0 AVISIT visit4 0.565  0.561 0.184 0.186  0.267  0.871\n12 contrast TRT01P40 - TRT01P0 AVISIT visit4 0.609  0.609 0.200 0.191  0.280  0.943\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs with a model fit using the mmrm package, we can also obtain inference about the average treatment effect across weeks 8 and 12 using the emmeans package:\n\nemm2a &lt;- emmeans(brmfit1, ~ TRT01P:AVISIT, weights=\"proportional\")\ncontrast(emm2a, \n         method=list(\"40 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0,0.5, -0.5,0,0,0.5),\n                     \"20 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0,0.5,0, -0.5,0,0.5,0),\n                     \"10 vs Placebo\" = c(0,0,0,0, 0,0,0,0, -0.5,0.5,0,0, -0.5,0.5,0,0)))\n\n contrast      estimate lower.HPD upper.HPD\n 40 vs Placebo    0.526     0.180     0.898\n 20 vs Placebo    0.501     0.177     0.835\n 10 vs Placebo    0.477     0.143     0.828\n\nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\n\n\n\n\n\n\n\n\n\n\n9.4.3 Model parameterization and the importance of (at least weak) priors\nHere, we explicitly specified a proper prior distribution (but very weak) for each regression coefficient. This is important, because by default brms would only have put a prior distribution on intercept terms. If we instead assume improper flat prior distributions, we often run into issues with sampling from the model. Weakly informative priors ensure stable sampling while not influencing the posterior distribution in relevant ways. One common approach to define a weakly informative prior is by way of identifying the scale of the parameter in question, e.g. parameters on a logarithmic scale commonly do not vary over many magnitudes or a parameter on a logistic scale varies within \\(-3\\) to \\(3\\) provided that the effects are not extreme.\nIn which way we are comfortable to specify prior distributions may in part determine how we parameterize the model:\n\nWe chose to setup forward difference contrasts for changes between visits when specifying contrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif in combination with CHG ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT. I.e. the intercept term will refer to the expected change from baseline averaged across all visits, while there will be regression coefficients for the expected difference from visit 1 to 2, 2 to 3 and 3 to 4. We can check this using solve(cbind(1, contrasts(simulated_data$AVISIT))).\nNot only does it seem reasonable to specify prior information about such parameters, but this also induces a reasonable correlation between our prior beliefs for each visit. I.e. if values at one visit are higher, we would expect values at the surrounding visits to also be higher. This is illustrated by the very typical pattern seen in the weight loss over time in the EQUIP study (Allison et al. 2012) shown in the figure below. As is typical for clinical trial data, there are only small visit-to-visit changes in the mean outcome, which can be nicely represented by forward difference contrasts. Furthermore, also note that the forward differences parametrization does imply a correlation structure which resembles the fact that we observe patient longitudinally accross the subsequent visits.\nIf we had specified the same model code without setting up forward difference contrasts, the intercept would have stood for the expected change from baseline at the reference visit (by default visit 1), while the regression coefficients would have represented the difference in the expected change at all the remaining visits compared with the reference visit 1.\nAlternatively, we could have chosen the parameterization 0 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT as our main approach. In this cell means parametrization, we would need to provide prior distributions for each visit.\nIf we make the priors for each visit independent in the latter approach, the prior distribution will specify plausible values for the mean outcome at that visit, but will suggest that all sizes of visit-to-visit changes that keep values within the range of a-priori plausible values are equally plausible. This is typically not the case.\n\n\n\n\n\n\nChange in body weight over time in the EQUIP study (data digitized from published figure)\n\n\n\n\nWith very vague priors the differences between these approach will usually be negligible. If we are interested in specifying weakly informative or even informative prior distributions, one should choose the paramterization, in which it is easiest to express the available prior information. Other ways to set-up contrasts are illustrated in the case study on time-to-event data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.4 Meta-analytic combined (MAC) approach to historical data for MMRMs\n\nWARNING: There is very limited experience with a MAC approach for MMRMs and intuitions from MAC/MAP for a single control group parameter may not apply to high-dimensional settings, where many parameters are assumed to be exchangeable between studies.\n\nOne source of informative prior distributions is historical data. Let us assume that we have access to the individual patient data for the placebo groups of 10 historical studies.\n\n\nShow the code\nset.seed(3547844)\nN_studies_hist &lt;- 10L\nNf_hist &lt;- 500L\nif(use_small_N) {\n    Nf_hist &lt;- 50L\n}\nNf_per_study_hist &lt;- Nf_hist/N_studies_hist\nhistorical_studies &lt;- rmvnorm(n=10 * Nf_hist,\n                              mean = zero,\n                              sigma = cov_matrix) %&gt;%\n  as_tibble() %&gt;%\n  filter(BASE&gt;0) %&gt;%\n  filter(row_number()&lt;=Nf_hist) %&gt;%\n  mutate(STUDYID = rep(1L:N_studies_hist, each=Nf_per_study_hist ), # Create a study ID\n         # Add a random study effect\n         rseff = rep(rnorm(n=N_studies_hist, mean=0, sd=0.1), each=Nf_per_study_hist),\n         USUBJID = row_number(),\n         TRT01P=factor(0, levels=c(0, 10, 20, 40))) %&gt;%\n  pivot_longer(cols=starts_with(\"visit\"), names_to = \"AVISIT\", values_to=\"AVAL\") %&gt;%\n  mutate(\n    AVAL = AVAL + rseff,\n    ADY = case_when(AVISIT==\"visit1\" ~ 2L*7L,\n                    AVISIT==\"visit2\" ~ 4L*7L,\n                    AVISIT==\"visit3\" ~ 8L*7L,\n                    AVISIT==\"visit4\" ~ 12L*7L),\n    AVISIT = factor(AVISIT, paste0(\"visit\", 1:4)),\n    CHG = AVAL - BASE, \n    STUDYID = factor(STUDYID, levels=1L:N_studies_hist+1)) %&gt;%\n  dplyr::select(-rseff) %&gt;%\n  relocate(STUDYID, USUBJID, TRT01P, AVISIT, ADY, AVAL, CHG, BASE)\n\n\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_line()`).\n\n\n\n\n\n\n\n\n\nThere are two mathematically equivalent ways of using historical data: the meta-analytic predictive (MAP) and the meta-analytic combined (MAC) approaches. The first step of the MAP approach involves fitting a model to the historical data with model parameters allowed to vary hierarchically across trials (trial is a random effect). The posterior predictive distribution for the parameters of a new trial obtained from this model is then used as the prior distribution for the analysis of our new trial of interest. In the MAC approach, a single model is fit jointly to the historical and new data.\nHere, we use the MAC approach, because it is easier to implement when there are many model parameters that vary across trials. However, the brms models we specify in either case are almost identical. It is also useful to first fit the model to only the historical data, which we could then use as an .\nFor a MAP approach we would still use the same model structure as below and use the drop_unused_levels=FALSE option in brms in combination with coding STUDYID to have an additional 11th factor level (for our new study) and TRT01P with the treatment groups of the new study included in its factor levels. That way, the number and indexing of parameters inside the generated Stan code remains unchanged and brms already samples parameter values for a new 11th study. We can easily fit such a model only to the historical data in order to inspect the resulting predictive distribution, which tells us how much information about the parameters of a new study the historical data in combination with our specified prior distributions implies.\n\ncontrasts(historical_studies$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_mac_model &lt;- bf( CHG  ~ 1 + AVISIT + BASE + BASE:AVISIT + TRT01P + TRT01P:AVISIT + ( 1 + AVISIT + BASE + BASE:AVISIT | STUDYID),  \n                     autocor=~unstr(time=AVISIT, gr=USUBJID), \n                     # center = FALSE, # We would use this option, if we used a MAP approach\n                     sigma ~  1 + AVISIT + TRT01P + AVISIT:TRT01P + (1 + AVISIT + TRT01P + AVISIT:TRT01P | STUDYID))\n\n# Priors as set are based on the assumed prior unit standard deviation\n# (usd) of unity. For the random effect for the log of sigma the unit\n# standard deviation is about sqrt(2).\nmmrm_mac_prior &lt;- prior(normal(0, 2), class=Intercept) +\n    prior(normal(0, 1), class=b) +\n    prior(normal(0, 0.25), class=sd, coef=Intercept, group=STUDYID) + # moderate heterogeneity on the intercept (usd/4)\n    prior(normal(0, 0.125), class=sd, group=STUDYID) +                # smaller heterogeneity on regression coefficients (usd/8)\n\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) +\n    prior(normal(0, log(2.0)/1.64), class=b, dpar=sigma) +\n    prior(normal(0, sqrt(2)/4.0), class=sd, coef=Intercept, group=STUDYID, dpar=sigma) + # same heterogeneity logic \n    prior(normal(0, sqrt(2)/8.0), class=sd, group=STUDYID, dpar=sigma) +                 # as for main regression coefficients\n\n    prior(lkj_corr_cholesky(1), class=Lcortime) +\n    prior(lkj(2), class=cor, group=STUDYID)\n    \n\nhistfit &lt;- brm(\n    formula=mmrm_mac_model,\n    prior=mmrm_mac_prior,\n    data = historical_studies,\n    drop_unused_levels=FALSE,\n    seed = 234525,\n    control = control_args,\n    refresh = 0,\n)\n\nNote that our simulation code created a dataset with STUDYID column that labels the 10 different studies being simulated and in addition has an extra 11th factor level for a new study.\n\nlevels(historical_studies$STUDYID)\n\n [1] \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\"\n\n\nThis trick with the extra factor level (plus using the drop_unused_levels=FALSE option in brms) automatically creates samples from the predictive distribution for the parameters of a new hypothetical study. Without this trick we would have to sample from a multivariate normal distribution for each MCMC sample in order to get the correlation of predicted parameters right. Instead, we can now simply add the population-level parameter estimate plus the study specific parameter estimate for the hypothetical new (11th) study (for which we had no data). This will then already correctly reflect the correlation between the predictions for the different parameters.\n\npredictive_dist &lt;- histfit %&gt;%\n    as_draws_rvars(variable=c(\"^b_\", \"^r_STUDYID\"), regex=TRUE) %&gt;%\n    mutate_variables(b_Intercept = b_Intercept + r_STUDYID[11,\"Intercept\"],\n                     b_AVISITvisit2Mvisit1 = b_AVISITvisit2Mvisit1 + r_STUDYID[11,\"AVISITvisit2Mvisit1\"],\n                     b_AVISITvisit3Mvisit2 = b_AVISITvisit3Mvisit2 + r_STUDYID[11,\"AVISITvisit3Mvisit2\"],\n                     b_AVISITvisit4Mvisit3 = b_AVISITvisit4Mvisit3 + r_STUDYID[11,\"AVISITvisit4Mvisit3\"],\n                     b_BASE = b_BASE + r_STUDYID[11,\"BASE\"],\n                     `b_AVISITvisit2Mvisit1:BASE` = `b_AVISITvisit2Mvisit1:BASE` + r_STUDYID[11,\"AVISITvisit2Mvisit1:BASE\"],\n                     `b_AVISITvisit3Mvisit2:BASE` = `b_AVISITvisit3Mvisit2:BASE` + r_STUDYID[11,\"AVISITvisit3Mvisit2:BASE\"],\n                     `b_AVISITvisit4Mvisit3:BASE` = `b_AVISITvisit4Mvisit3:BASE` + r_STUDYID[11,\"AVISITvisit4Mvisit3:BASE\"]) %&gt;%\n    subset_draws(variable=c(\"^b_Intercept\", \"^b_AVISIT.*\", \"^b_BASE.*\"), regex=TRUE) %&gt;%\n    as_draws_df()\n\nWe could now explore and visualize the joint distribution of these predicted parameter values in order to get an idea for the prior distribution induced by the historical data.\n\n\n\n\n\n\n\n\n\nThe MAP approach has been very popular at Novartis, especially for proof of concept studies. To use it, we would now have to approximate the joint predictive distribution of the model parameters using a suitable multivariate distribution such as a multivariate Student-t distribution. Then, we would have to correctly add Stan code to the stanvars argument to the brm call in order to specify this prior distribution. We would also use center=FALSE to the brms-formula function bf() in order to make setting priors easier, which would otherwise get harder if we let brms center covariates.\nThus, implementing the MAP approach with brms is currently a quite advanced task. However, this will become a lot easier once support for it is available in the RBesT package.\n\nBelow is the code for fitting a MMRM to the historical and new trial data jointly.\n\ncombined_data &lt;- bind_rows(historical_studies %&gt;%\n                           mutate(historical=factor(1L, levels=0L:1L)),\n                           simulated_data %&gt;%\n                           mutate(STUDYID=factor(11, levels=1:11),\n                                  USUBJID=USUBJID+max(historical_studies$USUBJID),\n                                  historical=factor(0L, levels=0L:1L)))\n\ncontrasts(combined_data$AVISIT) &lt;- MASS::contr.sdif\n\nmacfit &lt;- brm(\n    formula = mmrm_mac_model,\n    prior = mmrm_mac_prior,\n    data = combined_data,       # now using the combined data\n    drop_unused_levels = FALSE, # Important, if we fit only to the historical data\n    seed = 234525,\n    control = control_args,\n    refresh = 0, iter=20, warmup=10\n)\n\nThis gives us the following inference about the treatment effects at each visit:          \n\nemmeans(macfit, ~ TRT01P | AVISIT, weights = \"proportional\") %&gt;%\n    contrast(., adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n9.4.5 Robust Meta-analytic combined (rMAC) approach to historical data for MMRMs\n\nWARNING: This section presents initial ideas for a robust MAC approach. The discussed technique is not well-understood in the MMRM setting with a larger number of parameters, about which borrowing of information is needed. The discussed approach is the subject of ongoing research and its operating characteristics are not well-understood. It is shared to illustrate features of brms and to gather feedback. Do not use without careful testing for your specific setting (e.g. via simulation).\n\nOne popular variant of the MAP approach is its robust rMAP version that adds a uninformative or weakly informative mixture component to the MAP prior. In this way the historical prior information can be discarded or donweighted when there is an apparent prior-data-conflict.\nA way of obtaining a type of robust MAC (rMAC) approach is to introduce an additional model parameter that indicates how much the parameters of the new data differ from the historical data and to assign this parameter a “slab-and-spike”-type prior. I.e. a prior distribution that is peaked at zero and heavy-tailed such as a double-exponential (DE) distribution (aka “Laplace distribution”).\n\nexpand_grid(mean=0, scale=c(0.125, 0.25, 0.5, 1), x=seq(-2.5, 2.5, 0.01)) %&gt;%\n    mutate(density = dasym_laplace(x, mean, scale),\n           scale=factor(scale)) %&gt;%\n    ggplot(aes(x=x, y=density, col=scale, label=scale)) +\n    geom_line(linewidth=2, alpha=0.7) +\n    ylab(\"DE(0, scale) density for various\\nscale parameter values\") +\n    directlabels::geom_dl(method=\"smart.grid\")\n\n\n\n\n\n\n\n\nHowever, while this approach is well understood for a single control group log-event-rate or logit-proportion, there is currently no experience with it for continuous data and multiple parameters. For example, it is not immediately obvious how to encode a prior belief that when outcomes at one visit are similar to historical data, we become more likely to believe this about other visits. Here we setup the scale of the DE distribution such that the tail probability beyond the unit standard deviation of unity is close to 5%. This will ensure that most mass of the prior is centered around 0 while very large deviations are allowed for by the long tail of the DE distribution. A more detailed analysis of this prior is subject to research.\nWith these caveats stated, here is how one can implement this model with brms:\n\nmmrm_rmac_model &lt;- bf( CHG  ~ 1 + AVISIT + BASE + historical + BASE:AVISIT + TRT01P + TRT01P:AVISIT + \n                         AVISIT:historical + BASE:historical + BASE:AVISIT:historical +\n                         ( 1 + AVISIT + BASE + BASE:AVISIT | STUDYID),  \n                     autocor=~unstr(time=AVISIT, gr=USUBJID), \n                     # center = FALSE, # We would use this option, if we used a MAP approach\n                     sigma ~  1 + AVISIT + (1 + AVISIT | STUDYID))\n\n# can be used to find out on instantiated parameters\n#get_prior(mmrm_rmac_model, combined_data)\n\nmmrm_rmac_prior &lt;- mmrm_mac_prior +\n    prior(double_exponential(0, 0.25), class=b, coef=historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit2Mvisit1:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit3Mvisit2:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit4Mvisit3:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit2Mvisit1:BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit3Mvisit2:BASE:historical1) +\n    prior(double_exponential(0, 0.125), class=b, coef=AVISITvisit4Mvisit3:BASE:historical1)\n\nrmacfit &lt;- brm(\n    formula=mmrm_rmac_model,\n    prior=mmrm_rmac_prior,\n    data = combined_data,\n    drop_unused_levels=FALSE,\n    seed = 234525,\n    control = control_args,\n    refresh = 0\n)\n\nWarning: Rows containing NAs were excluded from the model.\n\n\nThis gives us the following inference about the treatment effects at each visit:          \n\nemmeans(rmacfit, ~ TRT01P | AVISIT, weights = \"proportional\") %&gt;%\n    contrast(., adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n9.4.6 Going beyond the traditional MMRM: monotonic effects\nNow, let us look at some ways, in which we can add some advanced features of brms on top of a MMRM.\nWhat if we wanted to make our analysis more efficient by exploiting knowledge about the structure of the experiment? E.g. what if we wanted to assume a monotonic increase of efficacy across doses (for the treatment main effect) and assume the how this effect differs at each visit is also monotonly across doses?\n\ncontrasts(simulated_data$AVISIT) &lt;- MASS::contr.sdif\n\nmmrm_mo_model &lt;- bf( CHG ~ 1 + AVISIT + mo(TRT01P) + BASE + mo(TRT01P):AVISIT + BASE:AVISIT,\n                     autocor=~unstr(AVISIT, USUBJID), \n                     sigma ~1 + AVISIT + TRT01P + AVISIT:TRT01P)\n\n# use the same prior as before and use the default dirichlet(1) priors\n# of brms for the increase fractions of the monotonic effects. This\n# represents a a uniform prior over the fractions\nmmrm_mo_prior &lt;- mmrm_prior1\n\nbrmfit2 &lt;- brm(\n  formula = mmrm_mo_model,\n  prior = mmrm_mo_prior,\n  data = simulated_data %&gt;% mutate(TRT01P=ordered(TRT01P)),\n  control = control_args,\n  refresh = 0)\n\nThe syntax of emmeans would in this case still be very straightforward:\n\nemmeans(brmfit2, ~ TRT01P | AVISIT, weights=\"proportional\") %&gt;%\n    contrast(adjust=\"none\", method=\"trt.vs.ctrl\") %&gt;%\n    as_tibble() %&gt;%\n    ggplot(aes(x=AVISIT, y=estimate, ymin=lower.HPD, ymax=upper.HPD, color=contrast)) +\n    geom_hline(yintercept=0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    coord_flip() +\n    ylab(\"Treatment difference to placebo\") +\n    xlab(\"Visit\") +\n    scale_colour_manual(values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    theme(legend.position=\"bottom\") +\n    guides(color=guide_legend(nrow=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.7 Going beyond the traditional MMRM: non-linear functions\nAs an alternative to the assumptions we made regarding monotonic predictors, we can explicitly specify a functional form for how doses are related and how their effect develops over time. However, while we will assume a particular functional form for the treatment effect over time, we will keep how the control group is modeled as flexible as possible. Notice that we would want to set control = list(adapt_delta=0.999, max_treedepth=13), because we received warnings that there were divergent transitions (hence an adapt_delta closer to 1 than before) and poor sampling with many transitions hitting the maximum tree depth (hence max_treedepth increased from 10 to 13).\n\nmmrm_dr_model &lt;- bf( CHG ~ E0 + Emax * ndose^h/(ndose^h + ED50^h) * nweek^ht/(nweek^ht + ET50^ht),\n                     autocor=~unstr(AVISIT, USUBJID), \n                     sigma ~ 1 + AVISIT + TRT01P + AVISIT:TRT01P,\n                     nlf(h ~ exp(logh)), nlf(ED50 ~ exp(logED50)),\n                     nlf(ht ~ exp(loght)), nlf(ET50 ~ exp(logET50)),\n                     E0 ~ 1 + AVISIT + BASE + BASE:AVISIT,\n                     Emax ~ 1,\n                     logED50 ~ 1, logh ~ 1,\n                     logET50 ~ 1, loght ~ 1,\n                     nl=TRUE,\n                    family=gaussian())\n\nmmrm_dr_prior &lt;- prior(normal(0, 2), class=b, coef=Intercept, nlpar=E0) +\n    prior(normal(0,1), class=b, nlpar=E0) +\n    prior(normal(0, log(4.0)/1.64), nlpar=logh) +\n    prior(normal(0, log(4.0)/1.64), nlpar=loght) +\n    prior(normal(0, 1), nlpar=Emax) +\n    prior(normal(2.5, log(10.0)/1.64), nlpar=logED50) +\n    prior(normal(log(4), log(5)/1.64), nlpar=logET50) +\n    prior(normal(0, log(10.0)/1.64), class=Intercept, dpar=sigma) +\n    prior(normal(0, log(2.0)/1.64), class=b, dpar=sigma) +\n    prior(lkj_corr_cholesky(1), class=Lcortime)\n\nbrmfit3 &lt;- brm(\n  formula=mmrm_dr_model,\n  prior = mmrm_dr_prior,\n  data = simulated_data %&gt;%\n    mutate(nweek = ADY/7,\n           ndose = as.numeric(levels(TRT01P)[TRT01P])),\n  control = control_args,\n  refresh = 0\n)\n\nNote that here, it is hard to see how to make emmeans work. Instead, we use hypothesis to get treatment differences at each visit.\n\n\nShow the code\ncomparisons_for_sigemax &lt;- expand_grid(dose = c(10, 20, 40), week=c(2,4,8,12)) %&gt;%\n      mutate(`Hypothesis string` = paste0(\n        \"Emax_Intercept * \", dose, \"^exp(logh_Intercept)/(\", dose,\n        \"^exp(logh_Intercept) + exp(logED50_Intercept)^exp(logh_Intercept)) * \",\n        week, \"^exp(loght_Intercept)/(\",\n        week,\n        \"^exp(loght_Intercept) + exp(logET50_Intercept)^exp(loght_Intercept)) &lt; 0\"),\n        evaluation = map(`Hypothesis string`,\n                         \\(x) hypothesis(brmfit3, x)$hypothesis %&gt;%\n                           as_tibble())) %&gt;%\n      unnest(evaluation) %&gt;%\n      mutate(Comparison = case_when(week==2 ~ paste0(\"Visit 1: \", dose, \" vs. placebo\"),\n                                    week==4 ~ paste0(\"Visit 2: \", dose, \" vs. placebo\"),\n                                    week==8 ~ paste0(\"Visit 3: \", dose, \" vs. placebo\"),\n                                    TRUE ~ paste0(\"Visit 4: \", dose, \" vs. placebo\")),\n             Model = \"BMMRM (non-linear)\")\n\n# Plot the results\ncomparisons_for_sigemax %&gt;%\n    bind_rows(expand_grid(week=0, Estimate=0, dose=c(10, 20, 40))) %&gt;%\n    ggplot(aes(x=week, y=Estimate, ymin=CI.Lower, ymax=CI.Upper,\n               group=dose, col=factor(dose))) +\n    geom_hline(yintercept = 0) +\n    geom_point(position=position_dodge(width=0.2)) +\n    geom_errorbar(position=position_dodge(width=0.2)) +\n    geom_line(position=position_dodge(width=0.2)) +\n    theme(legend.position=\"right\") +\n    scale_colour_manual(\"Dose\", values=c(\"#377eb8\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\")[-1]) +\n    ylab(\"Change from baseline\") +\n    ggtitle(\"Treatment difference to placebo\")\n\n\n\n\n\n\n\n\n\nJust note that the string specifying the hypothesis become something complex and long like\n\ncomparisons_for_sigemax$`Hypothesis string`[1]\n\n[1] \"Emax_Intercept * 10^exp(logh_Intercept)/(10^exp(logh_Intercept) + exp(logED50_Intercept)^exp(logh_Intercept)) * 2^exp(loght_Intercept)/(2^exp(loght_Intercept) + exp(logET50_Intercept)^exp(loght_Intercept)) &lt; 0\"\n\n\nfor the difference to placebo for the 10 mg dose at week 2.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#results",
    "href": "src/02h_mmrm.html#results",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.5 Results",
    "text": "9.5 Results\nHere is an overview of the results with some the different models we tried.\n\n\nShow the code\nmap2_dfr(list(brmfit1, brmfit2, macfit, rmacfit),\n         c(\"BMMRM\", \"BMMRM (monotonic)\", \"BMMRM (MAC)\", \"BMMRM (rMAC)\"),\n         \\(x, y) emmeans(x, ~ TRT01P | AVISIT, weights=\"proportional\") %&gt;%\n           contrast(adjust=\"none\", method=\"trt.vs.ctrl\", ref=1) %&gt;%\n           as_tibble() %&gt;%\n           mutate(Model=y)) %&gt;%\n  bind_rows(comparisons_for_sigemax %&gt;%\n              rename(estimate=Estimate, lower.HPD=CI.Lower, upper.HPD=CI.Upper) %&gt;%\n              mutate(AVISIT = paste0(\"visit\", case_when(week==2~1,\n                                                        week==4~2,\n                                                        week==8~3,\n                                                        week==12~4)),\n                contrast = paste0(dose,\" - 0\"))) %&gt;%\n  bind_rows(contrasts1 %&gt;%\n    confint() %&gt;% \n    as_tibble() %&gt;%\n    rename(lower.HPD=lower.CL,\n           upper.HPD=upper.CL) %&gt;%\n      mutate(Model=\"Frequentist MMRM\")) %&gt;%\n  dplyr::select(Model, contrast, AVISIT, estimate, lower.HPD, upper.HPD) %&gt;%\n  mutate(Model = factor(Model,\n                        levels=c(\"Frequentist MMRM\",\n                                 \"BMMRM\", \n                                 \"BMMRM (MAC)\", \"BMMRM (rMAC)\",\n                                 \"BMMRM (monotonic)\", \"BMMRM (non-linear)\"))) %&gt;%\n  ggplot(aes(x=contrast, y=estimate,\n             ymin=lower.HPD, ymax=upper.HPD, col=Model)) +\n  geom_hline(yintercept=0) +\n  geom_point(position=position_dodge(width=0.5)) +\n  geom_errorbar(position=position_dodge(width=0.5)) +\n  theme(legend.position=\"bottom\") +\n      guides(color=guide_legend(ncol=2, byrow=TRUE)) +\n  coord_flip() +\n  scale_color_manual(values=c(\"black\", \"#1b9e77\",\n                              \"#7570b3\", \"#e7298a\", \"#e6ab02\", \"#d95f02\")) +\n  facet_wrap(~AVISIT)\n\n\n\n\n\n\n\n\n\nUsing a BMMRM in the particular example had without informative prior distributions no particular advantage over a MMRM fit with REML. It is possible that there would be some advantage to using BMMRM with vague priors with smaller sample sizes. However, using historical control group informtion with a MAC (or rMAC) approach narrows the credible intervals. Making stronger assumptions about the dose response relationship over time turns out to also have a substantial effect on the width of the credible intervals. Of course, we could also use both prior information about the control group and assumptions about the dose response relationship for the drug.\nUnsurprisingly, putting in more information either in terms of prior knowledge about the control group or in terms of the structure of the dose response relationship leads to narrower CrIs and also affects the point estimates.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#conclusion",
    "href": "src/02h_mmrm.html#conclusion",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nWe can easily fit Bayesian MMRM models using brms including using an unstructured covariance matrix for how the residuals are correlated. As in other cases, a benefit of brms is that we can combine this feature with the many other features available in brms to gain a lot of flexibility in our modelling. With this modeling flexibility we can evaluate varying assumptions which can lead to greater statistical efficiency in our estimation as demonstrated. Given the flexibility of brms, these assumptions can range from just monotonicity up to a full non-linear model for the shape of the dose-response curve.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#exercises",
    "href": "src/02h_mmrm.html#exercises",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\n9.7.1 Excercise 1\nAnalyze the following data from the ACTIVE RCT, which compared interventions focused on memory, reasoning and speed of processing with a control group. The data of the ACTIVE trial is available from the US National archive of Computerized Data on Aging and is used as an example for repeated measures on the Advanced Statistics using R website. The outcome variable of interest is the Hopkins Verbal Learning Test (HVLT) total score compared with time 1 (baseline, hvltt column) assessed immediately after training (hvltt2 column), 1 year later (hvltt3 column) and 2 years later (hvltt4 column). Consider how to structure your analysis dataset and covariates might be sensible to include in the model.\n\nactive &lt;- read_csv(\"https://advstats.psychstat.org/data/active.csv\", \n                   col_types = \"iiiiiiiiiiiiii\") %&gt;% \n  mutate(group = factor(group, levels=1:4, \n                        labels=c(\"control\", \"memory\", \"reasoning\", \"speed\"))) %&gt;%\n  relocate(id, group, hvltt, hvltt2, hvltt3, hvltt4)\n\nhead(active)\n\n\n\n9.7.2 Excercise 2\nTry changing the size of the simulated data so that there is 80 (or 90%) power at the one-sided 2.5%, 5% or 10% significance level. Do our priors become more influential as the sample size decreases (compared with a frequentist MMRM fit using the mmrm package)\n\nwhen you use the standard Bayesian MMRM and\nwhen using the MAC (or rMAC) approaches?",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02h_mmrm.html#references",
    "href": "src/02h_mmrm.html#references",
    "title": "9  Bayesian Mixed effects Model for Repeated Measures",
    "section": "References",
    "text": "References\n\n\nAllison, David B., Kishore M. Gadde, William Timothy Garvey, Craig A.\nPeterson, Michael L. Schwiers, Thomas Najarian, Peter Y. Tam, Barbara\nTroupin, and Wesley W. Day. 2012. “Controlled-Release\nPhentermine/Topiramate in Severely Obese Adults: A Randomized Controlled\nTrial (EQUIP).” Obesity 20 (2): 330–42. https://doi.org/10.1038/oby.2011.330.\n\n\nCombescure, C, DS Courvoisier, G Haller, and TV Perneger. 2012.\n“Meta-Analysis of Two-Arm Studies: Modeling the Intervention\nEffect from Survival Probabilities.” Statistical Methods in\nMedical Research 25 (2): 857–71. https://doi.org/10.1177/0962280212469716.\n\n\nDias, S., N. J. Welton, A. J. Sutton, and A. Ades. 2014.\n“NICE DSU Technical Support Document 2: A Generalised\nLinear Modelling Framework for Pairwise and Network Meta-Analysis of\nRandomised Controlled Trials.” Technical report. NICE\nDecision Support Unit.\n\n\nHasselblad, Vic. 1998. “Meta-Analysis of Multitreatment\nStudies.” Medical Decision Making 18 (1): 37–43. https://doi.org/10.1177/0272989x9801800110.\n\n\nJanjigian, Yelena Y., Kohei Shitara, Markus Moehler, Marcelo Garrido,\nPamela Salman, Lin Shen, Lucjan Wyrwicz, et al. 2021. “First-Line\nNivolumab Plus Chemotherapy Versus Chemotherapy Alone for Advanced\nGastric, Gastro-Oesophageal Junction, and Oesophageal Adenocarcinoma\n(CheckMate 649): A Randomised, Open-Label, Phase 3 Trial.”\nThe Lancet 398 (July): 27–40. https://doi.org/10.1016/S0140-6736(21)00797-2.\n\n\nLu, G., and A. E. Ades. 2004. “Combination of Direct and Indirect\nEvidence in Mixed Treatment Comparisons.” Statistics in\nMedicine 23 (20): 3105–24. https://doi.org/10.1002/sim.1875.\n\n\nPiepho, H. P., E. R. Williams, and L. V. Madden. 2012. “The Use of\nTwo-Way Linear Mixed Models in Multitreatment Meta-Analysis.”\nBiometrics 68 (4): 1269–77. https://doi.org/10.1111/j.1541-0420.2012.01786.x.\n\n\nWarren, Fiona C., Keith R. Abrams, and Alex J. Sutton. 2014.\n“Hierarchical Network Meta-Analysis Models to Address Sparsity of\nEvents and Differing Treatment Classifications with Regard to Adverse\nOutcomes.” Statistics in Medicine 33 (14): 2449–66. https://doi.org/10.1002/sim.6131.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Mixed effects Model for Repeated Measures</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html",
    "href": "src/02i_time_to_event.html",
    "title": "10  Time-to-event data",
    "section": "",
    "text": "10.1 Background\nThe outcome of interest in clinical trials is often the occurence of an event. Such events might be negative for patients such as a worsening of a disease, hospitalization or death. They might also be a positive such as clearing the blood of malaria parasites, or being released from hospital.\nWe use time-to-event (or survival) analysis when it is important to patients whether the event of interest occurs earlier or later. Additionally, time-to-event methods let us deal with the fact that not all patients will have the event of interest during a trial, which leads to censored observations.\nIn this case study we focus on an Oncology late phase trial evaluating a test treatment in combination with standard of care (SoC) using progression free survival as endpoint in the indication of gastric cancer. Due to geographic differences in the SoC, two different SoC treatments are considered as control treatments. One active treatment is tested in combination with each SoC and compared to the control treatment using only the SoC, which leads to four trial arms in total. From clinical experience with the two SoCs on expects a similar PFS for both therapies. In addition, it is expected that the treatment effect of the active drug is similar when combined with each SoC respectivley. It is therefore desirable to express this prior knowledge when setting up the model.\nBy way of aligning the parametrization of the model we can encode the prior knowledge on expected similarities (no difference between the two SoCs and consistent treatment effect). For example, defining the average treatment effect and the difference between the two treatment effects as parameters, one can place a prior with limited width on the difference parameter. This expresses the a-priori expectation that differences in the treatment effect per SoC are not too large. This results in efficency gains for the estimation of the average treatment effect while only partially pooling the data.\nAs furthermore historical data is available on each SoC treatment, its use may allow for un-equal randomization between active and control arms. However, the available literature data only reports outcomes for a treatment arm which lumps together data from the two SoC treatments. Since about 50% of these patients were treated with either SoC, the reported data can be considered to report the average effect of both SoCs. How this historical data can be included in the analysis is outlined below.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#sec:tte-model-brms",
    "href": "src/02i_time_to_event.html#sec:tte-model-brms",
    "title": "10  Time-to-event data",
    "section": "10.2 Modeling time-to-event data with brms",
    "text": "10.2 Modeling time-to-event data with brms\nModeling time to event data of clinical trials is oftentimes run with the semi-parametric Cox proportional hazards model. This model avoids the need to specify a hazard function \\(h(t)\\), which is the rate of events provided no event as happened yet and it crucially defines the probability density of the survival times. Using the assumption of proportional hazards allows to quantify effects of explanatory variables on the hazard and thereby one can quantify the difference between two treatment groups in a trial, for example. The hazard for a patient \\(i\\) with covariates \\(x_i\\) is often modelled as \\(h_i(t) =\n\\exp\\{x_i \\, \\beta\\} \\, h_0(t)\\). The baseline hazard function \\(h_0(t)\\) is not required in the estimation of the effects \\(\\beta\\) and the Cox model parameters are estimated using a partial likelihood approach. However, as the Bayesian framework is based on the (full) likelihood principle, the semi-parametric Cox proportional hazards model is not applicale. Nonetheless, brms does offer a variant of the Cox proportional hazards model, which is designed to result in numerically very similar results if used with non-informative priors when compared to the respective Cox proportional hazards model. Instead of marginalizing out the baseline hazard function \\(h_0(t)\\), the baseline hazard function is modelled using an almost non-parametric approach based on a parametric spline approximation to \\(h_0(t)\\). The estimated regression coefficients \\(\\beta\\) for the Cox brms family correspond to logarithmic hazard ratios.\nIn contrast to the semi-parametric Cox model, parametric modeling of time to event data assumes a functional form for the hazard function \\(h(t)\\) or equivalently for the probability density function \\(f(t)\\) of the event times at \\(t=T\\). Both functions are related to one another by the basic relationship \\(h(t) = \\frac{f(t)}{S(t)}\\), where \\(S(t) = P(T&gt;t) =\n\\int_t^\\infty f(u)\\, du\\) is the survivor function (probability that an event occurs past time \\(t\\)). In brms the convention is to define via the family argument to brm the probability density of the event times. A description of the parametrization for these densities are found in the vignette “Parametrization of Response Distributions in brms”.\nIn this case study the Weibull distribution is used as literature data suggested its appropiateness and a parametric modeling approach may lead to greater statistical efficiency. The Weibull probability density is parametrized in brms in terms of shape \\(\\alpha\\) and scale parameter \\(s\\). Instead of directly modeling the scale \\(s\\) (as done in many other time to event programs), brms models the mean of the Weibull distribution \\(\\mu\\). Therefore, the scale \\(s\\) is set equal to \\(s=\\frac{\\mu}{\\Gamma(1 + \\frac{1}{\\alpha})}\\) in the Weibull probability density\n\\[ f_{\\mbox{Weibull}}(t) = \\frac{\\alpha}{s} \\left( \\frac{t}{s}\n\\right)^{\\alpha-1} \\, \\exp\\left(-\\left(\\frac{t}{s}\\right)^\\alpha\\right).\\]\nIn this form, the model fulfills the property of an accelerated failure time (AFT) model whenever explanatory variables are introduced. This follows from considering the survivor function\n\\[S_{\\mbox{Weibull}}(t) =\n\\exp\\left(-\\left(\\frac{t}{s}\\right)^\\alpha\\right)\\]\nand modeling the mean \\(\\mu_i\\) linearly on the \\(\\log\\) scale as a function of covariates \\(x_i\\) for subject \\(i\\), \\(\\log(\\mu_i) = \\beta_0 + x_i' \\,\n\\beta\\). Defining as the reference covariate level \\(x_i = 0\\) motivates the definition of a reference survivor function \\(S_{\\mbox{Weibull},0}(t)\\) for which the linear predictor \\(\\log(\\mu_0)\\) is equal to the intercept \\(\\beta_0\\). As a consequence, the survivor function of any patient \\(i\\) is related to the reference survivor function by\n\\[ S_{i}(t) = S_{\\mbox{Weibull},0}\\left(\\frac{t}{\\exp(x_i' \\, \\beta)} \\right),\\]\nwhich is the defining property of AFT models. The regression coefficients \\(\\beta\\) are then interpretable as relative speedup/slowdown of the process progression. That is, an increased time scale (slowdown) leads to a delay of an event. Given that modeling the scale of the Weibull distribution is a common convention (instead of the mean \\(\\mu\\) as in brms), it is useful to recall that this merely means that we need to offset the intercept as estimated by brms with \\(-\\log\\Gamma\\left(1 + \\frac{1}{\\alpha} \\right)\\) in order to obtain the scale on the log scale, \\(\\log(s)\\) (which is the what the survival R package would report with its survreg routine). The survival R package does futhermore use a log-linear representation of the statistical problem leading to an estimation of the inverse shape parameter \\(\\sigma=\\frac{1}{\\alpha}\\).\nThe Weibull AFT model as estimated by brms can be converted into the respective proportional hazard model by transforming the scale \\(s\\) with the relation \\(s = \\lambda^{-\\frac{1}{\\alpha}}\\) to an alternative scale parameter \\(\\lambda\\). Doing so casts the hazard function into\n\\[h_{\\mbox{Weibull}}(t) = \\lambda \\, \\alpha \\, t^{\\alpha-1}.\\]\nWhen we now model \\(\\lambda\\) as a function of covariates \\(x_i\\) linearly on the \\(\\log\\) scale one arrives at\n\\[ h_i(t) = \\exp(x_i' \\, \\xi) \\, h_{\\mbox{Weibull},0}(t),\\]\nwhere \\(h_{\\mbox{Weibull},0}(t)\\) is defined by the reference covariate level \\(x_i=0\\) such that \\(\\lambda = \\exp(\\xi_0)\\) for \\(h_{\\mbox{Weibull},0}(t)\\). To now convert the AFT regression coefficients \\(\\beta\\) as estimated by brms to their respective proportional hazard coefficients \\(\\xi\\) one may just apply the transformation resulting in the relationship\n\\[\\xi = -\\alpha \\, \\beta,\\]\nwhich converts from logarithmic speedup/slowdown \\(\\beta\\) (AFT model) into logarithmic hazard ratios \\(\\xi\\) (proportional hazard model).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#data",
    "href": "src/02i_time_to_event.html#data",
    "title": "10  Time-to-event data",
    "section": "10.3 Data",
    "text": "10.3 Data\nWe demonstrate here the trial analysis using a fake data simulation of the trial design. As key modeling choices for the parametric modeling approach are motivated from the historical data we start with the presentation of the historical data set and then describe the details of the trial simulation.\n\n10.3.1 Historical data\nThe CheckMate 649 trial (Janjigian et al. 2021) included 789 gastric cancer patients which were treated with either chemoA or chemoB and their progression free survival (PFS) was reported. Both chemotherapies were used in a roughly 1:1 ratio such that we will consider the reported data as “average” between both chemo therapies (despite lack of randomization occured wrt to chemotherapy assignment).\nThe data of trial has here been reconstructed from the published survival curves:\n\n\n\n\n\n\n\n\n\nTo now evaluate if the Weibull probability density function is an appropiate choice for this data we consider if the non-parametric estimate of the survivor function is compatible with properties of Weibull distributed event times. Specifically, if we transform the survivor function \\(S_{\\mbox{Weibull}}(t)\\) of the Weibull probability density function with the complementary log-log transformation, we obtain that\n\\[ \\mbox{cloglog}(S_{\\mbox{Weibull}}(t)) = \\log( - \\log(S_{\\mbox{Weibull}}(t))) =\n-\\alpha \\, \\log(s)  + \\alpha \\, \\log(t)\\]\nholds. Therefore, we can visualize an estimate of the survivor function on a transformed scale as a function of \\(\\log(t)\\) and we should observe a straight line with slope \\(\\alpha\\) and intercept \\(-\\alpha \\, \\log(s)\\):\n\nkm &lt;- survfit(Surv(time, status) ~ 1, data=hdata2)\nggsurvplot(km, data=hdata2, fun = \"cloglog\")\n\n\n\n\n\n\n\n\nWhile the line is not perfectly straight over the entire follow-up, the assumption of a straight line within the time period including the bulk of all events falls in the range of a good approximation with a line (as can be seen from the numbers at risk of the plot above).\nAs an additional evaluation of the Weibull distributional form we model the CheckMate 649 data via brms and perform a posterior predictive check. The priors are specified as illustrated in the later section on the used priors. The result looks reasonable.\n\nmodel_weibull_hist &lt;- bf(time | cens(1-status) ~ 1, family=weibull())\n\nprior_weibull_hist &lt;- prior(normal(log(6), log(4)/1.64), class=Intercept) +\n    prior(gamma(3, 2.7), class=shape)\n\nfit_hist_checkmate &lt;- brm(model_weibull_hist,\n                          data=hdata2,\n                          prior=prior_weibull_hist,\n                          seed=234235,\n                          refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.3 seconds.\nChain 2 finished in 1.4 seconds.\nChain 3 finished in 1.4 seconds.\nChain 4 finished in 1.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.3 seconds.\nTotal execution time: 5.7 seconds.\n\nfit_hist_checkmate\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: time | cens(1 - status) ~ 1 \n   Data: hdata2 (Number of observations: 789) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.44      0.04     2.37     2.51 1.00     2794     2455\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.22      0.04     1.14     1.30 1.00     2865     2574\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWith the posterior we can now perform a posterior predictive check. We do so by drawing 100 samples of the model parameters from the posterior and draw for each sample a realization of the event times for all patients. Hence, for each sample one obtains a fake data set simulated according to the Weibull distribution. Finally, each fake data set is summarized by a Kaplan-Meier estimate and drawn in light color while the actually observed data is summarized in the same way and drawn using a thick line. If the fitted model is appropiate, then the observed Kaplan Meier estimate should look just like a random realization among the many fake data generated ones. However, we observe that over the entire follow-up time the observed data is leaving for late follow-up times the bulk of the fake data generated Kaplan Meier curves. This may suggest a deviation from the Weibull distribution or the possibility that time-varying covariates are helpful. Nonetheless, if one focusses on the follow-up time frame, which includes the majoritiy of events (up to 15 month), then the posterior predictive check does look reasonable. This is why we use the Weibull distribution without a time-varying covariate for simplicity.\n\np_full_fup &lt;- pp_check(fit_hist_checkmate, status_y=hdata2$status,\n             type=\"km_overlay\", ndraws=100) +\n    scale_y_continuous(breaks=seq(0,1,by=0.1)) +\n  xlab(\"Time [month]\") + coord_cartesian(xlim=c(0, 35))\np_sub_fup &lt;- p_full_fup + coord_cartesian(xlim=c(0, 15))\nggpubr::ggarrange(p_full_fup, p_sub_fup, common.legend = TRUE)\n\n\n\n\n\n\n\n\nNote that the above predictive check does not simulate the censoring process. Thus, the censoring process is ignored and non-informative censoring is thereby assumed.\n\n\n10.3.2 Trial simulation\nHere we consider a fake data simulation of the trial of interest evaluating a novel treatment of gastric cancer for which an established treatment is available already. The trial is a randomized trial to study the efficacy and safety of adding a drug CompoundX in combination with a monoclonal antibody mAb + standard of care (SoC). Two chemotherapy backbones, ChemoA and ChemoB were used as SoC. There are in total four treatment arms including two control groups, which are mAb+ChemoA (controlChemoA) and mAb+ChemoB (controlChemoB), and two corresponding active arms with the test treatment administrated in addition, that is CompoundX+mAb+ChemoA (activeChemoA), CompoundX+mAb+ChemoB (activeChemoB).\nSynthetic data were generated for the randomised study. We let the sample size of both active and control group be 100, respectively. The sample size has been chosen to ensure adequate trial power of 80%. Efficacy was assessed using progression-free survival (PFS) measured in units of months. For patients not experiencing a progression event at the time of the analysis, their event time will be right censored at the time of their last valid tumor assessment. For patients experiencing an event, their event time will be interval censored, with the upper limit of the interval being the study day on which the progression event is observed, and the lower limit being the day of the last preceding valid tumor assessment at which the patient’s progression-free status was confirmed. While interval censoring is a supported feature of brms, this case study will for simplicity use the actual event times and leave it to the reader to explore interval censoring with brms.\nThere are two historical datasets available. The first one with 400 patients on a control arm, mAb+ChemoA (controlChemoA), is simulated together with the randomised study. And the PFS data of 789 patients receiving first-line programmed cell death (PD-1) inhibitor+chemotherapy are available from a randomised, open-label, phase 3 trial, CheckMate 649 (Janjigian et al. 2021), and it will be introduced later. These data are highly relevant and we would like to integrate the historical information into our analysis.\nSimulated data will be generated using simsurv function via simulating survival times from the Weibull model using the proportional hazard form of the model. We assumed a 20% hazard rate (HR) reduction due to active treatment administration (CompoundX) to the active group in comparison to the control group and a 5% difference in HR for the two chemotherapy SoCs (ChemoB is 5% worse than ChemoA on the hazard scale). We also assumed that both of the historical data have a 2% worse outcome on the hazards scale and we represent the two studies with hist1 and hist2 respectively (hist1 for simulated historidal data and hist2 for CheckMate 649).\n\n\nShow the code\nset.seed(46767)\n# n per group\nn_grp &lt;- 100\nn_hist &lt;- 400\n# use month as time-unit\nrate_1 &lt;- 1 / 6\nrate_cens &lt;- 1 / 10\nbeta &lt;- c(trt=-0.2,        ## roughly 20% HR reduction\n          soc_alt=0.05,    ## alternative chemotherapy is 5% worse\n          hist1=0.02)      ## simulated historical data has a 2% worse outcome\n# covariates of simulated trial data\ncovs &lt;- data.frame(id = seq(1, 2*n_grp), trt = c(0L, 1L),\n                   soc_alt=rbinom(2*n_grp, 1L, 0.3), hist1=0L, hist2=0L)\n\n# covariates of historical data\nhcovs &lt;- data.frame(id = seq(2*n_grp+1, 2*n_grp+1 +n_hist - 1),\n                    trt = c(0), soc_alt=0, hist1=1L, hist2=0L)\nsimulate_trial &lt;- function(lambda, gamma, lambda_cens, x, betas) {\n    ## simulate censoring times, note that we do not simulate end of\n    ## trial with maxt for now. Also note the different parametrization of\n    ## simsurv which models log(shape) with a linear predictor.\n    cens &lt;- simsurv(lambdas = lambda_cens, gammas = 1, x=x)\n    events &lt;- simsurv(lambdas = lambda, gammas = gamma, x=x, betas=betas)\n    names(cens) &lt;- paste0(names(cens), \"_cens\")\n    bind_cols(events, select(cens, -id_cens), select(x, -id)) %&gt;%\n        rename(censtime=eventtime_cens) %&gt;%\n    mutate(event=1L*(eventtime &lt;= censtime),\n           y=if_else(event==1, eventtime, censtime),\n           status=NULL, status_cens=NULL) %&gt;%\n        relocate(id, y, event) %&gt;%\n        mutate(soc=factor(soc_alt, c(0, 1), c(\"ChemoA\", \"ChemoB\")),\n               trt_ind=trt,\n               trt=factor(trt_ind, c(0,1), c(\"control\", \"active\")),\n               arm=factor(paste0(c(\"control\", \"active\")[trt_ind + 1], soc),\n                          levels=c(\"activeChemoA\", \"controlChemoA\",\n                                   \"activeChemoB\", \"controlChemoB\")))\n}\n\n# using shape=1 for simulation (corresponding to exponential survival times)\nsim &lt;- simulate_trial(rate_1, 1, rate_cens, covs, beta)\nhdata1 &lt;- simulate_trial(rate_1, 1, rate_cens, hcovs, beta) %&gt;%\n  mutate(id=id+max(sim$id))\n\n\nWe can check how the simulated data looks like and its sample size.\n\ngt_preview(sim) %&gt;% fmt_number(where(is.double), decimals=1)\n\n\n\n\n\n\n\n\n\nid\ny\nevent\neventtime\ncenstime\ntrt\nsoc_alt\nhist1\nhist2\nsoc\ntrt_ind\narm\n\n\n\n\n1\n1\n7.7\n0\n12.3\n7.7\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n2\n2\n0.1\n0\n20.4\n0.1\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n3\n3\n4.7\n0\n10.3\n4.7\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n4\n4\n2.7\n0\n19.0\n2.7\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n5\n5\n3.6\n1\n3.6\n7.5\ncontrol\n0\n0\n0\nChemoA\n0\ncontrolChemoA\n\n\n6..199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n200\n200\n11.8\n0\n17.1\n11.8\nactive\n0\n0\n0\nChemoA\n1\nactiveChemoA\n\n\n\n\n\n\n\ntable(sim$arm)\n\n\n activeChemoA controlChemoA  activeChemoB controlChemoB \n           72            73            28            27 \n\ntable(hdata1$arm)\n\n\n activeChemoA controlChemoA  activeChemoB controlChemoB \n            0           400             0             0",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#models",
    "href": "src/02i_time_to_event.html#models",
    "title": "10  Time-to-event data",
    "section": "10.4 Models",
    "text": "10.4 Models\n\n10.4.1 Model specification\nSurvival data will be fitted by a Bayesian generalized linear model.\nThe survival function follows a Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(s\\),\n\\[T|\\alpha,s \\sim f_{\\mbox{Weibull}}(T).\\]\nA common shape parameter \\(\\alpha\\) is assumed for all treatment arms, with the scale parameter allowed to vary by treatment arm. In place of directly modeling the scale parameter \\(s\\), the mean of the Weibull distribution is modelled on the logarithmic scale such that the scale is set to \\(s = \\frac{\\mu}{\\Gamma(1 + \\frac{1}{\\alpha})}\\) and \\(\\log(\\mu)\\) is modelled as a linear function of the covariates. We use \\(\\lambda_{\\mbox{activeChemoA}}\\) to represent the logarithm of the mean survival time for patients receiving CompoundX+mAb+ChemoA (\\(\\lambda_{\\mbox{activeChemoA}} = \\log(\\mu_{\\mbox{activeChemoA}})\\)), and \\(\\lambda_{\\mbox{controlChemoA}}\\) for patients receiving mAb+ChemoA. We define \\(\\lambda_{\\mbox{activeChemoB}}\\) and \\(\\lambda_{\\mbox{controlChemoB}}\\) similarly for patients receiving the ChemoB chemotherapy backbone.\nWith 4 different patient groups we may define 4 parameters in total. The default parametrization in R for categorical variables is the treatment contrast representation. This representation delcares a reference category to form the overall intercept and defines differences to the overall intercept for each categorical level. As this does automatically define the parametrization of the model, we here define the contrasts in a customized manner. Doing so ensures that we control directly the exact parametrization of the model. This is important in this problem as we can then setup priors in a tailored manner. More details in defining custom contrasts are explained by Ben Bolker here.\nIn addition to the overall mean (global intercept)\n\\[\\gamma = \\frac{1}{4}( \\lambda_{\\mbox{activeChemoA}} +\n\\lambda_{\\mbox{controlChemoA}} + \\lambda_{\\mbox{activeChemoB}} +\n\\lambda_{\\mbox{controlChemoB}} ), \\]\nwe define for the four groups the following contrasts of interest:\n\nAverage difference between the active and control arms in each chemotherapy backbone:\n\n\\[\\delta_{\\mbox{EffectAvg}} = \\frac{1}{2} {\n(\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) + (\n\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}} ) } \\]\n\nHalf of the difference in treatment effect between the two chemotherapy backbones:\n\n\\[\\delta_{\\mbox{effect}} = \\frac{1}{2} { (\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) - ( \\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}}) }\\]\n\nDifference between the two control arms:\n\n\\[\\delta_{\\mbox{control}} = - \\lambda_{\\mbox{controlChemoA}}  +\n\\lambda_{\\mbox{controlChemoB}} \\]\nTo formalize these contrast definitions, we summarize the arms to parameter mapping as matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n$\\lambda_{\\mbox{activeChemoA}}$\n$\\lambda_{\\mbox{controlChemoA}}$\n$\\lambda_{\\mbox{activeChemoB}}$\n$\\lambda_{\\mbox{controlChemoB}}$\n\n\n\n\n\\(\\gamma\\)\n1/4\n1/4\n1/4\n1/4\n\n\n\\(\\delta_{\\mbox{EffectAvg}}\\)\n1/2\n-1/2\n1/2\n-1/2\n\n\n\\(\\delta_{\\mbox{effect}}\\)$\n1/2\n-1/2\n-1/2\n1/2\n\n\n\\(\\delta_{\\mbox{control}}\\)\n0\n-1\n0\n1\n\n\n\n\n\n\n\n\nIn this form it is straightforward to derive the definition of a parameter and how it relates to the strata of the data based on the parameter definitions. However, in order to define the design matrix, which maps parameters to data strata, we will need the inverse of the above matrix. By convention R requires for the specification of custom contrats, the matrix mapping parameters to the data means such that the above matrix is referred to as \\(C^{-1}\\) in the following.\nThe linear model for \\(\\log(\\mu)\\) is parameterised in terms of the defined contrasts with additional parameters allowing for heterogeneity between data from the two separate historical data strata. For the 4 different treatment arms the linear predictor is then given by:\n\\[\\begin{align}\n\\begin{bmatrix}\n\\lambda_{\\mbox{activeChemoA}} \\\\\n\\lambda_{\\mbox{controlChemoA}} \\\\\n\\lambda_{\\mbox{activeChemoB}} \\\\\n\\lambda_{\\mbox{controlChemoB}}\n\\end{bmatrix}\n&= C \\,\n\\begin{bmatrix}\n    \\gamma \\\\\n    \\delta_{\\mbox{EffectAvg}} \\\\\n    \\delta_{\\mbox{effect}} \\\\\n    \\delta_{\\mbox{control}}\n        \\end{bmatrix}\n+ I_{\\mbox{hist1}} \\,\n\\beta_{\\mbox{hist1}} + I_{\\mbox{hist2}} \\, \\beta_{\\mbox{hist2}}\n\\end{align}\\]\nThe indicators \\(I_{\\mbox{hist1}}\\) and \\(I_{\\mbox{hist2}}\\) represent that a patient was in the simulated historical stratum, or in the historical CheckMate 649 stratum, respectively. The corresponding parameters \\(\\beta_{\\mbox{hist1}}\\) and \\(\\beta_{\\mbox{hist2}}\\) capture differences in outcome between these two historical strata, and patients in our randomised comparison. In order to borrow strength from the historical data, the prior on these two regression coefficients are set reasonably narrow which in effect emulates a random effects meta-analysis.\nAs the historical data set reported in CheckMate 649 reported the outcome for the control treatment with a mixed population of 50% being on either SoC, we consider this data to report an average SoC effect. Basic algebra shows that \\[\\frac{1}{2} \\,\n(\\lambda_{\\mbox{activeChemoA}} + \\lambda_{\\mbox{activeChemoB}}) = \\gamma -\n\\frac{1}{2} \\, \\delta_{\\mbox{effect}}. \\] Thus, we may simply add an additional row to the matrix \\(C\\) such that the full matrix \\(C\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment arm\n$\\gamma$\n$\\delta_{\\mbox{EffectAvg}}$\n$\\delta_{\\mbox{effect}}$\n$\\delta_{\\mbox{control}}$\n\n\n\n\nactiveChemoA\n1\n1/2\n1\n-1/2\n\n\ncontrolChemoA\n1\n-1/2\n0\n-1/2\n\n\nactiveChemoB\n1\n1/2\n-1\n1/2\n\n\ncontrolChemoB\n1\n-1/2\n0\n1/2\n\n\ncontrolAverage\n1\n-1/2\n0\n0\n\n\n\n\n\n\n\n\n\n\n10.4.2 Priors\nPriors for each random variable are described in the following table.\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPrior\n\n\n\n\n\\(\\alpha\\)\n\\(\\Gamma(3, 2.7)\\)\n\n\n\\(\\gamma\\)\n\\(\\mbox{Normal}( \\log(\\frac{8}{\\log(2)}), \\frac{\\log(4)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{effectAvg}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(2)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{effect}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.25)}{1.64})\\)\n\n\n\\(\\delta_{\\mbox{control}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.25)}{1.64})\\)\n\n\n\\(\\beta_{\\mbox{hist1}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.8)}{1.64})\\)\n\n\n\\(\\beta_{\\mbox{hist2}}\\)\n\\(\\mbox{Normal}(0, \\frac{\\log(1.8)}{1.64})\\)\n\n\n\n\n\n\n\n\nThese priors parametrize the central 90% credible interval for the respective normal priors used. As the parameters are additive on the \\(\\log\\) scale, the standard deviation of \\(\\log(2)/\\Phi^{-1}(95\\%)\n\\approx \\log(2)/1.64\\) defines a 90% credible interval of \\([-\\log(2), \\log(2)]\\) and implies on the back-transformed scale a respective interval of \\([1/2, 2]\\), i.e. a doubling or a halving of the progression speed.\nThe common shape parameter, \\(\\alpha\\), is set to have median value of unity and it’s 90% credible interval is \\([0.3, 2.33]\\), which admits substantial deviation from unity, which is the case of exponential survival time distribution.\nThe prior for \\(\\gamma\\), the intercept, is set with a mean approximately corresponding to expected median PFS on anti-PD-1 with SoC, as estimated from CheckMate 649 (approx. 8 months). The width of the central 90% credible interval for the intercept prior \\(\\gamma\\) is set to admit for a 4 fold increase or decrease. A priori it is expected that the outcome for controlChemoAand controlChemoB will be similar (Janjijian et al, mPFS by chemotherapy not provided, but mOS was 14 months for both anti-PD1+Chemotherapy backbones (considered as similar to mAb+ChemoA andmAb+ChemoB in our simulated data), hence a prior for \\(\\delta_{\\mbox{control}}\\) is centered on 0 with a standard deviation corresponding to a 25% increase or decrease is chosen for the 90% credible interval. Similarly, the treatment benefit from adding CompoundX is expected to be similar for both arms, so the prior for \\(\\delta_{\\mbox{effect}}\\) is again centered on 0 with the prior admitting a 25% increase or decrease a priori in terms of the 90% central credible interval. Priors for \\(\\beta_{\\mbox{hist1}}\\) and \\(\\beta_{\\mbox{hist2}}\\) are centered at a mean of 0, corresponding to no difference in outcome for the two historical studies as compared to this trial’s randomised part, but with the same variance allowing substantial differences between the randomised study and historical data. The use of Student-t priors for these parameters also robustifies against possible data discordance between the randomised comparison and historical data.\nThese prior definitions provide an uninformative prior, that allows for a broad range of shapes for the survival curve.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#implementationresults",
    "href": "src/02i_time_to_event.html#implementationresults",
    "title": "10  Time-to-event data",
    "section": "10.5 Implementation/Results",
    "text": "10.5 Implementation/Results\n\n10.5.1 Construct customized coded contrasts\nFirst we define customized contrasts which define the parametrization of the model as defined in the Model sepcification section.\nThe mapping of data strata to parameters is:\n\ncc_inv &lt;- matrix(c(1/4,  1/4,  1/4,  1/4,\n                   1/2, -1/2,  1/2, -1/2,\n                   1/2, -1/2, -1/2,  1/2,\n                   0,   -1,    0,    1),\nnrow=4, ncol=4, byrow=TRUE,\ndimnames=list(contrast=c(\"intercept\", \"deltaEffectAvg\",\n                         \"deltaEffect\", \"deltaControl\"),\n              arm=c(\"activeChemoA\", \"controlChemoA\",\n                    \"activeChemoB\", \"controlChemoB\")))\nMASS::fractions(cc_inv)\n\n                arm\ncontrast         activeChemoA controlChemoA activeChemoB controlChemoB\n  intercept       1/4          1/4           1/4          1/4         \n  deltaEffectAvg  1/2         -1/2           1/2         -1/2         \n  deltaEffect     1/2         -1/2          -1/2          1/2         \n  deltaControl      0           -1             0            1         \n\n\nintercept (\\(\\gamma\\)) is the overall mean while deltaControl (\\(\\delta_{\\mbox{control}}\\)) is the difference between the two control arms.\nBy way of defining deltaEffectAvg (\\(\\delta_{\\mbox{EffectAvg}}\\)) we obtain the average treatment effect. Setting deltaEffect (\\(\\delta_{\\mbox{effect}}\\)) to half of the difference between the two treatment effects one arrives at the following symmetric relations to obtain the chemo therapy specific treatment effect:\n\\[\\delta_{\\mbox{EffectAvg}} = \\frac{1}{2} \\, [\n(\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) +\n(\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}})]\\]\n\\[\\delta_{\\mbox{effect}} = \\frac{1}{2} \\, [ (\\lambda_{\\mbox{activeChemoA}} - \\lambda_{\\mbox{controlChemoA}}) - (\\lambda_{\\mbox{activeChemoB}} - \\lambda_{\\mbox{controlChemoB}})]\\]\n\\[ \\Leftrightarrow \\delta_{\\mbox{effectChemoA}}  =\n\\delta_{\\mbox{effectAvg}} + \\delta_{\\mbox{effect}}\\]\n\\[ \\Leftrightarrow \\delta_{\\mbox{effectChemoB}}  = \\delta_{\\mbox{effectAvg}} - \\delta_{\\mbox{effect}}\\]\nR requires to define the inverse relationship, i.e. the mapping from parameters to data strata (as needed to define the design matrix). Therefore, the contrast matrix \\(C\\) is:\n\ncc &lt;- solve(cc_inv)\nMASS::fractions(cc)\n\n              intercept deltaEffectAvg deltaEffect deltaControl\nactiveChemoA     1       1/2              1        -1/2        \ncontrolChemoA    1      -1/2              0        -1/2        \nactiveChemoB     1       1/2             -1         1/2        \ncontrolChemoB    1      -1/2              0         1/2        \n\n\nIn order to setup custom contrasts of discrete explanatory variables like treatment arms, R requires to define the explanatory variable as factor, which has been defined accordingly in the simulation routine of this case study:\n\nlevels(sim$arm)\n\n[1] \"activeChemoA\"  \"controlChemoA\" \"activeChemoB\"  \"controlChemoB\"\n\n\nAs R sets up the overall intercept separatley, we need to drop the overall intercept definition from \\(C\\) and define the contrast of the factor variable to be equal to the matrix \\(C\\):\n\ncontrasts(sim$arm) &lt;- cc[,-1]\n\nThen to check if the contrasts work as expected, we can use a dummy fit to see if things get used correctly and the results here are consistent with our definitions:\n\nlfit1 &lt;- lm(y ~ 1 + arm, sim)\nsummary(lfit1)\n\n\nCall:\nlm(formula = y ~ 1 + arm, data = sim)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.506 -2.909 -1.461  1.261 17.064 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         4.5023     0.3349  13.444   &lt;2e-16 ***\narmdeltaEffectAvg   0.9641     0.6698   1.439    0.152    \narmdeltaEffect     -0.4516     0.6698  -0.674    0.501    \narmdeltaControl     0.6574     0.9526   0.690    0.491    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.229 on 196 degrees of freedom\nMultiple R-squared:  0.02433,   Adjusted R-squared:  0.009398 \nF-statistic: 1.629 on 3 and 196 DF,  p-value: 0.1839\n\n\nWe can see that above model specification for lm estimates the parameter coefficient for the three contrasts armdeltaEffectAvg (\\(\\delta_{\\mbox{EffectAvg}}\\)), armdeltaEffect (\\(\\delta_{\\mbox{effect}}\\)) and armdeltaControl (\\(\\delta_{\\mbox{control}}\\)).\n\n\n10.5.2 Model fit with brms\nNow we start with analyzing the simulated randomized study and specify the brms model using a Weibull distribution. In a first version, we do not include the literature historical data which requires the definition of a custom design matrix as will be explained below.\nFirst we define with a call to bf the model formula and model family. This defines the model’s likelihood and the parametrization.\n\nmodel_weibull &lt;- bf(y | cens(1-event) ~ 1 + arm, family=weibull())\n\nThe left hand side of the formula, y | cens(1-event) ~ ..., denotes with \\(y\\) the data being modeled - the time variable -, while cens is a response modifier added with a vertical bar| to the left hand side. Here, cens defines which data rows correspond to (right) censored observations. The censoring information is passed into cens by the variable 1-event. The right hand side defines the linear regressor formula used for the mean parameter of the model, which is in this case modeled by default with a \\(\\log\\) link function. The formula 1 + arm defines an overall intercept and the covariate arm (coded as factor in the data) is used with it’s custom constrasts as explanatory variable. Finally, the family argument specifies that the event and censoring times stored in \\(y\\) are distributed according to a weibull probability density distribution.\nWith the model (and data) being defined, we are left to specify the priors for the model. With the help of the get_prior function we can report which default priors and parameters were instantiated by brms and hence need an assignment of a prior.\n\ngt(get_prior(model_weibull, sim))\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\n\nHere we define the priors with standard deviations stored in variables, which are passed as stanvars to the brms model. Doing so allows to change the prior quickly, since the Stan model as generated by brms remains unchanged whenever the standard deviation of a prior is changed. A priori it is expected that the outcome for the two control groups will be similar, so the prior on deltaControl (\\(\\delta_{\\mbox{control}}\\)) is centered on \\(0\\). The same is a priori assumed for the deltaEffect (\\(\\delta_{\\mbox{effect}}\\)) parameter.\n\nmodel_weibull_prior &lt;- prior(normal(meanInter, log(4)/1.64),\n                             class=\"Intercept\") +\n  prior(normal(0, sdEffectAvg), coef=armdeltaEffectAvg) +\n  prior(normal(0, sdDeltaEffect), coef=armdeltaEffect) +\n  prior(normal(0, sdDeltaControl), coef=armdeltaControl) +\n  prior(gamma(3, 2.7), class=shape)\n\nThe prior for the intercept, \\(\\gamma\\), is set to a mean approximately corresponding to an expected median PFS on anti-PD-1 + SoC, as estimated from CheckMate 649 (here 8 month)\n\nprior_mean &lt;- stanvar(log(8/log(2)), \"meanInter\")\n\nThe standard deviations are setup with reference to a \\(90 \\%\\) credible interval. Hence, \\(log(2)/1.64\\) for the standard deviation of the prior for deltaEffectAvg (\\(\\delta_{\\mbox{effectAvg}}\\)) means that the event process can increase/decrease in progression speed by a factor of \\(2\\) at most.\n\nprior_sd &lt;- stanvar(log(2)/1.64, \"sdEffectAvg\") +\n  stanvar(log(1.25)/1.64, \"sdDeltaEffect\")+\n  stanvar(log(1.25)/1.64, \"sdDeltaControl\")\n\nNow the model and prior specifications are complete and we are ready to run the model in brms. First fit the model using the randomized study data\n\nfit_weibull &lt;- brm(model_weibull, data=sim, prior=model_weibull_prior,\n               stanvars=prior_sd + prior_mean,\n               seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.7 seconds.\n\n\nWhen looking at the model summary, it is preferable to use robust=TRUE to get the median estimate of the posterior rather than the mean (which would change due to transforms)\n\nsummary(fit_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm \n   Data: sim (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.16      0.12     1.98     2.38 1.00     3998     2848\narmdeltaEffectAvg     0.26      0.19    -0.05     0.58 1.00     3806     2833\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4514     3328\narmdeltaControl       0.06      0.12    -0.13     0.26 1.00     4265     3056\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     0.97      0.08     0.85     1.10 1.00     4014     3098\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSummarize posterior draws based on point estimates, estimation errors and quantiles. And we transform the estimates to the original scale\n\npost_sum &lt;- posterior_summary(fit_weibull, prob=c(0.05, 0.95), robust=TRUE)\nprint(exp(post_sum[,\"Estimate\"]), digits=2)\n\n        b_Intercept b_armdeltaEffectAvg    b_armdeltaEffect   b_armdeltaControl               shape           Intercept \n            8.6e+00             1.3e+00             1.0e+00             1.1e+00             2.6e+00             8.5e+00 \n             lprior                lp__ \n            1.0e+00            5.4e-140 \n\n\nThese estimates are the AFT regression coefficients such that the effect of treatment is to slowdown the time to an event and thus the treatment effect estimate \\(\\delta_{\\mbox{effectAvg}}\\) is positive. To convert to hazard ratios, one needs to multiply with the negative of the estimated shape parameter. Doing so for all 3 regression coefficients by using the rvar facility (provided from the posterior R package) yields:\n\npost_rv &lt;- as_draws_rvars(fit_weibull)\npost_rv_HR &lt;- with(post_rv, exp(-1 * shape * c(b_armdeltaEffectAvg, b_armdeltaEffect, b_armdeltaControl)))\nnames(post_rv_HR) &lt;- c(\"deltaEffectAvg\", \"deltaEffect\", \"deltaControl\")\nprint(summarize_draws(post_rv_HR), digits=2)\n\n# A tibble: 3 × 10\n  variable                    mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;                      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 post_rv_HR[deltaEffectAvg] 0.788  0.777 0.146 0.145 0.568  1.05  1.00    3872.    2876.\n2 post_rv_HR[deltaEffect]    1.00   0.995 0.104 0.103 0.839  1.18  1.00    4515.    3209.\n3 post_rv_HR[deltaControl]   0.950  0.947 0.110 0.107 0.777  1.13  1.00    4282.    2919.\n\n\nObtain posterior mean estimates (mean rate). Median of the posterior estimates indicated a roughly 20% increase in survival in activeChemoA compared to controlChemoA.\n\nnd &lt;- data.frame(y=0, event=0, arm=levels(sim$arm))\npost_mean &lt;- posterior_epred(fit_weibull, newdata=nd)\ncolnames(post_mean) &lt;- nd$arm\nquantile(post_mean[,\"activeChemoA\"] / post_mean[,\"controlChemoA\"],\n         probs=c(0.05, 0.5, 0.95))\n\n       5%       50%       95% \n0.9425351 1.3043160 1.8179356 \n\n\nNow extract posterior coefficients with a 90% credible interval\n\npost_est &lt;- fixef(fit_weibull, prob=c(0.05, 0.95), robust=TRUE)\ngt(as.data.frame(post_est)) %&gt;% fmt_number(everything(), decimals=2)\n\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ5\nQ95\n\n\n\n\n2.16\n0.12\n1.98\n2.38\n\n\n0.26\n0.19\n−0.05\n0.58\n\n\n0.01\n0.11\n−0.17\n0.18\n\n\n0.06\n0.12\n−0.13\n0.26\n\n\n\n\n\n\n\n\nDefine trial successfull if the deltaEffectAvg exceed 0 (which means the survival time is increased) and the result is not significant here.\n\npost_est[\"armdeltaEffectAvg\",\"Q5\"] &gt; 0\n\n[1] FALSE\n\n\n\n\n10.5.3 Incoporate simulated historical data\nSince we have the simulated historical data hist1 on one control arm, we can incorporate the information by adding the corresponding covariate hist1 (indicator variable being \\(1\\) for the historical study and \\(0\\) otherwise) when specifying the formula for meta-analytic-combined (MAC) analysis:\n\nmodel_mac_weibull &lt;- bf(y | cens(1-event) ~ 1 + arm + hist1, family=weibull())\n\nAnd get_prior function shows that in addition to the priors for the weibull model on randomised data, the co-data model needs one more prior on the historical data.\n\ngt(get_prior(model_mac_weibull, sim))\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\n\nb\nhist1\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\n\nWe use a student with 6 degrees of freedom for the regression coefficient for the historical data prior. Using a Student-t distribution robustifies against possible data discordance between the randomised study and the historical data, since very large deviations between the two data sets are admitted by the heavy tails of the prior. The prior is centered at a mean of \\(0\\), corresponding to no expected difference in the outcome for historical and current study.\n\nmodel_weibull_mac_prior &lt;- model_weibull_prior +\n  prior(student_t(6, 0, sdHist), coef=hist1)\n\nA variance allowing substantial differences, a \\(1.8\\) fold of increase/decrease, between the randomised study and historical data is set.\n\nprior_sdHist &lt;- stanvar(log(1.8)/1.64, \"sdHist\")\n\nNow we combine the simulated trial data and historical data to fit the co-data MAC model. Don’t forget to set contrasts of arm in the combined data to the contrast matrix.\n\ncomb_data &lt;- bind_rows(sim, hdata1)\ncontrasts(comb_data$arm) &lt;- cc[,-1]\nfit_mac_weibull &lt;- brm(model_mac_weibull, data=comb_data,\n                       prior=model_weibull_mac_prior,\n                   stanvars=prior_sd + prior_mean + prior_sdHist,\n                   seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.8 seconds.\nChain 2 finished in 1.8 seconds.\nChain 3 finished in 1.7 seconds.\nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.8 seconds.\nTotal execution time: 7.4 seconds.\n\n\nAnd we can compare the results from two models. Inclusion of historical data on controlChemoA affect the estimates of coefficients for overall mean and EffectAverage.\n\nsummary(fit_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm \n   Data: sim (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.16      0.12     1.98     2.38 1.00     3998     2848\narmdeltaEffectAvg     0.26      0.19    -0.05     0.58 1.00     3806     2833\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4514     3328\narmdeltaControl       0.06      0.12    -0.13     0.26 1.00     4265     3056\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     0.97      0.08     0.85     1.10 1.00     4014     3098\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit_mac_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm + hist1 \n   Data: comb_data (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.30 1.00     4909     3769\narmdeltaEffectAvg     0.29      0.18    -0.01     0.59 1.00     4051     2881\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4748     3261\narmdeltaControl       0.07      0.12    -0.13     0.27 1.00     4343     2769\nhist1                -0.20      0.14    -0.43     0.02 1.00     4039     2944\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.94     1.07 1.00     4712     3074\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNext, we check the informativeness of the historical data by only including the historical data. Note that we have to request that factor levels for which no realization is present in the data set must be kept, since only the controlChemoA arm is present in the historical data.\n\ncontrasts(hdata1$arm) &lt;- cc[,-1]\nfit_mac_prior_weibull &lt;- brm(model_mac_weibull, data=hdata1,\n                             prior=model_weibull_mac_prior,\n                             drop_unused_levels=FALSE,\n                         stanvars=prior_sd + prior_mean + prior_sdHist,\n                         seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.2 seconds.\nChain 2 finished in 1.1 seconds.\nChain 3 finished in 1.2 seconds.\nChain 4 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 4.9 seconds.\n\ngt(as.data.frame(fixef(fit_mac_prior_weibull, robust=TRUE))) %&gt;% fmt_number(everything(), decimals=2)\n\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\n1.73\n0.47\n0.76\n2.69\n\n\n0.01\n0.42\n−0.80\n0.85\n\n\n0.00\n0.14\n−0.28\n0.27\n\n\n0.00\n0.14\n−0.27\n0.27\n\n\n0.01\n0.38\n−0.86\n0.87\n\n\n\n\n\n\n\n\nWe see that we essentially sample the prior for most regression coefficients, but we do substantially reduce the standard error of the intercept estimate which is almost halved in comparison to the prior standard deviation of 0.85.\n\n\n10.5.4 Including historical data of average SoC\nSo far we have setup a custom parametrization allowing to define priors aligned with prior expectations (small difference of the treatment effect and small difference between SoC). However, the CheckMate 649 historical data reports the effect of an average SoC treatment. Thus, the data of CheckMate 649 is incompatible with the 4 defined categorical factor levels so far as it does not correspond to any of the 4 treatment arms in the current data set. However, as the custom factor contrasts and factor level definitions are translated by R into a design matrix, we can customize the created design matrix directly. This requires to setup the design matrix manually giving us the freedom to define the design matrix appropiatley for the CheckMate 649 data set.\nThere is an alternative way to use the customized contrast and setup analysis by using respective indicator flags, which is created using model.matrix. The model.matrix function is used internally by brms to create the design matrix for the regression problem\n\nXind &lt;- model.matrix(~ 1 + arm, comb_data)\ngt_preview(Xind)\n\n\n\n\n\n\n\n\n\n(Intercept)\narmdeltaEffectAvg\narmdeltaEffect\narmdeltaControl\n\n\n\n\n1\n1\n-0.5\n0\n-0.5\n\n\n2\n1\n0.5\n1\n-0.5\n\n\n3\n1\n-0.5\n0\n-0.5\n\n\n4\n1\n0.5\n1\n-0.5\n\n\n5\n1\n-0.5\n0\n-0.5\n\n\n6..599\n\n\n\n\n\n\n600\n1\n-0.5\n0\n-0.5\n\n\n\n\n\n\n\n\nBy adding the generated design matrix to the original data set, we can then simply delcare the model formula using these created indicator variables:\n\ncomb_data_ind &lt;- cbind(comb_data, Xind[,-1])\n\nThus we use those indicator variables in the fit as if these were continuous covariates which replaces the use of the categorical factor variable arm used in the first approach.\n\nmodel_mac_weibull_ind &lt;- bf(y | cens(1-event) ~ 1 + armdeltaEffectAvg +\n                            armdeltaEffect + armdeltaControl + hist1,\n                            family=weibull())\n\nAnd query what prior parameters need to be set\n\ngt(get_prior(model_mac_weibull_ind, comb_data_ind))\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaControl\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffect\n\n\n\n\n\n\ndefault\n\n\n\nb\narmdeltaEffectAvg\n\n\n\n\n\n\ndefault\n\n\n\nb\nhist1\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 1, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\ngamma(0.01, 0.01)\nshape\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\n\n\nFit the model using indicator flags with the same priors as before and compare with the previous one fitted with arm variable, the results are the same.\n\nmodel_weibull_mac_prior_ind &lt;- prior(normal(meanInter, log(4)/1.64),\n                                     class=\"Intercept\") +\n    prior(normal(0, sdEffectAvg), coef=armdeltaEffectAvg) +\n    prior(normal(0, sdDeltaEffect), coef=armdeltaEffect) +\n    prior(normal(0, sdDeltaControl), coef=armdeltaControl) +\n    prior(normal(0, sdHist), coef=hist1)\n\nfit_mac_weibull_ind &lt;- brm(model_mac_weibull_ind, data=comb_data_ind,\n                           prior=model_weibull_mac_prior_ind,\n                       stanvars=prior_sd + prior_mean + prior_sdHist,\n                       seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.8 seconds.\nChain 2 finished in 1.7 seconds.\nChain 3 finished in 1.8 seconds.\nChain 4 finished in 1.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 7.2 seconds.\n\nsummary(fit_mac_weibull, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + arm + hist1 \n   Data: comb_data (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.30 1.00     4909     3769\narmdeltaEffectAvg     0.29      0.18    -0.01     0.59 1.00     4051     2881\narmdeltaEffect        0.01      0.11    -0.17     0.18 1.00     4748     3261\narmdeltaControl       0.07      0.12    -0.13     0.27 1.00     4343     2769\nhist1                -0.20      0.14    -0.43     0.02 1.00     4039     2944\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.94     1.07 1.00     4712     3074\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit_mac_weibull_ind, prob=0.9, robust=TRUE)\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 \n   Data: comb_data_ind (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.96     2.29 1.00     5075     3553\narmdeltaEffectAvg     0.28      0.18     0.00     0.57 1.00     4184     3537\narmdeltaEffect        0.01      0.10    -0.17     0.17 1.00     4688     3177\narmdeltaControl       0.07      0.11    -0.12     0.25 1.00     5187     3251\nhist1                -0.19      0.14    -0.43     0.02 1.00     4334     3137\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.93     1.07 1.00     4680     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that the equality holds exactly for the entire posterior, since the very same problem (using the same random seed) has been fitted by brms:\n\npost_mac &lt;- as_draws_matrix(fit_mac_weibull)\npost_mac_ind &lt;- as_draws_matrix(fit_mac_weibull_ind)\nall(post_mac == post_mac_ind)\n\n[1] FALSE\n\n\nThe benefit of this approach is that we can then include external data corresponding to the controlAverage from the second historical study CheckMate 649, which is not one of the treatment groups but the average effect of control groups, \\(\\frac{1}{2} \\, (\\lambda_{\\mbox{controlChemoA}}+\\lambda_{\\mbox{controlChemoB}})\\).\nTo now find the indicators needed for the effect of controlAverage in the given parametrization so far, we can obtain this via simple algebra as:\n\ncc &lt;- MASS::fractions(solve(cc_inv))\ncc\n\n              intercept deltaEffectAvg deltaEffect deltaControl\nactiveChemoA     1       1/2              1        -1/2        \ncontrolChemoA    1      -1/2              0        -1/2        \nactiveChemoB     1       1/2             -1         1/2        \ncontrolChemoB    1      -1/2              0         1/2        \n\n0.5*(cc[\"controlChemoA\",] + cc[\"controlChemoB\",])\n\n     intercept deltaEffectAvg    deltaEffect   deltaControl \n             1           -1/2              0              0 \n\n\nHence we must set the deltaEffectAvg column to \\(-\\frac{1}{2}\\) and the others to \\(0\\) to get the indicator for controlAverage (the overall intercept is always added to the linear predictor as we define 1 as the first term of the model formula) in order to include the new historical data from CheckMate649.\n\nhdata2_ind &lt;- rename(hdata2, y=time, event=status) %&gt;%\n  mutate(hist1=0, hist2=1, armdeltaEffectAvg=-0.5,\n         armdeltaEffect=0, armdeltaControl=0)\nall_comb_data_ind &lt;- bind_rows(comb_data_ind, hdata2_ind)\n\nTo include the second historical data, we specify the formula and the priors as we did for one historical study.\n\nmodel_mac_weibull_ind_all &lt;- bf(y | cens(1-event) ~ 1 + armdeltaEffectAvg +\n                              armdeltaEffect + armdeltaControl + hist1+ hist2,\n                              family=weibull())\nmodel_weibull_mac_prior_all &lt;- model_weibull_mac_prior +\n  prior(normal(0, sdHist), coef=hist2)\n\nFit the model and compare:\n\nfit_all_mac_weibull_ind &lt;- brm(model_mac_weibull_ind_all, data=all_comb_data_ind,\n                               prior=model_weibull_mac_prior_all,\n                               stanvars=prior_sd + prior_mean + prior_sdHist,\n                               seed=43534, refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 6.3 seconds.\nChain 2 finished in 6.5 seconds.\nChain 3 finished in 6.3 seconds.\nChain 4 finished in 6.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 6.3 seconds.\nTotal execution time: 25.7 seconds.\n\nfit_mac_weibull_ind\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 \n   Data: comb_data_ind (Number of observations: 600) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.12      0.10     1.93     2.33 1.00     5075     3553\narmdeltaEffectAvg     0.29      0.17    -0.05     0.63 1.00     4184     3537\narmdeltaEffect        0.01      0.10    -0.20     0.21 1.00     4688     3177\narmdeltaControl       0.07      0.11    -0.16     0.29 1.00     5187     3251\nhist1                -0.20      0.14    -0.47     0.06 1.00     4334     3137\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.00      0.04     0.92     1.09 1.00     4680     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nfit_all_mac_weibull_ind\n\n Family: weibull \n  Links: mu = log; shape = identity \nFormula: y | cens(1 - event) ~ 1 + armdeltaEffectAvg + armdeltaEffect + armdeltaControl + hist1 + hist2 \n   Data: all_comb_data_ind (Number of observations: 1389) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             2.09      0.09     1.92     2.27 1.00     4002     3141\narmdeltaEffectAvg     0.23      0.16    -0.09     0.55 1.00     2743     2920\narmdeltaEffect        0.00      0.10    -0.19     0.20 1.00     3616     2884\narmdeltaControl       0.09      0.12    -0.14     0.32 1.00     3555     3039\nhist1                -0.23      0.13    -0.49     0.00 1.00     2287     2849\nhist2                 0.48      0.12     0.24     0.71 1.00     2108     1868\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.12      0.03     1.07     1.18 1.00     4034     3292\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#conclusion",
    "href": "src/02i_time_to_event.html#conclusion",
    "title": "10  Time-to-event data",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\nHere we showed how brms enabled us to model time-to-event data with censoring and incorporate historical data into the analysis. By use of custom contrasts we controlled the parametrization of the problem allowing for the specification of priors aligned with the prior knowledge. This is possible in R using the contrasts function to setup custom contrast matrices used to setup the design matrix of the model. Going beyond that we also discussed that in some situations the facilities in R are too limiting and a customized setup of the model design matrix allows for even greater flexibility. This allowed in this case study the inclusion of historical data which corresponds to the average effect of standard of care.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02i_time_to_event.html#exercises",
    "href": "src/02i_time_to_event.html#exercises",
    "title": "10  Time-to-event data",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\n\nIn brms various families can be used to model time-to-event data. Re-fit the model using exponential distribution and compare with the weibull results.\nCox regression is implemented in brms using M-splines, which is a very close approximation to the semi-parametric Cox model. Show that the estimates for the covariate effect we get from brms are quite similar to what we get from survival package. Also compare the estimate of the overall intercept which needs to account for an offset as detailled in section @ref(sec:tte-model-brms).\n\n\n\n\n\n\n\nJanjigian, Yelena Y., Kohei Shitara, Markus Moehler, Marcelo Garrido, Pamela Salman, Lin Shen, Lucjan Wyrwicz, et al. 2021. “First-Line Nivolumab Plus Chemotherapy Versus Chemotherapy Alone for Advanced Gastric, Gastro-Oesophageal Junction, and Oesophageal Adenocarcinoma (CheckMate 649): A Randomised, Open-Label, Phase 3 Trial.” The Lancet 398 (July): 27–40. https://doi.org/10.1016/S0140-6736(21)00797-2.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Time-to-event data</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html",
    "href": "src/02j_network_meta_analysis.html",
    "title": "11  Network meta-analysis",
    "section": "",
    "text": "11.0.1 Background: Why network meta-analysis?\nNetwork (or multi-treatment) meta-analysis (NMA) is a term for analyses that combine the results of studies that compared several treatments and in which not all studies included all comparisons (Lu and Ades 2004). This is a frequent situation, because new drugs are often only compared to a placebo or to one commonly used current standard of care, but not to all existing current treatment options. As a result, physicians that want to choose between two treatment options often find that there are no or very few studies directly comparing them. NMA tries to address this issue by combining evidence from direct comparisons within each trial with indirect chains of comparisons across multiple trials.\nNetwork meta-analyses have their limitations. One particular concern would be factors that modify the effects of treatments resulting in treatment by trial interactions, e.g. if pathogens developing resistance to some drugs over time, if the presence of some background therapies has a synergistic or antagonistic interaction with a drug, or if some drugs work better or worse depending on disease severity.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#data",
    "href": "src/02j_network_meta_analysis.html#data",
    "title": "11  Network meta-analysis",
    "section": "11.1 Data",
    "text": "11.1 Data\nWe will use smoking cessation data (Hasselblad 1998), which consists of the number of patients \\(r\\) amongst the total in an study arm \\(n\\) that manage to stop smoking. There are 24 studies, in which different combinations of 4 interventions (no contact, self-help, individual counseling and group counseling) were tested. The aim of a network meta-analysis here would be to compare these different treatment in terms of the odds of patients managing to stop smoking.\n\ntrt_levels &lt;- c(\"No intervention\", \"Self-help\",\n                \"Individual counselling\", \"Group counselling\")\n\nsmoking &lt;- multinma::smoking %&gt;%\n  mutate(study = factor(studyn),\n         trtc = factor(trtc, trt_levels))\n\nlevels(smoking$trtc) &lt;- gsub(\" \", \"_\", trt_levels)\nlevels(smoking$trtc) &lt;- gsub(\"-\", \"_\", levels(smoking$trtc))\n\ngt_preview(smoking)\n\n\n\n\n\n\n\n\n\nstudyn\ntrtn\ntrtc\nr\nn\nstudy\n\n\n\n\n1\n1\n1\nNo_intervention\n9\n140\n1\n\n\n2\n1\n3\nIndividual_counselling\n23\n140\n1\n\n\n3\n1\n4\nGroup_counselling\n10\n138\n1\n\n\n4\n2\n2\nSelf_help\n11\n78\n2\n\n\n5\n2\n3\nIndividual_counselling\n12\n85\n2\n\n\n6..49\n\n\n\n\n\n\n\n\n50\n24\n4\nGroup_counselling\n3\n26\n24\n\n\n\n\n\n\n\n\n\nsmoking %&gt;%\n  ggplot(aes(x=study, y=r/n, fill=trtc, group=trtc,\n             ymin=qbeta(p=0.025, shape1=r, shape2=1+n-r),\n             ymax=qbeta(p=0.975, shape1=r+1, shape2=n-r))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(position = \"dodge\", alpha=0.3) +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\nThe totality of evidence about the treatments and their relative efficacy is commonly known as the evidence network. A common graphical representation, shown below, visualizes this network of treatments (the nodes) along with edges indicating the presence or absence of direct evidence comparing pairs of treatments from randomized trials. Below, the thickness of the edges is determined by the number of trials comparing each pair of treatments.\n\nnetwork &lt;- multinma::set_agd_arm(\n  data = smoking,\n  study = study,\n  trt = trtc,\n  r = r,\n  n = n,\n  trt_ref = \"No_intervention\"\n)\n\nplot(network)",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#model-description",
    "href": "src/02j_network_meta_analysis.html#model-description",
    "title": "11  Network meta-analysis",
    "section": "11.2 Model description",
    "text": "11.2 Model description\nWe will first describe the arm-based (aka “two-way mixed model”) approach to NMA advocated by Piepho (Piepho, Williams, and Madden 2012). It has some advantages over more commonly used NMA approach that analyzes treatment contrasts (Lu and Ades 2004), in terms of ease of use and familiarity (the two-way linear mixed model is of course very familiar). The latter approach, however, is widely used in health technology assessments, for example by the UK NICE. The NICE decision support unit has published methodological guidance on how to use this method in practice (Dias et al. 2014).\nSuppose we had individual patient data for patients \\(k=1,\\ldots,N_{ij}\\) randomly assigned to treatment \\(j\\) in trial \\(i\\), for \\(i = 1,\\ldots, n\\) and \\(j = 1,\\ldots, m\\). A two-way linear mixed model would model the outcome of interest \\(Y_{ijk}\\) as \\[ g(E(Y_{ijk})) = \\underbrace{\\alpha_i}_{\\text{fixed main effect of trial }i} + \\underbrace{\\beta_j}_{\\text{fixed main effect of treatment }j} + \\underbrace{u_{ij}}_{\\text{random effect of trial on treatment}},\\] where \\(g\\) is a link function such as the logit link in case of bernoulli data.\nThe random effects \\(u_{ij}\\) have mean zero and describe that the effect of treatments is allowed to vary across trials. If we wanted to assume the same effect of all treatment across all trials, we could instead omit this term. When we include it, we will ideally want to allow for the possibility that if one treatment has better outcomes in a trial, then other treatments might also. To achieve that we assume that for the trials \\(i=1,\\ldots,n\\) there is a multivariate normal random effect \\(\\boldsymbol{U}_i \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\), where \\(\\Sigma\\) is some covariance matrix that is either unstructured or has some particular structure (e.g. compound symmetric). In other words, the components of the random vector \\(\\boldsymbol{U}_i\\) for a trial \\(i\\) are correlated.\n\n11.2.1 Network meta-analysis with summaries for each arm\nIn case we have summary information by treatment arm, we will (just as for traditional arm-based meta-analyses) often be able to assume a normal sampling distribution for least squares means for each arm from linear models, log-odds, log-rates or log-hazard rates. We will denote these estimates for each arm by \\(\\hat{\\theta}_{ij}\\) and their standard error by \\(\\text{SE}(\\hat{\\theta}_{ij})\\). We then assume \\(\\theta_{ij} = \\alpha_i + \\beta_j + u_{ij}\\) as above and that \\(\\hat{\\theta}_{ij} \\sim N(\\theta_{ij}, \\text{SE}(\\hat{\\theta}_{ij}))\\).\nAlternatively, for a binary outcome, we can use that the number of patients with a success follows a binomial distribution and we would use a regression equation like above for the logit-probability \\(\\theta_{ij} = \\alpha_i + \\beta_j + u_{ij}\\) for each arm.\nTO BE DETERMINE WHAT TO DO FOR SURVIAL: Less clear, could one work with log-cumulative-hazard function (under a parametric model?)? Maybe one could get that from KM curves? Log-(or logit-?)survival probabilities at fixed points of time (e.g. 1-year)? Discussed by some papers such as this one (Combescure et al. 2012). Perhaps models fitted to reconstructed patient-level data/KM curves via, standard parametric models, fractional polynomial models and/or spline-based models? What about non-proportional hazards?\n\n\n11.2.2 Patient-, arm- or trial-level covariates\nAs needed, the regression equation \\(\\alpha_i + \\beta_j + u_{ij}\\) can be extended by allowing for patient level covariates \\(\\boldsymbol{x}_{ijk}\\) (in case we work with individual patient data), arm-level covariates \\(\\boldsymbol{x}_{ij}\\) or study-level covariates \\(\\boldsymbol{x}_i\\).\n\n\n11.2.3 Borrowing information from real-world data or other trials\nIf we have a fixed effect for the \\(\\alpha_i\\), then borrowing information between trials runs into the identifiability issues between the \\(\\alpha_i\\) and the \\(u_{ij}\\). In a frequentist approach we cannot resolve these issues. When we take a Bayesian approach, this identifiability issue will still severely limit how much information we can obtain about the distribution of the \\(u_{ij}\\) (while we would by definition not borrow information about the \\(\\alpha_i\\)) and the results will be sensitive to the choice of prior distributions.\nWe could borrow information across trials, we replace the trial main effect by an intercept term and a random trial effect on the intercept. However, then we will be conducting a meta-analysis, in which the causal effects of treatments are no longer just judged based on randomized within-trial comparisons. We would then also to an extent do comparisons across treatment arms from different trials. Besides the limitations any NMA is subject to, such comparisons are additionally affected by any prognostic factors that change the expected outcomes for a treatment in different trials. Such prognostic factors are much more common than effect modifiers that would cause trial by treatment interactions, and we know that to some extent such differences across trials will exist. Examples include improving (or worsening) patient outcomes due to better treatments or access to healthcare, different inclusion/exclusion criteria of different trials, and differences in recruited patient populations in terms of e.g. background therapy, disease severity, country/region and comorbidities. While we may hope that a random trial effect on the intercept would capture random variation in these aspects across trials, we have to be concerned that there could be a systematic bias in favor of some treatments in a NMA - especially if studies for some treatments were conducted later in time than for other treatments.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#implementation",
    "href": "src/02j_network_meta_analysis.html#implementation",
    "title": "11  Network meta-analysis",
    "section": "11.3 Implementation",
    "text": "11.3 Implementation\nIn this case, we have a binomial outcome. brms provides the y | trials(n) notation to indicate y successes out of n trials, while in stats::glm the way of specifying this is a little less intuitive: glm( cbind(y, n-y)~... ).\nWe will now try implement the network meta-analysis model we described above. In the process, we will discover some things that did not make a difference in a frequentist setting, but matter in the Bayesian setting with respect to how we can the most easily specify our prior knowledge (or lack thereof).\n\n11.3.1 Reference categories matter for setting priors\nFirstly, before we even think too much about any prior distributions, let see how brms parameterizes the model, if we specify it in the most obvious way by just writing 0 + study + trtc + (0 + trtc |study):\n\nbrmfit &lt;- brm(\n  data = smoking,\n  formula = r | trials(n) ~ 0 + study + trtc + (0 + trtc || study),\n  family = binomial(),\n  control = control_args,\n  prior = prior(class = b, normal(0, 3.14)),\n  silent = 2,\n  refresh = 0\n)\n\nAs we can see from the summary of the fitted model, there are coefficients for all study main effects, but not for all treatments: treatment 1 is omitted, and the coefficients for remaining treatments now represent contrasts against treatment 1. (Conversely, if we had instead written 0 + trtc + study + (0+trtc|study), then there would have been a coefficient for all treatments, but the study effects would have been parametrized in terms of contrasts with study 1.\n\nsummary(brmfit)\n\nWarning: There were 12 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 0 + study + trtc + (0 + trtc || study) \n   Data: smoking (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 24) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(trtcNo_intervention)            0.66      0.37     0.04     1.43 1.01      457      947\nsd(trtcSelf_help)                  0.42      0.37     0.02     1.37 1.00     1302     2183\nsd(trtcIndividual_counselling)     0.54      0.32     0.03     1.19 1.01      321     1326\nsd(trtcGroup_counselling)          0.90      0.63     0.07     2.46 1.00     1049     1172\n\nRegression Coefficients:\n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nstudy1                        -2.59      0.47    -3.48    -1.65 1.01      879     1834\nstudy2                        -2.26      0.44    -3.12    -1.40 1.00     1602     2164\nstudy3                        -1.24      0.72    -2.42     0.00 1.01      371     1376\nstudy4                        -3.73      0.56    -4.88    -2.63 1.00     2100     1622\nstudy5                        -2.19      0.38    -2.95    -1.40 1.00     1861     1806\nstudy6                        -2.66      0.74    -4.27    -1.34 1.01      834     1855\nstudy7                        -2.09      0.75    -3.59    -0.80 1.01      442     1532\nstudy8                        -2.03      0.66    -3.44    -0.90 1.01      726     1829\nstudy9                        -1.78      0.47    -2.68    -0.81 1.00     2433     1999\nstudy10                       -2.22      0.45    -3.09    -1.20 1.00     1670     1644\nstudy11                       -3.48      0.49    -4.41    -2.40 1.00     1860     1467\nstudy12                       -2.28      0.39    -3.06    -1.48 1.00     2019     1849\nstudy13                       -2.71      0.53    -3.77    -1.66 1.00     3107     2770\nstudy14                       -2.33      0.43    -3.16    -1.43 1.00     1895     1759\nstudy15                       -2.15      1.03    -4.48    -0.41 1.00     1616     2280\nstudy16                       -2.32      0.49    -3.29    -1.29 1.00     2196     1968\nstudy17                       -2.35      0.39    -3.16    -1.55 1.00     1616     1466\nstudy18                       -2.81      0.46    -3.66    -1.88 1.00     1120     1993\nstudy19                       -2.29      0.46    -3.16    -1.42 1.00      880     2095\nstudy20                       -3.03      0.42    -3.81    -2.19 1.00     1201     1832\nstudy21                       -0.84      0.47    -1.77     0.07 1.00     1789     2588\nstudy22                       -2.20      0.59    -3.36    -1.00 1.00     1798     2224\nstudy23                       -2.04      0.58    -3.22    -0.87 1.00     2066     1813\nstudy24                       -2.40      0.66    -3.73    -1.18 1.00     1893     2229\ntrtcSelf_help                  0.27      0.40    -0.55     1.09 1.00     1416     1766\ntrtcIndividual_counselling     0.61      0.28     0.01     1.14 1.01      973      896\ntrtcGroup_counselling          0.81      0.57    -0.33     1.98 1.00     1726     2167\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIs this a problem? In the frequentist context, this issue with the form of the design matrix for population-level effects is not important, because the pairwise treatment contrasts are uniquely estimable regardless of whether constraints such as \\(\\beta_1 = 0\\) are employed.\nHowever, if we wanted to imply some prior information or lack thereof, it may impact our preference for the specification of the population-level terms. For example, if we had some prior information (external to the likelihood) on a particular contrast between a pair of treatments, we would favor the parametrization above, in which the treatment terms are parametrized in terms of contrasts.\n\n\n11.3.2 Setting priors is easier in an over-parameterized model\nAnother possibility is to over-parametrize the model and have a main effect for each study (i.e. main effects for all studies study1 to study24 are present) and each treatment. The main rationale for this would be symmetry in how we set our prior distributions. I.e. we do not want to set our priors in a way that would somehow favor one treatment (or study - which will have studied some subset of treatments = might again favor somehow treatment) over any of the others. Unlike the specification above, the marginal variance of the each arm of each study will be constant in this approach.\nThis can be achieved by creating dummy variables for each treatment group and thereby enforcing the paramterization. We then specify our model as 0 + study + trtcA + trtcB + trtcC + trtcD + (0 + trtc | study). The code below constructs the dummy variables and corresponding formula in an automated way.\n\nB &lt;- model.matrix(~ 0 + trtc, data = smoking)\nS &lt;- model.matrix(~ 0 + study, data = smoking)\n\nsmoking_with_dummies &lt;- bind_cols(\n  dplyr::select(smoking, r, n, study, trtc),\n  as_tibble(B),\n  as_tibble(S)\n)\n\nf &lt;- as.formula(paste(\n  \"r | trials(n) ~ 0 + study +\",\n  paste(colnames(B), collapse = \" + \"),\n  \"+ (0 + trtc || study)\"\n))\n\n# could also do:\n# f &lt;- as.formula(paste(\n#     \"r | trials(n) ~ 0 + trtc +\",\n#     paste(colnames(S), collapse = \" + \"),\n#     \"+ (0 + trtc || study)\"\n# ))\n\nbrmfit_with_dummies &lt;- brm(data = smoking_with_dummies,\n                           formula = f,\n                           family = binomial(),\n                           control = control_args,\n                           prior = prior(class=b, normal(0, 3.14)),\n                           silent = 2,\n                           refresh = 0)\n\nWhen we now summarize the model fit, we have regression coefficients for all treatment groups and studies.\n\nsummary(brmfit_with_dummies)\n\nWarning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: binomial \n  Links: mu = logit \nFormula: r | trials(n) ~ 0 + study + trtcNo_intervention + trtcSelf_help + trtcIndividual_counselling + trtcGroup_counselling + (0 + trtc || study) \n   Data: smoking_with_dummies (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~study (Number of levels: 24) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(trtcNo_intervention)            0.62      0.34     0.05     1.30 1.01      578     1538\nsd(trtcSelf_help)                  0.43      0.39     0.02     1.44 1.00     2136     2856\nsd(trtcIndividual_counselling)     0.56      0.31     0.03     1.16 1.00      634     1428\nsd(trtcGroup_counselling)          1.02      0.72     0.09     2.77 1.00     1151     1536\n\nRegression Coefficients:\n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nstudy1                        -0.52      0.72    -1.97     0.86 1.01      545      904\nstudy2                        -0.23      0.70    -1.62     1.14 1.01      550     1112\nstudy3                         0.83      0.85    -0.82     2.48 1.01      482      802\nstudy4                        -1.68      0.78    -3.22    -0.19 1.00      679      951\nstudy5                        -0.08      0.69    -1.41     1.26 1.01      529     1090\nstudy6                        -0.66      0.91    -2.59     1.04 1.00      708     1237\nstudy7                        -0.05      0.89    -1.84     1.67 1.01      545      855\nstudy8                        -0.01      0.85    -1.77     1.60 1.00      625     1052\nstudy9                         0.31      0.73    -1.09     1.70 1.01      660     1494\nstudy10                       -0.14      0.71    -1.56     1.22 1.01      622     1402\nstudy11                       -1.39      0.72    -2.78     0.02 1.01      597     1198\nstudy12                       -0.15      0.69    -1.50     1.18 1.01      550     1097\nstudy13                       -0.61      0.76    -2.07     0.88 1.00      716     1448\nstudy14                       -0.24      0.72    -1.66     1.13 1.01      619     1281\nstudy15                       -0.34      1.24    -3.18     1.71 1.00     1134     1763\nstudy16                       -0.25      0.73    -1.71     1.16 1.01      624     1176\nstudy17                       -0.24      0.67    -1.56     1.03 1.01      535     1076\nstudy18                       -0.70      0.73    -2.11     0.73 1.01      567     1389\nstudy19                       -0.16      0.75    -1.62     1.29 1.01      551     1216\nstudy20                       -0.90      0.72    -2.31     0.46 1.01      532      923\nstudy21                        1.19      0.72    -0.22     2.56 1.00      609     1252\nstudy22                       -0.25      0.80    -1.84     1.33 1.00      777     1552\nstudy23                       -0.09      0.79    -1.69     1.41 1.00      762     1492\nstudy24                       -0.44      0.84    -2.14     1.16 1.00      727     1437\ntrtcNo_intervention           -2.29      0.62    -3.48    -1.06 1.01      458      771\ntrtcSelf_help                 -1.76      0.67    -3.08    -0.45 1.01      543     1060\ntrtcIndividual_counselling    -1.45      0.61    -2.62    -0.23 1.01      451      821\ntrtcGroup_counselling         -1.10      0.81    -2.61     0.54 1.00      710     1402\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhile the coefficients for each treatment have changed compared to the previous section, we should not actually look at the coefficients for each treament main effect, but at their contrasts. When we look at those, we see that the models lead to very similar inference (despite us not even setting any priors for Model 0).\n\n# utility function to estimate all pairwise contrasts between the main effects\n# for the variable named \"trtc\"\ncontrast_draws &lt;- function(brmfit, variable = \"trtc\"){\n  \n  trts &lt;- levels(brmfit$data[[variable]])\n  ntrt &lt;- nlevels(brmfit$data[[variable]])\n  \n  all_pairs &lt;- combn(1:ntrt, 2)\n  L_trt &lt;- t(apply(\n    all_pairs, 2, function(x){\n      out &lt;- numeric(ntrt)\n      out[x[1]] &lt;- 1\n      out[x[2]] &lt;- -1\n      out\n    }\n  ))\n  colnames(L_trt) &lt;- paste0(variable, trts)\n  \n  X &lt;- standata(brmfit)$X\n  L_study &lt;- matrix(0, nrow = nrow(L_trt), ncol = sum(!grepl(variable, colnames(X))))\n\n  # if the treatment variable is not dummy coded for every level then\n  # we need to drop the base category as this is masked by study\n  if(ncol(L_trt) == sum(grepl(variable, colnames(X)))){\n    L &lt;- cbind(L_study, L_trt)\n  } else{\n    L &lt;- cbind(L_study, L_trt[,-1])\n  }\n  \n  colnames(L) &lt;- colnames(X)\n  \n  # labels for the contrasts\n  labs &lt;- apply(combn(trts, 2), 2, paste, collapse = \" vs \")\n  rownames(L) &lt;- labs\n  \n  B &lt;- as_draws_matrix(brmfit, variable = \"^b_\", regex = TRUE)\n  \n  gamma_draws &lt;- B %*% t(L)\n  \n  # convert to posterior object\n  as_draws_matrix(gamma_draws)\n  \n}\n\nsummarize_contrasts &lt;- function(brmfit, model_name = \"model\", variable = \"trtc\", ...){\n  relocate(\n    mutate(summarize_draws(contrast_draws(brmfit, variable = variable), ...), model = model_name),\n    model\n  )\n}\n\nbind_rows(\n  summarize_contrasts(brmfit, \"Full-rank model\"),\n  summarize_contrasts(brmfit_with_dummies, \"Overparametrized model\")\n) %&gt;%\n    gt() %&gt;%\n    fmt_number(where(is.numeric), decimals=2)\n\n\n\n\n\n\n\n\nmodel\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nFull-rank model\nNo_intervention vs Self_help\n−0.27\n−0.27\n0.40\n0.36\n−0.92\n0.37\n1.00\n1,398.93\n1,747.87\n\n\nFull-rank model\nNo_intervention vs Individual_counselling\n−0.61\n−0.62\n0.28\n0.25\n−1.05\n−0.13\n1.01\n948.08\n890.87\n\n\nFull-rank model\nNo_intervention vs Group_counselling\n−0.81\n−0.80\n0.57\n0.50\n−1.73\n0.08\n1.00\n1,701.81\n2,147.63\n\n\nFull-rank model\nSelf_help vs Individual_counselling\n−0.34\n−0.33\n0.41\n0.38\n−0.99\n0.33\n1.00\n1,617.50\n2,473.83\n\n\nFull-rank model\nSelf_help vs Group_counselling\n−0.54\n−0.53\n0.58\n0.51\n−1.47\n0.41\n1.00\n2,485.66\n2,569.57\n\n\nFull-rank model\nIndividual_counselling vs Group_counselling\n−0.20\n−0.19\n0.54\n0.46\n−1.08\n0.63\n1.00\n2,808.62\n2,286.79\n\n\nOverparametrized model\nNo_intervention vs Self_help\n−0.52\n−0.50\n0.41\n0.37\n−1.22\n0.08\n1.00\n2,938.66\n2,981.21\n\n\nOverparametrized model\nNo_intervention vs Individual_counselling\n−0.83\n−0.83\n0.25\n0.23\n−1.27\n−0.43\n1.00\n4,997.22\n3,208.11\n\n\nOverparametrized model\nNo_intervention vs Group_counselling\n−1.19\n−1.13\n0.63\n0.51\n−2.24\n−0.30\n1.00\n3,245.52\n2,575.15\n\n\nOverparametrized model\nSelf_help vs Individual_counselling\n−0.31\n−0.32\n0.41\n0.38\n−0.97\n0.35\n1.00\n3,164.54\n3,204.82\n\n\nOverparametrized model\nSelf_help vs Group_counselling\n−0.66\n−0.63\n0.66\n0.54\n−1.73\n0.33\n1.00\n2,887.11\n2,348.81\n\n\nOverparametrized model\nIndividual_counselling vs Group_counselling\n−0.35\n−0.29\n0.61\n0.50\n−1.38\n0.48\n1.00\n3,338.67\n2,486.91\n\n\n\n\n\n\n\n\nNote also that it is hard to interpret the study main effects in these models. We could try to make them a little more interpretable by making them represent something like the average expected outcome across treatment groups (i.e. if we leave out the treatment main effects, we get a predicted outcome that is between the predicted outcomes for the treatment groups). For this purpose we encode the dummy for each treatment group as \\(0.5\\) vs. \\(-0.5\\) (if it is not that treatment group). However, treatment main effect coefficients should still be looked at in contrast to each other.\n\nsmoking_with_centered_dummies &lt;- bind_cols(\n  dplyr::select(smoking, r, n, study, trtc),\n  as_tibble(B - 0.5),\n  as_tibble(S)\n)\n\nbrmfit_with_centered_dummies &lt;- brm(\n    data = smoking_with_centered_dummies,\n    formula = f,\n    family = binomial(),\n    control = control_args,\n    prior = prior(class=b, normal(0, 3.14)),\n    silent = 2,\n    refresh = 0\n)\n\nWith the relatively low-information priors we’ve chosen here, all three of these models produce similar results\n\nall_contrasts &lt;- tibble(\n  model_name = c(\"Full-rank model\",\n                 \"Overparametrized model\",\n                 \"Overparametrized model with centering\"),\n  brmfit = list(brmfit, brmfit_with_dummies, brmfit_with_centered_dummies)\n) %&gt;%\n    mutate(contrasts = map2(brmfit, model_name, summarize_contrasts)) %&gt;%\n    select(-brmfit) %&gt;%\n    unnest(contrasts)\n\nall_contrasts %&gt;%\n    mutate(model=str_remove_all(model, \" model\"),\n           model=str_replace_all(model, \"with\", \"\\nwith\"),\n           variable=str_replace_all(variable, \"vs\", \"\\nvs\\n\"),\n           variable=str_replace_all(variable, \"counselling\", \"couns.\"),\n           variable=str_replace_all(variable, \"Individual\", \"Indiv.\"),\n           ) %&gt;%\n    ggplot(aes(x=median, y=model, xmin=q5, xmax=q95)) +\n    geom_vline(xintercept=0, color=\"darkred\", linetype=2) +\n    geom_point() +\n    geom_errorbarh() +\n    xlab(\"log-odds ratio\") +\n    facet_wrap(~variable)\n\n\n\n\n\n\n\n\n\n\n11.3.3 Ranking treatments and the probability of being the best treatment\nProbability that each treatment is best is straightforward in any of these approaches. To illustrate:\n\nB &lt;- as_draws_matrix(brmfit_with_dummies, variable = \"b_trtc\", regex = TRUE)\nbest_idx &lt;- apply(B, 1, which.max)\ntrts &lt;- levels(smoking$trtc)\nbest_trt &lt;- factor(trts, levels=trts)[best_idx]\nprob_best &lt;- prop.table(table(best_trt))\n\ngt(arrange(as.data.frame(prob_best), -Freq)) %&gt;% fmt_percent(Freq)\n\n\n\n\n\n\n\n\nbest_trt\nFreq\n\n\n\n\nGroup_counselling\n70.25%\n\n\nIndividual_counselling\n22.50%\n\n\nSelf_help\n7.25%\n\n\nNo_intervention\n0.00%\n\n\n\n\n\n\n\n\n\n\n11.3.4 Detailed look at a pairwise contrast\nIn some situations, there may be a particular treatment contrast that is of special interest. In some such scenarios, where we have several This is to illustrate that by including indirect evidence, we can enhance our inference for a pairwise contrast of interest, relative to a simple pairwise meta-analysis.\nThe following analyses are compared, in terms of their estimates of the relative effect (log odds ratio) of individual counselling versus no intervention:\n\nNetwork meta-analysis model (brmfit1 above)\nPairwise meta-analysis model (currently RBesT is used here, but we could equally use brms)\n“Stratified” analysis of the contrast separately in each study, using independent beta-binomial models\n\n\n\nShow the code\n# identify the A vs C studies\nac_studies &lt;- smoking %&gt;%\n  filter(trtc %in% c(\"No_intervention\", \"Individual_counselling\")) %&gt;%\n  add_count(study, name = \"num_treatments\") %&gt;%\n  filter(num_treatments == 2) %&gt;%\n  arrange(study, trtc)\n\n# summarize the A vs C log odds ratio using Beta-Binomial models independently\n# across studies and arms\narm_level_beta_binom &lt;- ac_studies %&gt;%\n  rowwise() %&gt;%\n  mutate(p = map2(r, n, ~ rbeta(10000, r + 0.5, n - r + 0.5))) %&gt;%\n  dplyr::select(study, trtc, p) %&gt;%\n  pivot_wider(names_from = trtc, values_from = p)\n\nstudy_effects &lt;- arm_level_beta_binom %&gt;%\n  mutate(log_odds_ratio = map2(No_intervention, Individual_counselling,\n                               ~ log(.x) + log(1 - .y) - log(1 - .x) - log(.y)),\n         out = map(log_odds_ratio, ~ summarize_draws(matrix(.), mean, sd, ~ quantile(., probs = c(0.025, 0.5, 0.975))))) %&gt;%\n  transmute(study = factor(study), out) %&gt;%\n  unnest(out) %&gt;%\n  dplyr::select(-variable)\n\n# pairwise meta-analysis of the log odds ratios based on summary statistics\nrbest_fit &lt;- RBesT::gMAP(cbind(mean, sd) ~ 1 | study,\n                         data = study_effects,\n                         tau.dist = \"HalfNormal\",\n                         tau.prior = 1,\n                         beta.prior = cbind(0,2))\n\n# study-level treatment-effect estimates from RBesT\nfitted_rbest &lt;- bind_cols(dplyr::select(study_effects, study), fitted(rbest_fit))\n\n# study-level treatment effect estimates from the NMA model\nnd &lt;- inner_join(smoking_with_dummies,\n                 dplyr::select(ac_studies, study, trtc),\n                 c(\"study\", \"trtc\")) %&gt;%\n  mutate(n = 1)\n\nlp &lt;- posterior_epred(brmfit, newdata = nd)\nlpA &lt;- lp[,nd$trtc == \"No_intervention\"]\nlpC &lt;- lp[,nd$trtc == \"Individual_counselling\"]\nlor &lt;- log(lpA) + log(1 - lpC) - log(1 - lpA) - log(lpC)\n\nfitted_nma &lt;- bind_cols(\n  dplyr::select(study_effects, study),\n  dplyr::select(summarize_draws(lor, mean, sd, ~ quantile(., probs = c(0.025, 0.5, 0.975))), -variable)\n)\n\n# mean and MAP treatment effect estimates from RBesT\nmean_rbest &lt;- summary(rbest_fit, type=\"response\")[c(\"theta.pred\", \"theta\")] %&gt;% \n  do.call(what = \"rbind\") %&gt;%\n  as_tibble(rownames = \"study\") %&gt;% \n  mutate(type = \"pairwise meta\",\n         study = c(\"theta_resp_pred\" = \"MAP\", \"theta_resp\" = \"Mean\")[study])\n\n# mean treatment effect estimate from NMA model\nmean_nma &lt;- summarize_contrasts(brmfit, variable = \"trtc\", model_name = \"nma\", mean, sd,\n                                ~ quantile(., probs = c(0.025, 0.5, 0.975))) %&gt;%\n  filter(variable == \"No_intervention vs Individual_counselling\") %&gt;%\n  mutate(study = \"Mean\", type = \"network meta\")\n\nall_ests &lt;- bind_rows(\n  mutate(study_effects, type = \"stratified\", estimate_type = \"Study-level\"),\n  mutate(fitted_rbest, type = \"pairwise meta\", estimate_type = \"Study-level\"),\n  mutate(fitted_nma, type = \"network meta\", estimate_type = \"Study-level\"),\n  mutate(mean_rbest, estimate_type = \"Mean\"),\n  mutate(mean_nma, estimate_type = \"Mean\")\n)\n\nggplot(all_ests, aes(y = study, x = mean, xmin = `2.5%`, xmax = `97.5%`,\n                     group = type, color = type)) +\n    geom_pointrange(position = position_dodge(0.4)) +\n    facet_grid(estimate_type ~ ., space = \"free\", scales = \"free\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    scale_x_continuous(breaks=seq(-5,1,by=1)) +\n    coord_cartesian(xlim=c(-5,0.5)) +\n    labs(x = \"Log odds ratio for individual counselling vs no intervention\",\n         y = \"Study\", color = \"Analysis\\nmethod\") +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#conclusion",
    "href": "src/02j_network_meta_analysis.html#conclusion",
    "title": "11  Network meta-analysis",
    "section": "11.4 Conclusion",
    "text": "11.4 Conclusion\nArm-based (network-)meta-analysis approaches compare favorably with contrast based approaches in many situations and fit very easily into the Bayesian regression modeling machinery provided by brms. The main challenge we face is how one should parametrize the model in order to be able to specify prior distributions easily, for which we showed several options.\nFor NMA, Bayesian approaches are very popular due to the ease with which one can obtain inference about various quantities of interest (e.g. ranking of treatments, probability that a treatment is the best one) via transformations of the MCMC samples from the posterior distribution.\nTBD: Further topics might include including observational data (aka real-world Exploiting that some drugs are in the same class, that some arms are different doses of the same drug, or when we have subgroup results from the same study.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/02j_network_meta_analysis.html#exercises",
    "href": "src/02j_network_meta_analysis.html#exercises",
    "title": "11  Network meta-analysis",
    "section": "11.5 Exercises",
    "text": "11.5 Exercises\n\n11.5.1 Excercise 1\nWe will use the data on the occurrence of malignancy anti-tumour necrosis factor (anti-TNF) drugs used in the paper by Warren, Abrams, and Sutton (2014). We have information for each arm of 13 studies of three different anti-TNF drugs (etanercept, adalimumab and infliximab) on how many patients developed a malignancy y out of the total number of patients in an arm n. It is common in drug development to simplistically reduce what is in truth a time-to-event process of adverse event occurrence into a binomial problem. In practice, we should be cautious about this simplification, because differential drop-out could lead to biased conclusions and we will by necessaity get variation in effect measures for binomial outcomes across studies of different duration even if the time-to-event distributions are identical across studies.\n\nantiTNF &lt;- tibble(\n  study = c(\"Ericson (1999)\", \"Ericson (1999)\", \"Ericson (1999)\", \n            \"Moreland (1999)\", \"Moreland (1999)\", \"Moreland (1999)\", \n            \"Genovese (2002)\", \"Genovese (2002)\", \"Genovese (2002)\", \n            \"Combe (2006)\", \"Combe (2006)\", \"Van der Heijde (2006)\", \n            \"Van der Heijde (2006)\", \"Weisman (2007)/Baumgartner (2004)\", \n            \"Weisman (2007)/Baumgartner (2004)\", \"Furst (2003)\", \"Furst (2003)\", \n            \"Weinblatt (2003)\", \"Weinblatt (2003)\", \"Weinblatt (2003)\", \n            \"Weinblatt (2003)\", \"Keystone (2004)\", \"Keystone (2004)\", \n            \"Van de Putte (2004)\", \"Van de Putte (2004)\", \"Van de Putte (2004)\", \n            \"Van de Putte (2004)\", \"Breedveld (2006)\", \"Breedveld (2006)\", \n            \"Maini (2004)\", \"Maini (2004)\", \"Maini (2004)\", \"St Clair (2004)\", \n            \"St Clair (2004)\", \"St Clair (2004)\"), \n  treatment = c(\"Control\", \"Etanercept\", \"Etanercept\", \"Control\", \"Etanercept\", \n                \"Etanercept\", \"Control\", \"Etanercept\", \"Etanercept\", \"Control\", \n                \"Etanercept\", \"Control\", \"Etanercept\", \"Control\", \"Etanercept\", \n                \"Control\", \"Adalimumab\", \"Control\", \"Adalimumab\", \"Adalimumab\", \n                \"Adalimumab\", \"Control\", \"Adalimumab\", \"Control\", \"Adalimumab\", \n                \"Adalimumab\", \"Adalimumab\", \"Control\", \"Adalimumab\", \n                \"Control\", \"Infliximab\", \"Infliximab\", \"Control\", \"Infliximab\", \n                \"Infliximab\"), \n  regimen = c(NA, \"25 mg biw\", \"10 mg qw or 25 mg qw or 10 mg biw\", NA, \n              \"25 mg biw\", \"10 mg biw\", NA, \"25 mg biw\", \"10 mg biw\", NA, \n              \"25 mg biw\", NA, \"25 mg biw\", NA, \"25 mg biw\", NA, \"40 mg eow\", \n              NA, \"40 mg eow\", \"20 mg eow\", \"80 mg eow\", NA, \n              \"20 mg qw or 40 mg eow\", NA, \"20 mg qw or 40 mg eow\", \n              \"20 mg eow\", \"40 mg qw\", NA, \"40 mg eow\", NA, \"3 mg/kg q8w\", \n              \"3 mg/kg q4w or 10 mg/kg q8w or 10 mg/kg q4w\", NA, \"3 mg/kg q8w\", \n              \"6 mg/q8w\"), \n  dose = c(NA, \"Rec\", \"Low\", NA, \"Rec\", \"Low\", NA, \"Rec\", \"Low\", NA, \"Rec\", NA, \n           \"Rec\", NA, \"Rec\", NA, \"Rec\", NA, \"Rec\", \"Low\", \"High\", NA, \"Rec\", NA, \n           \"Rec\", \"Low\", \"High\", NA, \"Rec\", NA, \"Rec\", \"High\", NA, \"Rec\", \n           \"High\"), \n  n = c(105L, 111L, 343L, 80L, 78L, 76L, 217L, 207L, 208L, 50L, 204L, 228L, \n        454L, 269L, 266L, 318L, 318L, 62L, 67L, 69L, 73L, 200L, 419L, 110L, \n        225L, 106L, 103L, 257L, 542L, 88L, 86L, 254L, 291L, 372L, 377L), \n  y = c(0L, 0L, 2L, 0L, 1L, 0L, 4L, 5L, 5L, 0L, 1L, 1L, 10L, 2L, 2L, 0L, 4L, 0L, \n        0L, 0L, 1L, 1L, 8L, 1L, 2L,  1L, 1L, 4L, 6L, 1L, 1L, 8L, 0L, 0L, 4L))\n\nIf we look at the data, we can see that the proportion of patients with an event is quite low (&lt;4%) across all studies and arms. I.e. we are in a rare event setting, where a Bayesian approach with sensibly chosen priors might be helpful.\n\nantiTNF %&gt;%\n  mutate(study = factor(study),\n         arm = paste0(treatment, ifelse(is.na(dose),\"\",paste0(\" \",dose)))) %&gt;%\n  ggplot(aes(x=study, y=y/n, \n             fill=arm, \n             group=arm,\n             ymin=qbeta(p=0.025, shape1=y, shape2=1+n-y),\n             ymax=qbeta(p=0.975, shape1=y+1, shape2=n-y))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(position = \"dodge\", alpha=0.3) +\n  theme(legend.position=\"bottom\") +\n  guides(fill=guide_legend(ncol=2)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nTry fitting models of increasing complexity (slightly different to those done in the Warren, Abrams, and Sutton (2014) paper, which you could also choose to reproduce):\n\nA simple meta-analysis of anti-TNF therapy vs. non-anti-TNF controls.\nA network meta-analysis with each anti-TNF drug having a separate fixed treatment effects parameter, but ignoring informaiton on doses and regimens.\nA network meta-analysis that treats all unique combinations of drug and dose/regimen as a separate treatment.\nA network meta-analysis that assumes that each drug may have a different effect on malignancy with this effect being monotonic across doses. Note the mo() notation provided by brms to define monotonic effects. Look into borrowing information about the maximum effect across drugs.\n\n\n\n\n\n\n\n\nCombescure, C, DS Courvoisier, G Haller, and TV Perneger. 2012. “Meta-Analysis of Two-Arm Studies: Modeling the Intervention Effect from Survival Probabilities.” Statistical Methods in Medical Research 25 (2): 857–71. https://doi.org/10.1177/0962280212469716.\n\n\nDias, S., N. J. Welton, A. J. Sutton, and A. Ades. 2014. “NICE DSU Technical Support Document 2: A Generalised Linear Modelling Framework for Pairwise and Network Meta-Analysis of Randomised Controlled Trials.” Technical report. NICE Decision Support Unit.\n\n\nHasselblad, Vic. 1998. “Meta-Analysis of Multitreatment Studies.” Medical Decision Making 18 (1): 37–43. https://doi.org/10.1177/0272989x9801800110.\n\n\nLu, G., and A. E. Ades. 2004. “Combination of Direct and Indirect Evidence in Mixed Treatment Comparisons.” Statistics in Medicine 23 (20): 3105–24. https://doi.org/10.1002/sim.1875.\n\n\nPiepho, H. P., E. R. Williams, and L. V. Madden. 2012. “The Use of Two-Way Linear Mixed Models in Multitreatment Meta-Analysis.” Biometrics 68 (4): 1269–77. https://doi.org/10.1111/j.1541-0420.2012.01786.x.\n\n\nWarren, Fiona C., Keith R. Abrams, and Alex J. Sutton. 2014. “Hierarchical Network Meta-Analysis Models to Address Sparsity of Events and Differing Treatment Classifications with Regard to Adverse Outcomes.” Statistics in Medicine 33 (14): 2449–66. https://doi.org/10.1002/sim.6131.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Network meta-analysis</span>"
    ]
  },
  {
    "objectID": "src/03a_stan_code.html",
    "href": "src/03a_stan_code.html",
    "title": "12  Exposing stan code",
    "section": "",
    "text": "In some situations it can be useful to make use of the Stan functions brms uses inside the Stan programs it creates. This is for example the case when creating a custom family as described in the vignette on custom response distributions. Whenever defining a custom family, one does provide Stan code snippets to brms which are used to setup the respective Stan model. Since the same functions are then useful for post-proccessing of the model posterior, the created functions can be used in R as well using the expose_functions from brms. Please refer to the vignette on custom families and the help on the expose function utility function ?expose_functions.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exposing stan code</span>"
    ]
  },
  {
    "objectID": "src/03b_parallel.html",
    "href": "src/03b_parallel.html",
    "title": "13  Parallel computation",
    "section": "",
    "text": "13.1 Implementation of cmq_brm\n# executes brm with parallelization via clustermq\ncmq_brm &lt;- function(..., seed, control=list(adapt_delta=0.9), cores, file, chains=4, .log_worker=FALSE) {\n    checkmate::assert_integer(as.integer(seed), lower=1, any.missing=FALSE, len=1)\n    brms_global &lt;- options()[grep(\"^brms\", names(options()), value=TRUE)]\n    cmdstanr_global &lt;- options()[grep(\"^cmdstanr\", names(options()), value=TRUE)]\n    dots &lt;- rlang::enquos(...)\n    brms_args &lt;- lapply(dots, rlang::eval_tidy)\n    suppressWarnings(model &lt;- do.call(brm, modifyList(brms_args, list(chains=0, cores=1))))\n    update_args &lt;- list(object=model, seed=seed, cores=1, chains=1, control=control)\n    if(!missing(file)) {\n        update_args$file &lt;- sub(\"\\\\.rds$\", \"\", file)\n    }\n    if(!missing(file)) {\n        brms_args$file &lt;- sub(\"\\\\.rds$\", \"\", file)\n    }\n    master_lib_paths &lt;- .libPaths()\n    update_model &lt;- function(chain_id) {\n        .libPaths(master_lib_paths)\n        library(brms)\n        # in case file is part of extra-arguments, we add here the chain_id\n        # to get correct by-chain file caching\n        if(\"file\" %in% names(update_args)) {\n            update_args &lt;- modifyList(update_args, list(file=paste0(update_args$file, \"-\", chain_id)))\n        }\n        if(\"file\" %in% names(brms_args)) {\n            brms_args &lt;- modifyList(brms_args, list(file=paste0(brms_args$file, \"-\", chain_id)))\n        }\n        update_args$chain_id &lt;- chain_id\n        brms_args$chain_id &lt;- chain_id\n        # ensure the same brms & cmdstanr global options are set\n        options(brms_global)\n        options(cmdstanr_global)\n        ## for the rstan backend we do an update while for cmdstanr we\n        ## have to avoid this for whatever reason\n        if(model$backend == \"cmdstanr\") {\n            msg &lt;- capture.output(fit &lt;- do.call(brm, modifyList(brms_args, list(chains=1))))\n        } else {\n            msg &lt;- capture.output(fit &lt;- do.call(update, update_args))\n        }\n        list(fit=fit, msg=msg)\n    }\n    n_jobs &lt;- chains\n    backend &lt;- getOption(\"clustermq.scheduler\", \"multiprocess\")\n    if(backend %in% c(\"multiprocess\", \"multicore\")) {\n        n_jobs &lt;- min(chains, getOption(\"mc.cores\", 1))\n    }\n    cores_per_chain &lt;- 1\n    if(!is.null(model$threads$threads)) {\n        cores_per_chain &lt;- model$threads$threads\n    }\n    if(chains == 1 & cores_per_chain == 1) {\n        ## looks like a debugging run...avoid clustermq\n        return(update_model(1)$fit)\n    }\n    message(\"Starting \", chains, \" chains with a concurrency of \", n_jobs, \" and using \", cores_per_chain, \" cores per chain with backend \", backend, \"...\\n\")\n    cluster_update &lt;- clustermq::Q(update_model, chain_id=1:chains, n_jobs=n_jobs, export=list(update_args=update_args, brms_args=brms_args, brms_global=brms_global, cmdstanr_global=cmdstanr_global, master_lib_paths=master_lib_paths, model=model),\n                                   template=list(cores=cores_per_chain),\n                                   log_worker=.log_worker)\n    fit &lt;- combine_models(mlist=lapply(cluster_update, \"[[\", \"fit\"))\n    msg &lt;- lapply(cluster_update, \"[[\", \"msg\")\n    for(i in seq_len(length(msg))) {\n        if(length(msg[[i]]) == 0)\n            next\n        cat(paste0(\"Output for chain \", i, \":\\n\"))\n        cat(paste(msg[[i]], collapse=\"\\n\"), \"\\n\")\n    }\n    fit$file &lt;- NULL\n    fit\n}",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parallel computation</span>"
    ]
  }
]